{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70c9d164",
   "metadata": {},
   "source": [
    "# All path need to be adjusted accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8848a0",
   "metadata": {},
   "source": [
    "# extract extensions for each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca2dd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "BASE_DIR = r\"M:\\FULL_DATA_COLLECTED\"\n",
    "\n",
    "OUTPUT_CSV = os.path.join(BASE_DIR, 'extracted_file_extensions_with_paths_after_removing.csv')\n",
    "\n",
    "def extract_unique_file_extensions(base_dir):\n",
    "    extensions_data = []\n",
    "\n",
    "    for project_name in os.listdir(base_dir):\n",
    "        project_path = os.path.join(base_dir, project_name)\n",
    "        if not os.path.isdir(project_path):\n",
    "            continue\n",
    "\n",
    "        for commit_dir in os.listdir(project_path):\n",
    "            commit_path = os.path.join(project_path, commit_dir)\n",
    "            if not os.path.isdir(commit_path):\n",
    "                continue\n",
    "\n",
    "            all_versions_path = os.path.join(commit_path, 'all_versions')\n",
    "            if not os.path.exists(all_versions_path):\n",
    "                continue\n",
    "\n",
    "            processed_files = set()\n",
    "\n",
    "            for version_file in os.listdir(all_versions_path):\n",
    "                version_file_path = os.path.join(all_versions_path, version_file)\n",
    "                if not os.path.isfile(version_file_path):\n",
    "                    continue\n",
    "\n",
    "                if \".\" in version_file:\n",
    "                    first_dot_index = version_file.find(\".\")\n",
    "                    remaining_part = version_file[first_dot_index + 1:]\n",
    "\n",
    "                    if \"_\" in remaining_part:\n",
    "                        first_underscore_index = remaining_part.find(\"_\")\n",
    "                        extension = remaining_part[:first_underscore_index]\n",
    "                    else:\n",
    "                        extension = \"No Extension\"\n",
    "                else:\n",
    "                    extension = \"No Extension\"\n",
    "\n",
    "                base_name = version_file.split('_v')[0] if \"_v\" in version_file else version_file\n",
    "\n",
    "                if base_name not in processed_files:\n",
    "                    processed_files.add(base_name)\n",
    "\n",
    "\n",
    "\n",
    "                    extensions_data.append({\n",
    "                        'Project': project_name,\n",
    "                        'File Path': all_versions_path,\n",
    "                        'Original File Name': base_name,\n",
    "                        'Extension': extension\n",
    "                    })\n",
    "\n",
    "    return extensions_data\n",
    "\n",
    "def save_extensions_to_csv(extensions_data, output_csv):\n",
    "    with open(output_csv, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=['Project', 'File Path', 'Original File Name', 'Extension'])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(extensions_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    extensions_data = extract_unique_file_extensions(BASE_DIR)\n",
    "    save_extensions_to_csv(extensions_data, OUTPUT_CSV)\n",
    "    print(f\"Extracted unique file extensions saved to: {OUTPUT_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e31cbc",
   "metadata": {},
   "source": [
    "# total files per extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbcdf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "BASE_DIR = r\"M:\\FULL_DATA_COLLECTED\"\n",
    "\n",
    "OUTPUT_CSV = os.path.join(BASE_DIR, 'extracted_file_extensions_with_paths_afterRemoving.csv')\n",
    "SUMMARY_CSV = os.path.join(BASE_DIR, 'extension_summary_afterRemoving_afterRemoving.csv')\n",
    "\n",
    "def extract_unique_file_extensions(base_dir):\n",
    "    extensions_data = []\n",
    "    extension_counts = Counter()\n",
    "\n",
    "    for project_name in os.listdir(base_dir):\n",
    "        project_path = os.path.join(base_dir, project_name)\n",
    "        if not os.path.isdir(project_path):\n",
    "            continue\n",
    "\n",
    "        for commit_dir in os.listdir(project_path):\n",
    "            commit_path = os.path.join(project_path, commit_dir)\n",
    "            if not os.path.isdir(commit_path):\n",
    "                continue\n",
    "\n",
    "            all_versions_path = os.path.join(commit_path, 'all_versions')\n",
    "            if not os.path.exists(all_versions_path):\n",
    "                continue\n",
    "\n",
    "            processed_files = set()\n",
    "\n",
    "            for version_file in os.listdir(all_versions_path):\n",
    "                version_file_path = os.path.join(all_versions_path, version_file)\n",
    "                if not os.path.isfile(version_file_path):\n",
    "                    continue\n",
    "\n",
    "                if \".\" in version_file:\n",
    "                    first_dot_index = version_file.find(\".\")\n",
    "                    remaining_part = version_file[first_dot_index + 1:]\n",
    "\n",
    "                    if \"_\" in remaining_part:\n",
    "                        first_underscore_index = remaining_part.find(\"_\")\n",
    "                        extension = remaining_part[:first_underscore_index]\n",
    "                    else:\n",
    "                        extension = \"No Extension\"\n",
    "                else:\n",
    "                    extension = \"No Extension\"\n",
    "\n",
    "                base_name = version_file.split('_v')[0] if \"_v\" in version_file else version_file\n",
    "\n",
    "                if base_name not in processed_files:\n",
    "                    processed_files.add(base_name)\n",
    "\n",
    "\n",
    "\n",
    "                    extensions_data.append({\n",
    "                        'Project': project_name,\n",
    "                        'File Path': all_versions_path,\n",
    "                        'Original File Name': base_name,\n",
    "                        'Extension': extension\n",
    "                    })\n",
    "\n",
    "                    extension_counts[extension] += 1\n",
    "\n",
    "    return extensions_data, extension_counts\n",
    "\n",
    "def save_extensions_to_csv(extensions_data, output_csv):\n",
    "    with open(output_csv, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=['Project', 'File Path', 'Original File Name', 'Extension'])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(extensions_data)\n",
    "\n",
    "def save_extension_summary_to_csv(extension_counts, summary_csv):\n",
    "    with open(summary_csv, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(['Extension', 'Total Files'])\n",
    "        for extension, count in extension_counts.items():\n",
    "            writer.writerow([extension, count])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    extensions_data, extension_counts = extract_unique_file_extensions(BASE_DIR)\n",
    "    save_extensions_to_csv(extensions_data, OUTPUT_CSV)\n",
    "    save_extension_summary_to_csv(extension_counts, SUMMARY_CSV)\n",
    "    print(f\"Extracted unique file extensions saved to: {OUTPUT_CSV}\")\n",
    "    print(f\"Extension summary saved to: {SUMMARY_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd3ba00",
   "metadata": {},
   "source": [
    "# removing non C M:\\FULL_DATA_COLLECTED\\chromium\\chromium_517ac71c9ee27f856f9becde8abea7d1604af9d4\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4d779f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "COMMIT_DIR = r\"M:\\FULL_DATA_COLLECTED\\chromium\\chromium_517ac71c9ee27f856f9becde8abea7d1604af9d4\"\n",
    "\n",
    "C_RELATED_EXTENSIONS = {'c', 'h', 'cc', 'cpp', 'hpp', 'cxx', 'hxx', 'ino', 'hh', 'C'}\n",
    "\n",
    "LOG_FILE = os.path.join(COMMIT_DIR, 'cleanup_log.txt')\n",
    "\n",
    "def extract_true_extension(file_name):\n",
    "    if \".\" in file_name:\n",
    "        first_dot_index = file_name.find(\".\")\n",
    "        underscore_index = file_name.find(\"_\", first_dot_index)\n",
    "        if underscore_index != -1:\n",
    "            return file_name[first_dot_index + 1:underscore_index]\n",
    "    return \"No Extension\"\n",
    "\n",
    "def clean_non_c_files_in_directory(directory):\n",
    "    total_removed = 0\n",
    "    total_retained = 0\n",
    "\n",
    "    with open(LOG_FILE, 'w') as log:\n",
    "        log.write(f\"Cleanup Log for Directory: {directory}\\n\")\n",
    "        log.write(\"=\" * 50 + \"\\n\")\n",
    "\n",
    "        for root, _, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                true_extension = extract_true_extension(file)\n",
    "\n",
    "                if true_extension not in C_RELATED_EXTENSIONS:\n",
    "                    try:\n",
    "                        os.remove(file_path)\n",
    "                        total_removed += 1\n",
    "                        log.write(f\"Removed: {file_path} (Extension: {true_extension})\\n\")\n",
    "                    except Exception as e:\n",
    "                        log.write(f\"Failed to remove {file_path}: {e}\\n\")\n",
    "                else:\n",
    "                    total_retained += 1\n",
    "                    log.write(f\"Retained: {file_path} (Extension: {true_extension})\\n\")\n",
    "\n",
    "        log.write(\"\\n\")\n",
    "        log.write(\"=\" * 50 + \"\\n\")\n",
    "        log.write(f\"Total files removed: {total_removed}\\n\")\n",
    "        log.write(f\"Total files retained: {total_retained}\\n\")\n",
    "\n",
    "    print(f\"Cleanup completed. Log file saved at {LOG_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    clean_non_c_files_in_directory(COMMIT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4029d8b2",
   "metadata": {},
   "source": [
    "# removing only chromium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0840ffba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "BASE_DIR = r\"M:\\FULL_DATA_COLLECTED\\chromium\"\n",
    "\n",
    "C_RELATED_EXTENSIONS = {'c', 'h', 'cc', 'cpp', 'hpp', 'cxx', 'hxx', 'ino', 'hh', 'C'}\n",
    "\n",
    "LOG_FILE = os.path.join(BASE_DIR, 'chromium_cleanup_log.txt')\n",
    "\n",
    "def extract_true_extension(file_name):\n",
    "    if \".\" in file_name:\n",
    "        first_dot_index = file_name.find(\".\")\n",
    "        underscore_index = file_name.find(\"_\", first_dot_index)\n",
    "        if underscore_index != -1:\n",
    "            return file_name[first_dot_index + 1:underscore_index]\n",
    "    return \"No Extension\"\n",
    "\n",
    "def clean_non_c_files_in_directory(directory, log):\n",
    "    total_removed = 0\n",
    "    total_retained = 0\n",
    "\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            true_extension = extract_true_extension(file)\n",
    "\n",
    "            if true_extension not in C_RELATED_EXTENSIONS:\n",
    "                try:\n",
    "                    os.remove(file_path)\n",
    "                    total_removed += 1\n",
    "                    log.write(f\"Removed: {file_path} (Extension: {true_extension})\\n\")\n",
    "                except Exception as e:\n",
    "                    log.write(f\"Failed to remove {file_path}: {e}\\n\")\n",
    "            else:\n",
    "                total_retained += 1\n",
    "                log.write(f\"Retained: {file_path} (Extension: {true_extension})\\n\")\n",
    "\n",
    "    return total_removed, total_retained\n",
    "\n",
    "def process_chromium_folder(base_dir):\n",
    "    total_files_removed = 0\n",
    "    total_files_retained = 0\n",
    "    total_commits = 0\n",
    "\n",
    "    with open(LOG_FILE, 'w') as log:\n",
    "        log.write(f\"Chromium Cleanup Log\\n\")\n",
    "        log.write(\"=\" * 50 + \"\\n\\n\")\n",
    "\n",
    "        for commit_dir in os.listdir(base_dir):\n",
    "            commit_path = os.path.join(base_dir, commit_dir)\n",
    "            if not os.path.isdir(commit_path):\n",
    "                continue\n",
    "\n",
    "            total_commits += 1\n",
    "            log.write(f\"Processing Commit Directory: {commit_path}\\n\")\n",
    "            log.write(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "            removed, retained = clean_non_c_files_in_directory(commit_path, log)\n",
    "\n",
    "            total_files_removed += removed\n",
    "            total_files_retained += retained\n",
    "\n",
    "            log.write(f\"Summary for {commit_path}:\\n\")\n",
    "            log.write(f\"  Files Removed: {removed}\\n\")\n",
    "            log.write(f\"  Files Retained: {retained}\\n\")\n",
    "            log.write(\"-\" * 50 + \"\\n\\n\")\n",
    "\n",
    "        log.write(\"\\n\")\n",
    "        log.write(\"=\" * 50 + \"\\n\")\n",
    "        log.write(f\"Total Commit Directories Processed: {total_commits}\\n\")\n",
    "        log.write(f\"Total Files Removed: {total_files_removed}\\n\")\n",
    "        log.write(f\"Total Files Retained: {total_files_retained}\\n\")\n",
    "\n",
    "    print(f\"Total Commit Directories Processed: {total_commits}\")\n",
    "    print(f\"Total Files Removed: {total_files_removed}\")\n",
    "    print(f\"Total Files Retained: {total_files_retained}\")\n",
    "    print(f\"Detailed log saved at: {LOG_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_chromium_folder(BASE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5854b2d7",
   "metadata": {},
   "source": [
    "# removing all non c files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39ca53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "BASE_DIR = r\"M:\\FULL_DATA_COLLECTED\"\n",
    "\n",
    "C_RELATED_EXTENSIONS = {'c', 'h', 'cc', 'cpp', 'hpp', 'cxx', 'hxx', 'ino', 'hh', 'C'}\n",
    "\n",
    "LOG_FILE = os.path.join(BASE_DIR, 'cleanup_log_for_all_projects.txt')\n",
    "\n",
    "def extract_true_extension(file_name):\n",
    "    if \".\" in file_name:\n",
    "        first_dot_index = file_name.find(\".\")\n",
    "        underscore_index = file_name.find(\"_\", first_dot_index)\n",
    "        if underscore_index != -1:\n",
    "            return file_name[first_dot_index + 1:underscore_index]\n",
    "    return \"No Extension\"\n",
    "\n",
    "def clean_non_c_files_in_directory(directory, log):\n",
    "    total_removed = 0\n",
    "    total_retained = 0\n",
    "\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            true_extension = extract_true_extension(file)\n",
    "\n",
    "            if true_extension not in C_RELATED_EXTENSIONS:\n",
    "                try:\n",
    "                    os.remove(file_path)\n",
    "                    total_removed += 1\n",
    "                    log.write(f\"Removed: {file_path} (Extension: {true_extension})\\n\")\n",
    "                except Exception as e:\n",
    "                    log.write(f\"Failed to remove {file_path}: {e}\\n\")\n",
    "            else:\n",
    "                total_retained += 1\n",
    "                log.write(f\"Retained: {file_path} (Extension: {true_extension})\\n\")\n",
    "\n",
    "    return total_removed, total_retained\n",
    "\n",
    "def process_all_projects(base_dir):\n",
    "    total_files_removed = 0\n",
    "    total_files_retained = 0\n",
    "    total_projects = 0\n",
    "\n",
    "    with open(LOG_FILE, 'w') as log:\n",
    "        log.write(f\"Cleanup Log for All Projects\\n\")\n",
    "        log.write(\"=\" * 50 + \"\\n\\n\")\n",
    "\n",
    "        for project_dir in os.listdir(base_dir):\n",
    "            project_path = os.path.join(base_dir, project_dir)\n",
    "            if not os.path.isdir(project_path):\n",
    "                continue\n",
    "\n",
    "            total_projects += 1\n",
    "            log.write(f\"Processing Project Directory: {project_path}\\n\")\n",
    "            log.write(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "            for commit_dir in os.listdir(project_path):\n",
    "                commit_path = os.path.join(project_path, commit_dir)\n",
    "                if not os.path.isdir(commit_path):\n",
    "                    continue\n",
    "\n",
    "                log.write(f\"Processing Commit Directory: {commit_path}\\n\")\n",
    "                log.write(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "                removed, retained = clean_non_c_files_in_directory(commit_path, log)\n",
    "\n",
    "                total_files_removed += removed\n",
    "                total_files_retained += retained\n",
    "\n",
    "                log.write(f\"Summary for {commit_path}:\\n\")\n",
    "                log.write(f\"  Files Removed: {removed}\\n\")\n",
    "                log.write(f\"  Files Retained: {retained}\\n\")\n",
    "                log.write(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "            log.write(f\"Finished Processing Project Directory: {project_path}\\n\\n\")\n",
    "\n",
    "        log.write(\"\\n\")\n",
    "        log.write(\"=\" * 50 + \"\\n\")\n",
    "        log.write(f\"Total Projects Processed: {total_projects}\\n\")\n",
    "        log.write(f\"Total Files Removed: {total_files_removed}\\n\")\n",
    "        log.write(f\"Total Files Retained: {total_files_retained}\\n\")\n",
    "\n",
    "    print(f\"Total Projects Processed: {total_projects}\")\n",
    "    print(f\"Total Files Removed: {total_files_removed}\")\n",
    "    print(f\"Total Files Retained: {total_files_retained}\")\n",
    "    print(f\"Detailed log saved at: {LOG_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_all_projects(BASE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e906fb",
   "metadata": {},
   "source": [
    "# file per commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e7f26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "data_path = r\"M:\\FULL_DATA_COLLECTED\"\n",
    "\n",
    "project_stats = {}\n",
    "files_changed_per_commit = defaultdict(int)\n",
    "file_count_distribution = defaultdict(int)\n",
    "\n",
    "def extract_file_name_and_extension(file_name):\n",
    "    \"\"\"\n",
    "    Extract the full file name including extension.\n",
    "    - File name: base name + extension (e.g., bookmark_manager_view.h)\n",
    "    - Extension: part after the first '.' and before the first '_'.\n",
    "    \"\"\"\n",
    "    if \".\" in file_name and \"_\" in file_name:\n",
    "        first_dot = file_name.find(\".\")\n",
    "        after_dot = file_name[first_dot + 1:]\n",
    "        first_underscore = after_dot.find(\"_\")\n",
    "        if first_underscore != -1:\n",
    "            extension = after_dot[:first_underscore]\n",
    "            base_name_with_ext = file_name[:first_dot + 1] + extension\n",
    "            return base_name_with_ext, extension\n",
    "    return file_name, \"No Extension\"\n",
    "\n",
    "for project_name in os.listdir(data_path):\n",
    "    project_path = os.path.join(data_path, project_name)\n",
    "    if not os.path.isdir(project_path):\n",
    "        continue\n",
    "\n",
    "    total_commits = 0\n",
    "    for commit_dir in os.listdir(project_path):\n",
    "        commit_path = os.path.join(project_path, commit_dir)\n",
    "        if not os.path.isdir(commit_path):\n",
    "            continue\n",
    "\n",
    "        total_commits += 1\n",
    "        all_versions_path = os.path.join(commit_path, 'all_versions')\n",
    "\n",
    "        if os.path.exists(all_versions_path):\n",
    "            unique_files = set()\n",
    "            files_in_commit = os.listdir(all_versions_path)\n",
    "\n",
    "            for file in files_in_commit:\n",
    "                full_file_name, extension = extract_file_name_and_extension(file)\n",
    "                unique_files.add(full_file_name)\n",
    "\n",
    "            file_count = len(unique_files)\n",
    "            files_changed_per_commit[commit_dir] = file_count\n",
    "\n",
    "            file_count_distribution[file_count] += 1\n",
    "\n",
    "    project_stats[project_name] = total_commits\n",
    "\n",
    "output_file = \"commit_file_statistics_corrected_v3.xlsx\"\n",
    "\n",
    "commit_data = pd.DataFrame({\n",
    "    \"Commit\": list(files_changed_per_commit.keys()),\n",
    "    \"Unique Files Changed\": list(files_changed_per_commit.values())\n",
    "})\n",
    "commit_data.to_excel(output_file, index=False)\n",
    "\n",
    "total_commits_processed = sum(file_count_distribution.values())\n",
    "distribution_summary = {\n",
    "    \"1 file\": file_count_distribution[1],\n",
    "    \"2 files\": file_count_distribution[2],\n",
    "    \"3 files\": file_count_distribution[3],\n",
    "    \"4 files\": file_count_distribution[4],\n",
    "    \"5 files\": file_count_distribution[5],\n",
    "    \"6 files\": file_count_distribution[6],\n",
    "    \"7 files\": file_count_distribution[7],\n",
    "    \"8 files\": file_count_distribution[8],\n",
    "    \"9 files\": file_count_distribution[9],\n",
    "    \"10 files\": file_count_distribution[10],\n",
    "    \"More than 10 files\": sum(v for k, v in file_count_distribution.items() if k > 10),\n",
    "}\n",
    "\n",
    "print(f\"Total number of commits processed: {total_commits_processed}\")\n",
    "for files_changed, count in distribution_summary.items():\n",
    "    print(f\"Commits with {files_changed}: {count}\")\n",
    "\n",
    "print(f\"\\nStatistics saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c87557",
   "metadata": {},
   "source": [
    "# get distinct extensions of the commits those have 2 commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a272afee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_path = r\"M:\\FULL_DATA_COLLECTED\"\n",
    "\n",
    "def extract_file_name_and_extension(file_name):\n",
    "    \"\"\"\n",
    "    Extract the full file name including extension.\n",
    "    - File name: base name + extension (e.g., bookmark_manager_view.h)\n",
    "    - Extension: part after the first '.' and before the first '_'.\n",
    "    \"\"\"\n",
    "    if \".\" in file_name and \"_\" in file_name:\n",
    "        first_dot = file_name.find(\".\")\n",
    "        after_dot = file_name[first_dot + 1:]\n",
    "        first_underscore = after_dot.find(\"_\")\n",
    "        if first_underscore != -1:\n",
    "            extension = after_dot[:first_underscore]\n",
    "            base_name_with_ext = file_name[:first_dot + 1] + extension\n",
    "            return base_name_with_ext, extension\n",
    "    return file_name, \"No Extension\"\n",
    "\n",
    "two_file_commit_extensions = []\n",
    "\n",
    "for project_name in os.listdir(data_path):\n",
    "    project_path = os.path.join(data_path, project_name)\n",
    "    if not os.path.isdir(project_path):\n",
    "        continue\n",
    "\n",
    "    for commit_dir in os.listdir(project_path):\n",
    "        commit_path = os.path.join(project_path, commit_dir)\n",
    "        if not os.path.isdir(commit_path):\n",
    "            continue\n",
    "\n",
    "        all_versions_path = os.path.join(commit_path, 'all_versions')\n",
    "\n",
    "        if os.path.exists(all_versions_path):\n",
    "            unique_files = set()\n",
    "            extensions_in_commit = set()\n",
    "            files_in_commit = os.listdir(all_versions_path)\n",
    "\n",
    "            for file in files_in_commit:\n",
    "                full_file_name, extension = extract_file_name_and_extension(file)\n",
    "                unique_files.add(full_file_name)\n",
    "                extensions_in_commit.add(extension)\n",
    "\n",
    "            if len(unique_files) == 2:\n",
    "                two_file_commit_extensions.append({\n",
    "                    \"Project\": project_name,\n",
    "                    \"Commit\": commit_dir,\n",
    "                    \"Extensions\": \", \".join(extensions_in_commit)\n",
    "                })\n",
    "\n",
    "output_file = \"two_file_commit_extensions_per_commit.xlsx\"\n",
    "\n",
    "two_file_commit_data = pd.DataFrame(two_file_commit_extensions)\n",
    "two_file_commit_data.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Total commits with exactly two files: {len(two_file_commit_extensions)}\")\n",
    "print(f\"\\nExtensions per commit saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9173800f",
   "metadata": {},
   "source": [
    "# get  extensions of the commits those have 1 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b123b343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_path = r\"M:\\FULL_DATA_COLLECTED\"\n",
    "\n",
    "def extract_file_name_and_extension(file_name):\n",
    "    \"\"\"\n",
    "    Extract the full file name including extension.\n",
    "    - File name: base name + extension (e.g., bookmark_manager_view.h)\n",
    "    - Extension: part after the first '.' and before the first '_'.\n",
    "    \"\"\"\n",
    "    if \".\" in file_name and \"_\" in file_name:\n",
    "        first_dot = file_name.find(\".\")\n",
    "        after_dot = file_name[first_dot + 1:]\n",
    "        first_underscore = after_dot.find(\"_\")\n",
    "        if first_underscore != -1:\n",
    "            extension = after_dot[:first_underscore]\n",
    "            base_name_with_ext = file_name[:first_dot + 1] + extension\n",
    "            return base_name_with_ext, extension\n",
    "    return file_name, \"No Extension\"\n",
    "\n",
    "one_file_commit_extensions = []\n",
    "\n",
    "for project_name in os.listdir(data_path):\n",
    "    project_path = os.path.join(data_path, project_name)\n",
    "    if not os.path.isdir(project_path):\n",
    "        continue\n",
    "\n",
    "    for commit_dir in os.listdir(project_path):\n",
    "        commit_path = os.path.join(project_path, commit_dir)\n",
    "        if not os.path.isdir(commit_path):\n",
    "            continue\n",
    "\n",
    "        all_versions_path = os.path.join(commit_path, 'all_versions')\n",
    "\n",
    "        if os.path.exists(all_versions_path):\n",
    "            unique_files = set()\n",
    "            extensions_in_commit = set()\n",
    "            files_in_commit = os.listdir(all_versions_path)\n",
    "\n",
    "            for file in files_in_commit:\n",
    "                full_file_name, extension = extract_file_name_and_extension(file)\n",
    "                unique_files.add(full_file_name)\n",
    "                extensions_in_commit.add(extension)\n",
    "\n",
    "            if len(unique_files) == 1:\n",
    "                one_file_commit_extensions.append({\n",
    "                    \"Project\": project_name,\n",
    "                    \"Commit\": commit_dir,\n",
    "                    \"Extension\": list(extensions_in_commit)[0]\n",
    "                })\n",
    "\n",
    "output_file = \"one_file_commit_extensions_per_commit.xlsx\"\n",
    "\n",
    "one_file_commit_data = pd.DataFrame(one_file_commit_extensions)\n",
    "one_file_commit_data.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Total commits with exactly one file: {len(one_file_commit_extensions)}\")\n",
    "print(f\"\\nExtensions per commit saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e21fa0",
   "metadata": {},
   "source": [
    "# total empty folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163793a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "BASE_DIR = r\"M:\\FULL_DATA_COLLECTED\"\n",
    "\n",
    "EMPTY_FOLDERS_LOG = os.path.join(BASE_DIR, 'empty_folders_log.txt')\n",
    "\n",
    "def find_empty_folders(base_dir):\n",
    "    total_empty_folders = 0\n",
    "    empty_folders_list = []\n",
    "\n",
    "    with open(EMPTY_FOLDERS_LOG, 'w') as log:\n",
    "        log.write(f\"Empty Folders Log\\n\")\n",
    "        log.write(\"=\" * 50 + \"\\n\\n\")\n",
    "\n",
    "        for root, dirs, files in os.walk(base_dir):\n",
    "            for folder in dirs:\n",
    "                folder_path = os.path.join(root, folder)\n",
    "\n",
    "                if not os.listdir(folder_path):\n",
    "                    total_empty_folders += 1\n",
    "                    empty_folders_list.append(folder_path)\n",
    "                    log.write(f\"Empty Folder: {folder_path}\\n\")\n",
    "\n",
    "        log.write(\"\\n\")\n",
    "        log.write(\"=\" * 50 + \"\\n\")\n",
    "        log.write(f\"Total Empty Folders Found: {total_empty_folders}\\n\")\n",
    "\n",
    "    print(f\"Total Empty Folders Found: {total_empty_folders}\")\n",
    "    print(f\"Detailed log saved at: {EMPTY_FOLDERS_LOG}\")\n",
    "\n",
    "    return empty_folders_list\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    empty_folders = find_empty_folders(BASE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b112a859",
   "metadata": {},
   "source": [
    "# Remove empty folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640d878c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "BASE_DIR = r\"M:\\FULL_DATA_COLLECTED\"\n",
    "\n",
    "REMOVED_FOLDERS_LOG = os.path.join(BASE_DIR, 'removed_empty_folders_log.txt')\n",
    "\n",
    "def remove_empty_folders(base_dir):\n",
    "    total_removed_folders = 0\n",
    "\n",
    "    with open(REMOVED_FOLDERS_LOG, 'w') as log:\n",
    "        log.write(f\"Removed Empty Folders Log\\n\")\n",
    "        log.write(\"=\" * 50 + \"\\n\\n\")\n",
    "\n",
    "        for root, dirs, files in os.walk(base_dir, topdown=False):\n",
    "            for folder in dirs:\n",
    "                folder_path = os.path.join(root, folder)\n",
    "\n",
    "                if not os.listdir(folder_path):\n",
    "                    try:\n",
    "                        os.rmdir(folder_path)\n",
    "                        total_removed_folders += 1\n",
    "                        log.write(f\"Removed Empty Folder: {folder_path}\\n\")\n",
    "                    except Exception as e:\n",
    "                        log.write(f\"Failed to remove {folder_path}: {e}\\n\")\n",
    "\n",
    "        log.write(\"\\n\")\n",
    "        log.write(\"=\" * 50 + \"\\n\")\n",
    "        log.write(f\"Total Empty Folders Removed: {total_removed_folders}\\n\")\n",
    "\n",
    "    print(f\"Total Empty Folders Removed: {total_removed_folders}\")\n",
    "    print(f\"Detailed log saved at: {REMOVED_FOLDERS_LOG}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    remove_empty_folders(BASE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ae9930",
   "metadata": {},
   "source": [
    "# total commit per project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe8e255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "BASE_DIR = r\"M:\\FULL_DATA_COLLECTED\"\n",
    "\n",
    "COMMIT_STATS_LOG = os.path.join(BASE_DIR, 'commit_statistics_log.txt')\n",
    "\n",
    "def count_commits(base_dir):\n",
    "    total_commits = 0\n",
    "    project_stats = {}\n",
    "\n",
    "    with open(COMMIT_STATS_LOG, 'w') as log:\n",
    "        log.write(f\"Commit Statistics Log\\n\")\n",
    "        log.write(\"=\" * 50 + \"\\n\\n\")\n",
    "\n",
    "        for project_name in os.listdir(base_dir):\n",
    "            project_path = os.path.join(base_dir, project_name)\n",
    "            if not os.path.isdir(project_path):\n",
    "                continue\n",
    "\n",
    "            commit_count = sum(1 for entry in os.listdir(project_path) if os.path.isdir(os.path.join(project_path, entry)))\n",
    "            project_stats[project_name] = commit_count\n",
    "            total_commits += commit_count\n",
    "\n",
    "            log.write(f\"Project: {project_name}, Commits: {commit_count}\\n\")\n",
    "\n",
    "        log.write(\"\\n\")\n",
    "        log.write(\"=\" * 50 + \"\\n\")\n",
    "        log.write(f\"Total Projects Processed: {len(project_stats)}\\n\")\n",
    "        log.write(f\"Total Commits: {total_commits}\\n\")\n",
    "\n",
    "    print(f\"Total Projects Processed: {len(project_stats)}\")\n",
    "    print(f\"Total Commits: {total_commits}\")\n",
    "    print(f\"Detailed log saved at: {COMMIT_STATS_LOG}\")\n",
    "\n",
    "    return total_commits, project_stats\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    total_commits, project_stats = count_commits(BASE_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa50e28c",
   "metadata": {},
   "source": [
    "# total projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d978b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_path = r\"M:\\FULL_DATA_COLLECTED\"\n",
    "\n",
    "def count_projects(data_path):\n",
    "    project_count = 0\n",
    "    for item in os.listdir(data_path):\n",
    "        item_path = os.path.join(data_path, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            project_count += 1\n",
    "    return project_count\n",
    "\n",
    "total_projects = count_projects(data_path)\n",
    "\n",
    "print(f\"Total number of projects: {total_projects}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d81eecc",
   "metadata": {},
   "source": [
    "# filename containing _test or test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6df6f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_path = r\"M:\\FULL_DATA_COLLECTED\"\n",
    "\n",
    "def extract_file_name_and_extension(file_name):\n",
    "    \"\"\"\n",
    "    Extract the full file name including extension.\n",
    "    - File name: base name + extension (e.g., bookmark_manager_view.h)\n",
    "    - Extension: part after the first '.' and before the first '_'.\n",
    "    \"\"\"\n",
    "    if \".\" in file_name and \"_\" in file_name:\n",
    "        first_dot = file_name.find(\".\")\n",
    "        after_dot = file_name[first_dot + 1:]\n",
    "        first_underscore = after_dot.find(\"_\")\n",
    "        if first_underscore != -1:\n",
    "            extension = after_dot[:first_underscore]\n",
    "            base_name_with_ext = file_name[:first_dot + 1] + extension\n",
    "            return base_name_with_ext, extension\n",
    "    return file_name, \"No Extension\"\n",
    "\n",
    "test_file_commits = []\n",
    "\n",
    "for project_name in os.listdir(data_path):\n",
    "    project_path = os.path.join(data_path, project_name)\n",
    "    if not os.path.isdir(project_path):\n",
    "        continue\n",
    "\n",
    "    for commit_dir in os.listdir(project_path):\n",
    "        commit_path = os.path.join(project_path, commit_dir)\n",
    "        if not os.path.isdir(commit_path):\n",
    "            continue\n",
    "\n",
    "        all_versions_path = os.path.join(commit_path, 'all_versions')\n",
    "\n",
    "        if os.path.exists(all_versions_path):\n",
    "            test_files_in_commit = []\n",
    "            files_in_commit = os.listdir(all_versions_path)\n",
    "\n",
    "            for file in files_in_commit:\n",
    "                if \"_test\" in file or \"test_\" in file:\n",
    "                    test_files_in_commit.append(file)\n",
    "\n",
    "            if test_files_in_commit:\n",
    "                test_file_commits.append({\n",
    "                    \"Project\": project_name,\n",
    "                    \"Commit\": commit_path,\n",
    "                    \"Matching Files\": \", \".join(test_files_in_commit)\n",
    "                })\n",
    "\n",
    "output_file = \"all_commits_with_test_files.xlsx\"\n",
    "\n",
    "test_file_commit_data = pd.DataFrame(test_file_commits)\n",
    "test_file_commit_data.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Total commits with '_test' or 'test_' in file names: {len(test_file_commits)}\")\n",
    "print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd1907f",
   "metadata": {},
   "source": [
    "# number of _test or test_ in commits where 1 file changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8597781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_path = r\"M:\\FULL_DATA_COLLECTED\"\n",
    "\n",
    "def extract_file_name_and_extension(file_name):\n",
    "    \"\"\"\n",
    "    Extract the full file name including extension.\n",
    "    - File name: base name + extension (e.g., bookmark_manager_view.h)\n",
    "    - Extension: part after the first '.' and before the first '_'.\n",
    "    \"\"\"\n",
    "    if \".\" in file_name and \"_\" in file_name:\n",
    "        first_dot = file_name.find(\".\")\n",
    "        after_dot = file_name[first_dot + 1:]\n",
    "        first_underscore = after_dot.find(\"_\")\n",
    "        if first_underscore != -1:\n",
    "            extension = after_dot[:first_underscore]\n",
    "            base_name_with_ext = file_name[:first_dot + 1] + extension\n",
    "            return base_name_with_ext, extension\n",
    "    return file_name, \"No Extension\"\n",
    "\n",
    "test_one_file_commits = []\n",
    "\n",
    "for project_name in os.listdir(data_path):\n",
    "    project_path = os.path.join(data_path, project_name)\n",
    "    if not os.path.isdir(project_path):\n",
    "        continue\n",
    "\n",
    "    for commit_dir in os.listdir(project_path):\n",
    "        commit_path = os.path.join(project_path, commit_dir)\n",
    "        if not os.path.isdir(commit_path):\n",
    "            continue\n",
    "\n",
    "        all_versions_path = os.path.join(commit_path, 'all_versions')\n",
    "\n",
    "        if os.path.exists(all_versions_path):\n",
    "            unique_files = set()\n",
    "            files_in_commit = os.listdir(all_versions_path)\n",
    "            test_file_found = False\n",
    "\n",
    "            for file in files_in_commit:\n",
    "                full_file_name, _ = extract_file_name_and_extension(file)\n",
    "                unique_files.add(full_file_name)\n",
    "\n",
    "                if \"_test\" in file or \"test_\" in file:\n",
    "                    test_file_found = True\n",
    "\n",
    "            if len(unique_files) == 1 and test_file_found:\n",
    "                test_one_file_commits.append({\n",
    "                    \"Project\": project_name,\n",
    "                    \"Commit\": commit_dir,\n",
    "                    \"File Name\": list(unique_files)[0]\n",
    "                })\n",
    "\n",
    "output_file = \"one_file_commits_with_test.xlsx\"\n",
    "\n",
    "test_one_file_commit_data = pd.DataFrame(test_one_file_commits)\n",
    "test_one_file_commit_data.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Total one-file commits with '_test' or 'test_' in the file name: {len(test_one_file_commits)}\")\n",
    "print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7f7598",
   "metadata": {},
   "source": [
    "# number of test where 2 files are changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f3ced3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_path = r\"M:\\FULL_DATA_COLLECTED\"\n",
    "\n",
    "def extract_file_name_and_extension(file_name):\n",
    "    \"\"\"\n",
    "    Extract the full file name including extension.\n",
    "    - File name: base name + extension (e.g., bookmark_manager_view.h)\n",
    "    - Extension: part after the first '.' and before the first '_'.\n",
    "    \"\"\"\n",
    "    if \".\" in file_name and \"_\" in file_name:\n",
    "        first_dot = file_name.find(\".\")\n",
    "        after_dot = file_name[first_dot + 1:]\n",
    "        first_underscore = after_dot.find(\"_\")\n",
    "        if first_underscore != -1:\n",
    "            extension = after_dot[:first_underscore]\n",
    "            base_name_with_ext = file_name[:first_dot + 1] + extension\n",
    "            return base_name_with_ext, extension\n",
    "    return file_name, \"No Extension\"\n",
    "\n",
    "two_file_test_commits = []\n",
    "\n",
    "for project_name in os.listdir(data_path):\n",
    "    project_path = os.path.join(data_path, project_name)\n",
    "    if not os.path.isdir(project_path):\n",
    "        continue\n",
    "\n",
    "    for commit_dir in os.listdir(project_path):\n",
    "        commit_path = os.path.join(project_path, commit_dir)\n",
    "        if not os.path.isdir(commit_path):\n",
    "            continue\n",
    "\n",
    "        all_versions_path = os.path.join(commit_path, 'all_versions')\n",
    "\n",
    "        if os.path.exists(all_versions_path):\n",
    "            unique_files = set()\n",
    "            test_files = []\n",
    "            files_in_commit = os.listdir(all_versions_path)\n",
    "\n",
    "            for file in files_in_commit:\n",
    "                full_file_name, _ = extract_file_name_and_extension(file)\n",
    "                unique_files.add(full_file_name)\n",
    "\n",
    "                if \"_test\" in file or \"test_\" in file:\n",
    "                    test_files.append(file)\n",
    "\n",
    "            if len(unique_files) == 2 and len(test_files) == 2:\n",
    "                two_file_test_commits.append({\n",
    "                    \"Project\": project_name,\n",
    "                    \"Commit\": commit_dir,\n",
    "                    \"File 1\": test_files[0],\n",
    "                    \"File 2\": test_files[1]\n",
    "                })\n",
    "\n",
    "output_file = \"two_file_commits_with_test.xlsx\"\n",
    "\n",
    "two_file_test_commit_data = pd.DataFrame(two_file_test_commits)\n",
    "two_file_test_commit_data.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Total two-file commits with both files containing '_test' or 'test_': {len(two_file_test_commits)}\")\n",
    "print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dbc01a",
   "metadata": {},
   "source": [
    "# if no future version after fixed version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde47f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "data_path = r\"M:\\FULL_DATA_COLLECTED\"\n",
    "output_file = \"files_without_future_versions.csv\"\n",
    "\n",
    "total_files_checked = 0\n",
    "files_without_future_versions = 0\n",
    "files_without_future_versions_data = []\n",
    "\n",
    "def extract_version_number(file_name):\n",
    "    \"\"\"\n",
    "    Extract the version number from a filename if '_v' is followed by digits.\n",
    "    Returns the version number as an integer or None if no valid version is found.\n",
    "    \"\"\"\n",
    "    if \"_v\" in file_name:\n",
    "        try:\n",
    "            version_part = file_name.split(\"_v\")[1]\n",
    "            version_number = int(version_part.split(\"_\")[0])\n",
    "            return version_number\n",
    "        except (IndexError, ValueError):\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "for project_name in os.listdir(data_path):\n",
    "    project_path = os.path.join(data_path, project_name)\n",
    "    if not os.path.isdir(project_path):\n",
    "        continue\n",
    "\n",
    "    for commit_dir in os.listdir(project_path):\n",
    "        commit_path = os.path.join(project_path, commit_dir)\n",
    "        if not os.path.isdir(commit_path):\n",
    "            continue\n",
    "\n",
    "        all_versions_path = os.path.join(commit_path, 'all_versions')\n",
    "\n",
    "        if os.path.exists(all_versions_path):\n",
    "            files_in_commit = os.listdir(all_versions_path)\n",
    "            unique_files = {}\n",
    "\n",
    "            for file in files_in_commit:\n",
    "                if \"_v\" in file:\n",
    "                    base_name = file.split(\"_v\")[0]\n",
    "                    if base_name not in unique_files:\n",
    "                        unique_files[base_name] = []\n",
    "                    unique_files[base_name].append(file)\n",
    "\n",
    "            for base_name, versions in unique_files.items():\n",
    "                total_files_checked += 1\n",
    "\n",
    "                versioned_files = [\n",
    "                    (file, extract_version_number(file))\n",
    "                    for file in versions\n",
    "                ]\n",
    "                versioned_files = [\n",
    "                    (file, version)\n",
    "                    for file, version in versioned_files\n",
    "                    if version is not None\n",
    "                ]\n",
    "\n",
    "                if not versioned_files:\n",
    "                    continue\n",
    "\n",
    "                sorted_versions = sorted(versioned_files, key=lambda x: x[1])\n",
    "\n",
    "                has_future_versions = False\n",
    "                for idx, (file, version) in enumerate(sorted_versions):\n",
    "                    if \"fixed_version\" in file:\n",
    "                        if idx < len(sorted_versions) - 1:\n",
    "                            has_future_versions = True\n",
    "                        break\n",
    "\n",
    "                if not has_future_versions:\n",
    "                    files_without_future_versions += 1\n",
    "                    files_without_future_versions_data.append({\n",
    "                        \"Project\": project_name,\n",
    "                        \"Commit\": commit_dir,\n",
    "                        \"File Path\": os.path.join(all_versions_path, sorted_versions[-1][0])\n",
    "                    })\n",
    "\n",
    "with open(output_file, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=[\"Project\", \"Commit\", \"File Path\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(files_without_future_versions_data)\n",
    "\n",
    "print(f\"Total files checked: {total_files_checked}\")\n",
    "print(f\"Files without future versions after the fixed version: {files_without_future_versions}\")\n",
    "print(f\"Paths saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5f8765",
   "metadata": {},
   "source": [
    "# remove if not future version .. works only for yodl project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cca9091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "project_path = r\"M:\\FULL_DATA_COLLECTED\\yodl\"\n",
    "log_file = os.path.join(project_path, \"removed_files_log.csv\")\n",
    "\n",
    "def extract_version_number(file_name):\n",
    "    \"\"\"\n",
    "    Extract the version number from a filename if '_v' is followed by digits.\n",
    "    Returns the version number as an integer or None if no valid version is found.\n",
    "    \"\"\"\n",
    "    if \"_v\" in file_name:\n",
    "        try:\n",
    "            version_part = file_name.split(\"_v\")[1]\n",
    "            version_number = int(version_part.split(\"_\")[0])\n",
    "            return version_number\n",
    "        except (IndexError, ValueError):\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def remove_file(file_path, commit_dir, folder_name, log):\n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            os.remove(file_path)\n",
    "            log.append({\"Commit\": commit_dir, \"File Path\": file_path, \"Folder\": folder_name})\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to remove {file_path}: {e}\")\n",
    "\n",
    "removed_files_log = []\n",
    "\n",
    "for commit_dir in os.listdir(project_path):\n",
    "    commit_path = os.path.join(project_path, commit_dir)\n",
    "    if not os.path.isdir(commit_path):\n",
    "        continue\n",
    "\n",
    "    all_versions_path = os.path.join(commit_path, \"all_versions\")\n",
    "    commit_messages_path = os.path.join(commit_path, \"commit_messages\")\n",
    "    file_changes_path = os.path.join(commit_path, \"file_changes_in_versions\")\n",
    "\n",
    "    if os.path.exists(all_versions_path):\n",
    "        files_in_commit = os.listdir(all_versions_path)\n",
    "        unique_files = {}\n",
    "\n",
    "        for file in files_in_commit:\n",
    "            if \"_v\" in file:\n",
    "                base_name = file.split(\"_v\")[0]\n",
    "                if base_name not in unique_files:\n",
    "                    unique_files[base_name] = []\n",
    "                unique_files[base_name].append(file)\n",
    "\n",
    "        for base_name, versions in unique_files.items():\n",
    "            versioned_files = [\n",
    "                (file, extract_version_number(file))\n",
    "                for file in versions\n",
    "            ]\n",
    "            versioned_files = [\n",
    "                (file, version)\n",
    "                for file, version in versioned_files\n",
    "                if version is not None\n",
    "            ]\n",
    "\n",
    "            if not versioned_files:\n",
    "                continue\n",
    "\n",
    "            sorted_versions = sorted(versioned_files, key=lambda x: x[1])\n",
    "\n",
    "            has_future_versions = False\n",
    "            for idx, (file, version) in enumerate(sorted_versions):\n",
    "                if \"fixed_version\" in file:\n",
    "                    if idx < len(sorted_versions) - 1:\n",
    "                        has_future_versions = True\n",
    "                    break\n",
    "\n",
    "            if not has_future_versions:\n",
    "                for file, _ in versioned_files:\n",
    "                    file_path = os.path.join(all_versions_path, file)\n",
    "                    remove_file(file_path, commit_dir, \"all_versions\", removed_files_log)\n",
    "\n",
    "    if os.path.exists(commit_messages_path):\n",
    "        for file in os.listdir(commit_messages_path):\n",
    "            if any(base_name in file for base_name in unique_files):\n",
    "                file_path = os.path.join(commit_messages_path, file)\n",
    "                remove_file(file_path, commit_dir, \"commit_messages\", removed_files_log)\n",
    "\n",
    "    if os.path.exists(file_changes_path):\n",
    "        for file in os.listdir(file_changes_path):\n",
    "            if any(base_name in file for base_name in unique_files):\n",
    "                file_path = os.path.join(file_changes_path, file)\n",
    "                remove_file(file_path, commit_dir, \"file_changes_in_versions\", removed_files_log)\n",
    "\n",
    "with open(log_file, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=[\"Commit\", \"File Path\", \"Folder\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(removed_files_log)\n",
    "\n",
    "print(f\"Cleanup completed for project 'yodl'.\")\n",
    "print(f\"Log of removed files saved to: {log_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2073e1ec",
   "metadata": {},
   "source": [
    "# chromium 1 commit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce979cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "commit_path = r\"M:\\FULL_DATA_COLLECTED\\chromium\\chromium_4ab22cfc619ee8ff17a8c50e289ec3b30731ceba\"\n",
    "log_file = os.path.join(commit_path, \"removed_files_log.csv\")\n",
    "\n",
    "def extract_version_number(file_name):\n",
    "    \"\"\"\n",
    "    Extract the version number from a filename if '_v' is followed by digits.\n",
    "    Returns the version number as an integer or None if no valid version is found.\n",
    "    \"\"\"\n",
    "    if \"_v\" in file_name:\n",
    "        try:\n",
    "            version_part = file_name.split(\"_v\")[1]\n",
    "            version_number = int(version_part.split(\"_\")[0])\n",
    "            return version_number\n",
    "        except (IndexError, ValueError):\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def remove_file(file_path, folder_name, log):\n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            os.remove(file_path)\n",
    "            log.append({\"File Path\": file_path, \"Folder\": folder_name})\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to remove {file_path}: {e}\")\n",
    "\n",
    "removed_files_log = []\n",
    "\n",
    "all_versions_path = os.path.join(commit_path, \"all_versions\")\n",
    "commit_messages_path = os.path.join(commit_path, \"commit_messages\")\n",
    "file_changes_path = os.path.join(commit_path, \"file_changes_in_versions\")\n",
    "\n",
    "files_without_future_versions = set()\n",
    "if os.path.exists(all_versions_path):\n",
    "    files_in_commit = os.listdir(all_versions_path)\n",
    "    unique_files = {}\n",
    "\n",
    "    for file in files_in_commit:\n",
    "        if \"_v\" in file:\n",
    "            base_name = file.split(\"_v\")[0]\n",
    "            if base_name not in unique_files:\n",
    "                unique_files[base_name] = []\n",
    "            unique_files[base_name].append(file)\n",
    "\n",
    "    for base_name, versions in unique_files.items():\n",
    "        versioned_files = [\n",
    "            (file, extract_version_number(file))\n",
    "            for file in versions\n",
    "        ]\n",
    "        versioned_files = [\n",
    "            (file, version)\n",
    "            for file, version in versioned_files\n",
    "            if version is not None\n",
    "        ]\n",
    "\n",
    "        if not versioned_files:\n",
    "            continue\n",
    "\n",
    "        sorted_versions = sorted(versioned_files, key=lambda x: x[1])\n",
    "\n",
    "        has_future_versions = False\n",
    "        for idx, (file, version) in enumerate(sorted_versions):\n",
    "            if \"fixed_version\" in file:\n",
    "                if idx < len(sorted_versions) - 1:\n",
    "                    has_future_versions = True\n",
    "                break\n",
    "\n",
    "        if not has_future_versions:\n",
    "            for file, _ in versioned_files:\n",
    "                file_path = os.path.join(all_versions_path, file)\n",
    "                remove_file(file_path, \"all_versions\", removed_files_log)\n",
    "                files_without_future_versions.add(base_name)\n",
    "\n",
    "if os.path.exists(commit_messages_path):\n",
    "    for file in os.listdir(commit_messages_path):\n",
    "        if any(base_name in file for base_name in files_without_future_versions):\n",
    "            file_path = os.path.join(commit_messages_path, file)\n",
    "            remove_file(file_path, \"commit_messages\", removed_files_log)\n",
    "\n",
    "if os.path.exists(file_changes_path):\n",
    "    for file in os.listdir(file_changes_path):\n",
    "        if any(base_name in file for base_name in files_without_future_versions):\n",
    "            file_path = os.path.join(file_changes_path, file)\n",
    "            remove_file(file_path, \"file_changes_in_versions\", removed_files_log)\n",
    "\n",
    "with open(log_file, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=[\"File Path\", \"Folder\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(removed_files_log)\n",
    "\n",
    "print(f\"Cleanup completed for commit 'chromium_4ab22cfc619ee8ff17a8c50e289ec3b30731ceba'.\")\n",
    "print(f\"Log of removed files saved to: {log_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bd1470",
   "metadata": {},
   "source": [
    "# full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a75fa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "root_path = r\"M:\\FULL_DATA_COLLECTED\"\n",
    "log_file = os.path.join(root_path, \"removed_files_log.csv\")\n",
    "\n",
    "def extract_version_number(file_name):\n",
    "    \"\"\"\n",
    "    Extract the version number from a filename if '_v' is followed by digits.\n",
    "    Returns the version number as an integer or None if no valid version is found.\n",
    "    \"\"\"\n",
    "    if \"_v\" in file_name:\n",
    "        try:\n",
    "            version_part = file_name.split(\"_v\")[1]\n",
    "            version_number = int(version_part.split(\"_\")[0])\n",
    "            return version_number\n",
    "        except (IndexError, ValueError):\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def remove_file(file_path, folder_name, commit_dir, project_name, log):\n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            os.remove(file_path)\n",
    "            log.append({\n",
    "                \"Project\": project_name,\n",
    "                \"Commit\": commit_dir,\n",
    "                \"File Path\": file_path,\n",
    "                \"Folder\": folder_name\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to remove {file_path}: {e}\")\n",
    "\n",
    "removed_files_log = []\n",
    "\n",
    "for project_name in os.listdir(root_path):\n",
    "    project_path = os.path.join(root_path, project_name)\n",
    "    if not os.path.isdir(project_path):\n",
    "        continue\n",
    "\n",
    "    for commit_dir in os.listdir(project_path):\n",
    "        commit_path = os.path.join(project_path, commit_dir)\n",
    "        if not os.path.isdir(commit_path):\n",
    "            continue\n",
    "\n",
    "        all_versions_path = os.path.join(commit_path, \"all_versions\")\n",
    "        commit_messages_path = os.path.join(commit_path, \"commit_messages\")\n",
    "        file_changes_path = os.path.join(commit_path, \"file_changes_in_versions\")\n",
    "\n",
    "        files_without_future_versions = set()\n",
    "\n",
    "        if os.path.exists(all_versions_path):\n",
    "            files_in_commit = os.listdir(all_versions_path)\n",
    "            unique_files = {}\n",
    "\n",
    "            for file in files_in_commit:\n",
    "                if \"_v\" in file:\n",
    "                    base_name = file.split(\"_v\")[0]\n",
    "                    if base_name not in unique_files:\n",
    "                        unique_files[base_name] = []\n",
    "                    unique_files[base_name].append(file)\n",
    "\n",
    "            for base_name, versions in unique_files.items():\n",
    "                versioned_files = [\n",
    "                    (file, extract_version_number(file))\n",
    "                    for file in versions\n",
    "                ]\n",
    "                versioned_files = [\n",
    "                    (file, version)\n",
    "                    for file, version in versioned_files\n",
    "                    if version is not None\n",
    "                ]\n",
    "\n",
    "                if not versioned_files:\n",
    "                    continue\n",
    "\n",
    "                sorted_versions = sorted(versioned_files, key=lambda x: x[1])\n",
    "\n",
    "                has_future_versions = False\n",
    "                for idx, (file, version) in enumerate(sorted_versions):\n",
    "                    if \"fixed_version\" in file:\n",
    "                        if idx < len(sorted_versions) - 1:\n",
    "                            has_future_versions = True\n",
    "                        break\n",
    "\n",
    "                if not has_future_versions:\n",
    "                    for file, _ in versioned_files:\n",
    "                        file_path = os.path.join(all_versions_path, file)\n",
    "                        remove_file(file_path, \"all_versions\", commit_dir, project_name, removed_files_log)\n",
    "                        files_without_future_versions.add(base_name)\n",
    "\n",
    "        if os.path.exists(commit_messages_path):\n",
    "            for file in os.listdir(commit_messages_path):\n",
    "                if any(base_name in file for base_name in files_without_future_versions):\n",
    "                    file_path = os.path.join(commit_messages_path, file)\n",
    "                    remove_file(file_path, \"commit_messages\", commit_dir, project_name, removed_files_log)\n",
    "\n",
    "        if os.path.exists(file_changes_path):\n",
    "            for file in os.listdir(file_changes_path):\n",
    "                if any(base_name in file for base_name in files_without_future_versions):\n",
    "                    file_path = os.path.join(file_changes_path, file)\n",
    "                    remove_file(file_path, \"file_changes_in_versions\", commit_dir, project_name, removed_files_log)\n",
    "\n",
    "with open(log_file, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=[\"Project\", \"Commit\", \"File Path\", \"Folder\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(removed_files_log)\n",
    "\n",
    "print(f\"Cleanup completed for all projects in the root directory.\")\n",
    "print(f\"Log of removed files saved to: {log_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c399683a",
   "metadata": {},
   "source": [
    "# file with 0 size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254cde6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "data_path = r\"M:\\FULL_DATA_COLLECTED\"\n",
    "log_file = os.path.join(data_path, \"zero_size_files_log.csv\")\n",
    "commit_summary_file = os.path.join(data_path, \"zero_size_commit_summary.csv\")\n",
    "\n",
    "zero_size_files = []\n",
    "commit_stats = defaultdict(lambda: {\"Project\": \"\", \"Zero-Size Files\": 0})\n",
    "total_files_checked = 0\n",
    "\n",
    "for root, _, files in os.walk(data_path):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) and os.path.getsize(file_path) == 0:\n",
    "                zero_size_files.append({\"File Path\": file_path})\n",
    "\n",
    "                relative_path = os.path.relpath(file_path, data_path)\n",
    "                parts = relative_path.split(os.sep)\n",
    "\n",
    "                if len(parts) >= 3:\n",
    "                    project_name = parts[0]\n",
    "                    commit_dir = parts[1]\n",
    "                    commit_path = os.path.join(project_name, commit_dir)\n",
    "\n",
    "                    commit_stats[commit_path][\"Project\"] = project_name\n",
    "                    commit_stats[commit_path][\"Zero-Size Files\"] += 1\n",
    "\n",
    "            total_files_checked += 1\n",
    "\n",
    "            if total_files_checked % 50000 == 0:\n",
    "                print(f\"Checked {total_files_checked} files so far...\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error checking file size for {file_path}: {e}\")\n",
    "\n",
    "with open(log_file, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=[\"File Path\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(zero_size_files)\n",
    "\n",
    "with open(commit_summary_file, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=[\"Project\", \"Commit Path\", \"Zero-Size Files\"])\n",
    "    writer.writeheader()\n",
    "    for commit_path, stats in commit_stats.items():\n",
    "        writer.writerow({\n",
    "            \"Project\": stats[\"Project\"],\n",
    "            \"Commit Path\": commit_path,\n",
    "            \"Zero-Size Files\": stats[\"Zero-Size Files\"]\n",
    "        })\n",
    "\n",
    "print(f\"Total files checked: {total_files_checked}\")\n",
    "print(f\"Total zero-size files found: {len(zero_size_files)}\")\n",
    "print(f\"Detailed log saved to: {log_file}\")\n",
    "print(f\"Commit-level summary saved to: {commit_summary_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e2931c",
   "metadata": {},
   "source": [
    "# 0 size files are from which folder. if commit message, it's ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce833bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "data_path = r\"M:\\FULL_DATA_COLLECTED\"\n",
    "log_file = os.path.join(data_path, \"zero_size_files_in_changes_log.csv\")\n",
    "commit_summary_file = os.path.join(data_path, \"zero_size_commit_changes_summary.csv\")\n",
    "\n",
    "zero_size_files = []\n",
    "commit_stats = defaultdict(lambda: {\"Project\": \"\", \"Zero-Size Files\": 0})\n",
    "total_files_checked = 0\n",
    "\n",
    "for project_name in os.listdir(data_path):\n",
    "    project_path = os.path.join(data_path, project_name)\n",
    "    if not os.path.isdir(project_path):\n",
    "        continue\n",
    "\n",
    "    for commit_dir in os.listdir(project_path):\n",
    "        commit_path = os.path.join(project_path, commit_dir)\n",
    "        if not os.path.isdir(commit_path):\n",
    "            continue\n",
    "\n",
    "        changes_path = os.path.join(commit_path, 'file_changes_in_versions')\n",
    "        if not os.path.exists(changes_path):\n",
    "            continue\n",
    "\n",
    "        for root, _, files in os.walk(changes_path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    if os.path.isfile(file_path) and os.path.getsize(file_path) == 0:\n",
    "                        zero_size_files.append({\"File Path\": file_path})\n",
    "\n",
    "                        relative_path = os.path.relpath(file_path, data_path)\n",
    "                        parts = relative_path.split(os.sep)\n",
    "\n",
    "                        if len(parts) >= 3:\n",
    "                            project_name = parts[0]\n",
    "                            commit_dir = parts[1]\n",
    "                            commit_path = os.path.join(project_name, commit_dir)\n",
    "\n",
    "                            commit_stats[commit_path][\"Project\"] = project_name\n",
    "                            commit_stats[commit_path][\"Zero-Size Files\"] += 1\n",
    "\n",
    "                    total_files_checked += 1\n",
    "\n",
    "                    if total_files_checked % 10000 == 0:\n",
    "                        print(f\"Checked {total_files_checked} files so far...\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error checking file size for {file_path}: {e}\")\n",
    "\n",
    "with open(log_file, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=[\"File Path\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(zero_size_files)\n",
    "\n",
    "with open(commit_summary_file, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=[\"Project\", \"Commit Path\", \"Zero-Size Files\"])\n",
    "    writer.writeheader()\n",
    "    for commit_path, stats in commit_stats.items():\n",
    "        writer.writerow({\n",
    "            \"Project\": stats[\"Project\"],\n",
    "            \"Commit Path\": commit_path,\n",
    "            \"Zero-Size Files\": stats[\"Zero-Size Files\"]\n",
    "        })\n",
    "\n",
    "print(f\"Total files checked: {total_files_checked}\")\n",
    "print(f\"Total zero-size files found in 'file_changes_in_versions': {len(zero_size_files)}\")\n",
    "print(f\"Detailed log saved to: {log_file}\")\n",
    "print(f\"Commit-level summary saved to: {commit_summary_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02077bf",
   "metadata": {},
   "source": [
    "# remove commits more than 1 file only from ioq3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ec2e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import csv\n",
    "\n",
    "project_path = r\"M:\\FULL_DATA_COLLECTED\\libarchive\"\n",
    "log_file = os.path.join(project_path, \"libarchive_filtered_commits_log.csv\")\n",
    "\n",
    "def extract_file_name_and_extension(file_name):\n",
    "    \"\"\"\n",
    "    Extract the full file name including extension.\n",
    "    - File name: base name + extension (e.g., bookmark_manager_view.h)\n",
    "    - Extension: part after the first '.' and before the first '_'.\n",
    "    \"\"\"\n",
    "    if \".\" in file_name and \"_\" in file_name:\n",
    "        first_dot = file_name.find(\".\")\n",
    "        after_dot = file_name[first_dot + 1:]\n",
    "        first_underscore = after_dot.find(\"_\")\n",
    "        if first_underscore != -1:\n",
    "            extension = after_dot[:first_underscore]\n",
    "            base_name_with_ext = file_name[:first_dot + 1] + extension\n",
    "            return base_name_with_ext, extension\n",
    "    return file_name, \"No Extension\"\n",
    "\n",
    "def count_unique_files(all_versions_path):\n",
    "    unique_files = set()\n",
    "    if os.path.exists(all_versions_path):\n",
    "        files_in_commit = os.listdir(all_versions_path)\n",
    "        for file in files_in_commit:\n",
    "            full_file_name, _ = extract_file_name_and_extension(file)\n",
    "            unique_files.add(full_file_name)\n",
    "    return len(unique_files)\n",
    "\n",
    "def remove_commit(commit_path):\n",
    "    if os.path.exists(commit_path):\n",
    "        shutil.rmtree(commit_path)\n",
    "\n",
    "retained_commits = []\n",
    "discarded_commits = []\n",
    "\n",
    "if os.path.isdir(project_path):\n",
    "    for commit_dir in os.listdir(project_path):\n",
    "        commit_path = os.path.join(project_path, commit_dir)\n",
    "        if not os.path.isdir(commit_path):\n",
    "            continue\n",
    "\n",
    "        all_versions_path = os.path.join(commit_path, 'all_versions')\n",
    "        commit_messages_path = os.path.join(commit_path, 'commit_messages')\n",
    "        file_changes_path = os.path.join(commit_path, 'file_changes_in_versions')\n",
    "\n",
    "        unique_file_count = count_unique_files(all_versions_path)\n",
    "\n",
    "        if unique_file_count == 1:\n",
    "            retained_commits.append({\n",
    "                \"Project\": \"libarchive\",\n",
    "                \"Commit\": commit_dir,\n",
    "                \"Files Changed\": unique_file_count\n",
    "            })\n",
    "        else:\n",
    "            discarded_commits.append({\n",
    "                \"Project\": \"libarchive\",\n",
    "                \"Commit\": commit_dir,\n",
    "                \"Files Changed\": unique_file_count\n",
    "            })\n",
    "            remove_commit(commit_path)\n",
    "\n",
    "with open(log_file, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=[\"Project\", \"Commit\", \"Files Changed\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(retained_commits + discarded_commits)\n",
    "\n",
    "print(f\"Total commits retained for libarchive: {len(retained_commits)}\")\n",
    "print(f\"Total commits discarded for libarchive: {len(discarded_commits)}\")\n",
    "print(f\"Log saved to: {log_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dac77d",
   "metadata": {},
   "source": [
    "# remove the directories where more than 1 file exist whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0c4d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import csv\n",
    "\n",
    "data_path = r\"M:\\FULL_DATA_COLLECTED\"\n",
    "log_file = os.path.join(data_path, \"filtered_commits_log.csv\")\n",
    "\n",
    "def extract_file_name_and_extension(file_name):\n",
    "    \"\"\"\n",
    "    Extract the full file name including extension.\n",
    "    - File name: base name + extension (e.g., bookmark_manager_view.h)\n",
    "    - Extension: part after the first '.' and before the first '_'.\n",
    "    \"\"\"\n",
    "    if \".\" in file_name and \"_\" in file_name:\n",
    "        first_dot = file_name.find(\".\")\n",
    "        after_dot = file_name[first_dot + 1:]\n",
    "        first_underscore = after_dot.find(\"_\")\n",
    "        if first_underscore != -1:\n",
    "            extension = after_dot[:first_underscore]\n",
    "            base_name_with_ext = file_name[:first_dot + 1] + extension\n",
    "            return base_name_with_ext, extension\n",
    "    return file_name, \"No Extension\"\n",
    "\n",
    "def count_unique_files(all_versions_path):\n",
    "    unique_files = set()\n",
    "    if os.path.exists(all_versions_path):\n",
    "        files_in_commit = os.listdir(all_versions_path)\n",
    "        for file in files_in_commit:\n",
    "            full_file_name, _ = extract_file_name_and_extension(file)\n",
    "            unique_files.add(full_file_name)\n",
    "    return len(unique_files)\n",
    "\n",
    "def remove_commit(commit_path):\n",
    "    if os.path.exists(commit_path):\n",
    "        shutil.rmtree(commit_path)\n",
    "\n",
    "retained_commits = []\n",
    "discarded_commits = []\n",
    "\n",
    "for project_name in os.listdir(data_path):\n",
    "    project_path = os.path.join(data_path, project_name)\n",
    "    if not os.path.isdir(project_path):\n",
    "        continue\n",
    "\n",
    "    for commit_dir in os.listdir(project_path):\n",
    "        commit_path = os.path.join(project_path, commit_dir)\n",
    "        if not os.path.isdir(commit_path):\n",
    "            continue\n",
    "\n",
    "        all_versions_path = os.path.join(commit_path, 'all_versions')\n",
    "        commit_messages_path = os.path.join(commit_path, 'commit_messages')\n",
    "        file_changes_path = os.path.join(commit_path, 'file_changes_in_versions')\n",
    "\n",
    "        unique_file_count = count_unique_files(all_versions_path)\n",
    "\n",
    "        if unique_file_count == 1:\n",
    "            retained_commits.append({\n",
    "                \"Project\": project_name,\n",
    "                \"Commit\": commit_dir,\n",
    "                \"Files Changed\": unique_file_count\n",
    "            })\n",
    "        else:\n",
    "            discarded_commits.append({\n",
    "                \"Project\": project_name,\n",
    "                \"Commit\": commit_dir,\n",
    "                \"Files Changed\": unique_file_count\n",
    "            })\n",
    "            remove_commit(commit_path)\n",
    "\n",
    "with open(log_file, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=[\"Project\", \"Commit\", \"Files Changed\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(retained_commits + discarded_commits)\n",
    "\n",
    "print(f\"Total commits retained: {len(retained_commits)}\")\n",
    "print(f\"Total commits discarded: {len(discarded_commits)}\")\n",
    "print(f\"Log saved to: {log_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abeb4461",
   "metadata": {},
   "source": [
    "# commit message directory empty file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852fe473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "data_path = r\"M:\\FULL_DATA_COLLECTED\"\n",
    "log_file = os.path.join(data_path, \"zero_size_files_in_commit_messages_log.csv\")\n",
    "commit_summary_file = os.path.join(data_path, \"zero_size_commit_changes_summary.csv\")\n",
    "\n",
    "zero_size_files = []\n",
    "commit_stats = defaultdict(lambda: {\"Project\": \"\", \"Zero-Size Files\": 0})\n",
    "total_files_checked = 0\n",
    "\n",
    "for project_name in os.listdir(data_path):\n",
    "    project_path = os.path.join(data_path, project_name)\n",
    "    if not os.path.isdir(project_path):\n",
    "        continue\n",
    "\n",
    "    for commit_dir in os.listdir(project_path):\n",
    "        commit_path = os.path.join(project_path, commit_dir)\n",
    "        if not os.path.isdir(commit_path):\n",
    "            continue\n",
    "\n",
    "        changes_path = os.path.join(commit_path, 'commit_messages')\n",
    "        if not os.path.exists(changes_path):\n",
    "            continue\n",
    "\n",
    "        for root, _, files in os.walk(changes_path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    if os.path.isfile(file_path) and os.path.getsize(file_path) == 0:\n",
    "                        zero_size_files.append({\"File Path\": file_path})\n",
    "\n",
    "                        relative_path = os.path.relpath(file_path, data_path)\n",
    "                        parts = relative_path.split(os.sep)\n",
    "\n",
    "                        if len(parts) >= 3:\n",
    "                            project_name = parts[0]\n",
    "                            commit_dir = parts[1]\n",
    "                            commit_path = os.path.join(project_name, commit_dir)\n",
    "\n",
    "                            commit_stats[commit_path][\"Project\"] = project_name\n",
    "                            commit_stats[commit_path][\"Zero-Size Files\"] += 1\n",
    "\n",
    "                    total_files_checked += 1\n",
    "\n",
    "                    if total_files_checked % 10000 == 0:\n",
    "                        print(f\"Checked {total_files_checked} files so far...\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error checking file size for {file_path}: {e}\")\n",
    "\n",
    "with open(log_file, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=[\"File Path\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(zero_size_files)\n",
    "\n",
    "with open(commit_summary_file, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=[\"Project\", \"Commit Path\", \"Zero-Size Files\"])\n",
    "    writer.writeheader()\n",
    "    for commit_path, stats in commit_stats.items():\n",
    "        writer.writerow({\n",
    "            \"Project\": stats[\"Project\"],\n",
    "            \"Commit Path\": commit_path,\n",
    "            \"Zero-Size Files\": stats[\"Zero-Size Files\"]\n",
    "        })\n",
    "\n",
    "print(f\"Total files checked: {total_files_checked}\")\n",
    "print(f\"Total zero-size files found in 'file_changes_in_versions': {len(zero_size_files)}\")\n",
    "print(f\"Detailed log saved to: {log_file}\")\n",
    "print(f\"Commit-level summary saved to: {commit_summary_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b727b855",
   "metadata": {},
   "source": [
    "# total number of commits inside commit_messages directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3951a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_path = r\"M:\\FULL_DATA_COLLECTED - Copy\"\n",
    "\n",
    "\n",
    "total_commit_messages = 0\n",
    "\n",
    "for project_name in os.listdir(data_path):\n",
    "    project_path = os.path.join(data_path, project_name)\n",
    "    if not os.path.isdir(project_path):\n",
    "        continue\n",
    "\n",
    "    for commit_dir in os.listdir(project_path):\n",
    "        commit_path = os.path.join(project_path, commit_dir)\n",
    "        if not os.path.isdir(commit_path):\n",
    "            continue\n",
    "\n",
    "        commit_messages_path = os.path.join(commit_path, 'commit_messages')\n",
    "        if os.path.exists(commit_messages_path):\n",
    "            total_commit_messages += len(os.listdir(commit_messages_path))\n",
    "\n",
    "print(f\"Total commit message files: {total_commit_messages}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c08e9f",
   "metadata": {},
   "source": [
    "# multiple patterns match\n",
    "# added CWE keyword in addition to the paper \"Automated Identification of Security Issues from Commit Messages and Bug Reports\"\n",
    "# \"Finding A Needle in a Haystack:Automated Mining of Silent Vulnerability Fixes\" this paper only considers three keywords from this. nvd, cve and vuln.\n",
    "# we can think about it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7503ba0c",
   "metadata": {},
   "source": [
    "# high and medium category wise one project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2681c5",
   "metadata": {},
   "source": [
    "# with customized vul keyword added"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cdbc66",
   "metadata": {},
   "source": [
    "# code modified for better mapping with flattened json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e841c151",
   "metadata": {},
   "source": [
    "# updated fully to match the flattened json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5f35b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "\n",
    "project_path = r\"M:\\FULL_DATA_COLLECTED\\FFmpeg\"\n",
    "\n",
    "output_csv = os.path.join(project_path, \"security_patterns_found.csv\")\n",
    "\n",
    "strong_vuln_patterns = re.compile(\n",
    "    r\"(?i)(denial[.\\s_-]of[.\\s_-]service|\\bXXE\\b|remote[.\\s_-]code[.\\s_-]execution|\\bopen[.\\s_-]redirect|OSVDB|\\bvuln\\w*|\\bCVE\\w*|\"\n",
    "    r\"\\bXSS\\b|\\bReDoS\\b|\\bNVD\\w*|malicious|x\\-frame\\-options|attack|cross[.\\s_-]site|exploit|directory[.\\s_-]traversal|\"\n",
    "    r\"\\bRCE\\b|\\bdos\\b|\\bXSRF\\b|clickjack|session[.\\s_-]fixation|hijack|advisory|insecure|security|\"\n",
    "    r\"\\bcross[.\\s_-]origin\\b|unauthori(?:z|s)ed|infinite[.\\s_-]loop|CWE\\w*)\"\n",
    ")\n",
    "\n",
    "medium_vuln_patterns = re.compile(\n",
    "    r\"(?i)(authenticat(?:e|ion)|bruteforce|bypass|constant[.\\s_-]time|crack|credential|\\bDoS\\b|expos(?:e|ing)|hack|\"\n",
    "    r\"harden|injection|lockout|overflow|password|\\bPoC\\b|proof[.\\s_-]of[.\\s_-]concept|poison|privilege|\"\n",
    "    r\"\\b(?:in)?secur(?:e|ity)|(?:de)?serializ|spoof|timing|traversal)\"\n",
    ")\n",
    "\n",
    "custom_vuln_patterns = re.compile(\n",
    "    r\"\\b(?:bug\\w*|fix\\w*|issue\\w*|defect\\w*|ticket\\w*|problem\\w*|patch\\w*|repair\\w*)[\\s\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "def extract_version(commit_file):\n",
    "    match = re.search(r'_v(\\d+)_\\d{8}_\\d{6}_[a-f0-9]{40}_commit_message', commit_file)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "results = []\n",
    "\n",
    "if os.path.exists(project_path):\n",
    "    for commit_folder in os.listdir(project_path):\n",
    "        commit_path = os.path.join(project_path, commit_folder)\n",
    "        if not os.path.isdir(commit_path):\n",
    "            continue\n",
    "\n",
    "        parent_commit_id = commit_folder\n",
    "\n",
    "        commit_messages_path = os.path.join(commit_path, \"commit_messages\")\n",
    "        if not os.path.exists(commit_messages_path):\n",
    "            continue\n",
    "\n",
    "        for commit_file in sorted(os.listdir(commit_messages_path)):\n",
    "            file_path = os.path.join(commit_messages_path, commit_file)\n",
    "            try:\n",
    "                current_version = extract_version(commit_file)\n",
    "                if current_version is not None and current_version > 1:\n",
    "                    previous_version = current_version - 1\n",
    "                    file_base_name = commit_file.split(\"_v\")[0]\n",
    "                    found_version = f\"{file_base_name}_changes_v{previous_version}_v{current_version}.txt\"\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "\n",
    "                    strong_matches = set(map(str.lower, strong_vuln_patterns.findall(content)))\n",
    "                    medium_matches = set(map(str.lower, medium_vuln_patterns.findall(content)))\n",
    "                    custom_matches = set(map(str.lower, custom_vuln_patterns.findall(content)))\n",
    "\n",
    "                    if strong_matches or medium_matches or custom_matches:\n",
    "                        results.append({\n",
    "                            \"Project\": \"FFmpeg\",\n",
    "                            \"Commit\": parent_commit_id,\n",
    "                            \"Found Version\": found_version,\n",
    "                            \"Matches (High)\": \", \".join(sorted(strong_matches)) if strong_matches else \"\",\n",
    "                            \"Matches (Medium)\": \", \".join(sorted(medium_matches)) if medium_matches else \"\",\n",
    "                            \"Matches (Custom)\": \", \".join(sorted(custom_matches)) if custom_matches else \"\",\n",
    "                        })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Directory {project_path} does not exist.\")\n",
    "\n",
    "if results:\n",
    "    with open(output_csv, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=[\"Project\", \"Commit\", \"Found Version\", \"Matches (High)\", \"Matches (Medium)\", \"Matches (Custom)\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(results)\n",
    "\n",
    "    print(f\"Security patterns found and saved to {output_csv}\")\n",
    "else:\n",
    "    print(\"No security patterns found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a19c353",
   "metadata": {},
   "source": [
    "# updated with syntactic score and several other modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688a6cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "\n",
    "project_path = r\"M:\\FULL_DATA_COLLECTED\\FFmpeg\"\n",
    "\n",
    "output_csv = os.path.join(project_path, \"security_patterns_found.csv\")\n",
    "\n",
    "strong_vuln_patterns = re.compile(\n",
    "    r\"(?i)(denial[.\\s_-]of[.\\s_-]service|\\bXXE\\b|remote[.\\s_-]code[.\\s_-]execution|\\bopen[.\\s_-]redirect|OSVDB|\\bvuln\\w*|\\bCVE\\w*|\"\n",
    "    r\"\\bXSS\\b|\\bReDoS\\b|\\bNVD\\w*|malicious|x\\-frame\\-options|attack|cross[.\\s_-]site|exploit|directory[.\\s_-]traversal|\"\n",
    "    r\"\\bRCE\\b|\\bdos\\b|\\bXSRF\\b|clickjack|session[.\\s_-]fixation|hijack|advisory|insecure|security|\"\n",
    "    r\"\\bcross[.\\s_-]origin\\b|unauthori(?:z|s)ed|infinite[.\\s_-]loop|CWE\\w*)\"\n",
    ")\n",
    "\n",
    "medium_vuln_patterns = re.compile(\n",
    "    r\"(?i)(authenticat(?:e|ion)|bruteforce|bypass|constant[.\\s_-]time|crack|credential|\\bDoS\\b|expos(?:e|ing)|hack|\"\n",
    "    r\"harden|injection|lockout|overflow|password|\\bPoC\\b|proof[.\\s_-]of[.\\s_-]concept|poison|privilege|\"\n",
    "    r\"\\b(?:in)?secur(?:e|ity)|(?:de)?serializ|spoof|timing|traversal)\"\n",
    ")\n",
    "\n",
    "bug_number_patterns = re.compile(\n",
    "    r\"\\bbug\\s*\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "\n",
    "plain_number_pattern = re.compile(r\"\\b\\d+\\b\")\n",
    "\n",
    "keyword_patterns = re.compile(\n",
    "    r\"\\b(fix(?:ed|es)?|bug(?:s)?|defect(?:s)?|patch(?:es)?|issue(?:s)?|ticket(?:s)?|repair(?:s)?)\\b\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "\n",
    "def extract_version(commit_file):\n",
    "    match = re.search(r'_v(\\d+)_\\d{8}_\\d{6}_[a-f0-9]{40}_commit_message', commit_file)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "results = []\n",
    "\n",
    "if os.path.exists(project_path):\n",
    "    for commit_folder in os.listdir(project_path):\n",
    "        commit_path = os.path.join(project_path, commit_folder)\n",
    "        if not os.path.isdir(commit_path):\n",
    "            continue\n",
    "\n",
    "        parent_commit_id = commit_folder\n",
    "\n",
    "        commit_messages_path = os.path.join(commit_path, \"commit_messages\")\n",
    "        if not os.path.exists(commit_messages_path):\n",
    "            continue\n",
    "\n",
    "        for commit_file in sorted(os.listdir(commit_messages_path)):\n",
    "            file_path = os.path.join(commit_messages_path, commit_file)\n",
    "            try:\n",
    "                current_version = extract_version(commit_file)\n",
    "                if current_version is not None and current_version > 1:\n",
    "                    previous_version = current_version - 1\n",
    "                    file_base_name = commit_file.split(\"_v\")[0]\n",
    "                    found_version = f\"{file_base_name}_changes_v{previous_version}_v{current_version}.txt\"\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "\n",
    "                    strong_matches = set(map(str.lower, strong_vuln_patterns.findall(content)))\n",
    "                    medium_matches = set(map(str.lower, medium_vuln_patterns.findall(content)))\n",
    "\n",
    "                    bug_numbers = set(bug_number_patterns.findall(content))\n",
    "                    plain_numbers = set(plain_number_pattern.findall(content))\n",
    "                    keywords = set(keyword_patterns.findall(content))\n",
    "\n",
    "                    syntactic_confidence = 0\n",
    "                    if bug_numbers:\n",
    "                        syntactic_confidence += 1\n",
    "                    if keywords:\n",
    "                        syntactic_confidence += 1\n",
    "                    elif plain_numbers and not keywords:\n",
    "                        syntactic_confidence += 1\n",
    "\n",
    "                    if strong_matches or medium_matches or keywords:\n",
    "                        results.append({\n",
    "                            \"Project\": \"FFmpeg\",\n",
    "                            \"Commit\": parent_commit_id,\n",
    "                            \"Found Version\": found_version,\n",
    "                            \"Matches (High)\": \", \".join(sorted(strong_matches)) if strong_matches else \"\",\n",
    "                            \"Matches (Medium)\": \", \".join(sorted(medium_matches)) if medium_matches else \"\",\n",
    "                            \"Bug Numbers\": \", \".join(bug_numbers) if bug_numbers else \"\",\n",
    "                            \"Plain Numbers\": \", \".join(plain_numbers) if plain_numbers else \"\",\n",
    "                            \"Keywords\": \", \".join(keywords) if keywords else \"\",\n",
    "                            \"Syntactic Confidence\": syntactic_confidence\n",
    "                        })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Directory {project_path} does not exist.\")\n",
    "\n",
    "if results:\n",
    "    with open(output_csv, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=[\n",
    "            \"Project\", \"Commit\", \"Found Version\",\n",
    "            \"Matches (High)\", \"Matches (Medium)\",\n",
    "            \"Bug Numbers\", \"Plain Numbers\", \"Keywords\", \"Syntactic Confidence\"\n",
    "        ])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(results)\n",
    "\n",
    "    print(f\"Security patterns found and saved to {output_csv}\")\n",
    "else:\n",
    "    print(\"No security patterns found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f703f2c7",
   "metadata": {},
   "source": [
    "# high and medium category wise full project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a436cc65",
   "metadata": {},
   "source": [
    "# with custom keyword full pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f30ad9a",
   "metadata": {},
   "source": [
    "# full project updated for mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2379497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "\n",
    "root_directory = r\"M:\\FULL_DATA_COLLECTED\"\n",
    "\n",
    "output_csv = os.path.join(root_directory, \"security_patterns_unified.csv\")\n",
    "\n",
    "strong_vuln_patterns = re.compile(\n",
    "    r\"(?i)(denial[.\\s_-]of[.\\s_-]service|\\bXXE\\b|remote[.\\s_-]code[.\\s_-]execution|\\bopen[.\\s_-]redirect|OSVDB|\\bvuln\\w*|\\bCVE\\w*|\"\n",
    "    r\"\\bXSS\\b|\\bReDoS\\b|\\bNVD\\w*|malicious|x\\-frame\\-options|attack|cross[.\\s_-]site|exploit|directory[.\\s_-]traversal|\"\n",
    "    r\"\\bRCE\\b|\\bdos\\b|\\bXSRF\\b|clickjack|session[.\\s_-]fixation|hijack|advisory|insecure|security|\"\n",
    "    r\"\\bcross[.\\s_-]origin\\b|unauthori(?:z|s)ed|infinite[.\\s_-]loop|CWE\\w*)\"\n",
    ")\n",
    "\n",
    "medium_vuln_patterns = re.compile(\n",
    "    r\"(?i)(authenticat(?:e|ion)|bruteforce|bypass|constant[.\\s_-]time|crack|credential|\\bDoS\\b|expos(?:e|ing)|hack|\"\n",
    "    r\"harden|injection|lockout|overflow|password|\\bPoC\\b|proof[.\\s_-]of[.\\s_-]concept|poison|privilege|\"\n",
    "    r\"\\b(?:in)?secur(?:e|ity)|(?:de)?serializ|spoof|timing|traversal)\"\n",
    ")\n",
    "\n",
    "bug_number_patterns = re.compile(\n",
    "    r\"\\bbug\\s*\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plain_number_pattern = re.compile(r\"\\b\\d+\\b\")\n",
    "\n",
    "keyword_patterns = re.compile(\n",
    "    r\"\\b(fix(?:ed|es)?|bug(?:s)?|defect(?:s)?|patch(?:es)?|issue(?:s)?|ticket(?:s)?|repair(?:s)?)\\b\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "\n",
    "def extract_version(commit_file):\n",
    "    match = re.search(r'_v(\\d+)_\\d{8}_\\d{6}_[a-f0-9]{40}_commit_message', commit_file)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for project_name in os.listdir(root_directory):\n",
    "    project_path = os.path.join(root_directory, project_name)\n",
    "    if not os.path.isdir(project_path):\n",
    "        continue\n",
    "\n",
    "    project_results = []\n",
    "\n",
    "    for commit_folder in os.listdir(project_path):\n",
    "        commit_path = os.path.join(project_path, commit_folder)\n",
    "        if not os.path.isdir(commit_path):\n",
    "            continue\n",
    "\n",
    "        parent_commit_id = commit_folder\n",
    "\n",
    "        commit_messages_path = os.path.join(commit_path, \"commit_messages\")\n",
    "        if not os.path.exists(commit_messages_path):\n",
    "            continue\n",
    "\n",
    "        for commit_file in sorted(os.listdir(commit_messages_path)):\n",
    "            file_path = os.path.join(commit_messages_path, commit_file)\n",
    "            try:\n",
    "                current_version = extract_version(commit_file)\n",
    "                if current_version is not None and current_version > 1:\n",
    "                    previous_version = current_version - 1\n",
    "                    file_base_name = commit_file.split(\"_v\")[0]\n",
    "                    found_version = f\"{file_base_name}_changes_v{previous_version}_v{current_version}.txt\"\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "\n",
    "                    strong_matches = set(map(str.lower, strong_vuln_patterns.findall(content)))\n",
    "                    medium_matches = set(map(str.lower, medium_vuln_patterns.findall(content)))\n",
    "\n",
    "                    bug_numbers = set(bug_number_patterns.findall(content))\n",
    "                    plain_numbers = set(plain_number_pattern.findall(content))\n",
    "                    keywords = set(keyword_patterns.findall(content))\n",
    "\n",
    "                    syntactic_confidence = 0\n",
    "                    if bug_numbers:\n",
    "                        syntactic_confidence += 1\n",
    "                    if keywords:\n",
    "                        syntactic_confidence += 1\n",
    "                    elif plain_numbers and not keywords:\n",
    "                        syntactic_confidence += 1\n",
    "\n",
    "                    if strong_matches or medium_matches or keywords:\n",
    "                        project_results.append({\n",
    "                            \"Project\": project_name,\n",
    "                            \"Commit\": parent_commit_id,\n",
    "                            \"Found Version\": found_version,\n",
    "                            \"Matches (High)\": \", \".join(sorted(strong_matches)) if strong_matches else \"\",\n",
    "                            \"Matches (Medium)\": \", \".join(sorted(medium_matches)) if medium_matches else \"\",\n",
    "                            \"Bug Numbers\": \", \".join(bug_numbers) if bug_numbers else \"\",\n",
    "                            \"Plain Numbers\": \", \".join(plain_numbers) if plain_numbers else \"\",\n",
    "                            \"Keywords\": \", \".join(keywords) if keywords else \"\",\n",
    "                            \"Syntactic Confidence\": syntactic_confidence\n",
    "                        })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "    all_results.extend(project_results)\n",
    "    print(f\" Processed project: {project_name} ({len(project_results)} records)\")\n",
    "\n",
    "if all_results:\n",
    "    with open(output_csv, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=[\n",
    "            \"Project\", \"Commit\", \"Found Version\",\n",
    "            \"Matches (High)\", \"Matches (Medium)\",\n",
    "            \"Bug Numbers\", \"Plain Numbers\", \"Keywords\", \"Syntactic Confidence\"\n",
    "        ])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(all_results)\n",
    "\n",
    "    print(f\" Security patterns found and saved to {output_csv}\")\n",
    "else:\n",
    "    print(\" No security patterns found in any project.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6362904b",
   "metadata": {},
   "source": [
    "# total line change statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a81508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "root_directory = r\"M:\\\\FULL_DATA_COLLECTED\"\n",
    "\n",
    "output_csv = os.path.join(root_directory, \"all_projects_line_statistics.csv\")\n",
    "\n",
    "def count_lines_and_hunks(diff_file):\n",
    "    added = 0\n",
    "    deleted = 0\n",
    "    hunks = 0\n",
    "    with open(diff_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"+\") and not line.startswith(\"+++\"):\n",
    "                added += 1\n",
    "            elif line.startswith(\"-\") and not line.startswith(\"---\"):\n",
    "                deleted += 1\n",
    "            elif line.startswith(\"@@\"):\n",
    "                hunks += 1\n",
    "    return added, deleted, hunks\n",
    "\n",
    "def process_project(project_path):\n",
    "    project_results = []\n",
    "    for commit in os.listdir(project_path):\n",
    "        commit_dir = os.path.join(project_path, commit)\n",
    "        if not os.path.isdir(commit_dir):\n",
    "            continue\n",
    "\n",
    "        fixed_version_diff = None\n",
    "        changes_dir = os.path.join(commit_dir, \"file_changes_in_versions\")\n",
    "        if os.path.exists(changes_dir):\n",
    "            for file in os.listdir(changes_dir):\n",
    "                if \"fixed_version\" in file and file.endswith(\".txt\"):\n",
    "                    fixed_version_diff = os.path.join(changes_dir, file)\n",
    "                    break\n",
    "\n",
    "        if not fixed_version_diff:\n",
    "            continue\n",
    "\n",
    "        added, deleted, hunks = count_lines_and_hunks(fixed_version_diff)\n",
    "        project_results.append({\n",
    "            \"project\": os.path.basename(project_path),\n",
    "            \"commit\": commit,\n",
    "            \"lines_added\": added,\n",
    "            \"lines_deleted\": deleted,\n",
    "            \"hunks\": hunks\n",
    "        })\n",
    "\n",
    "    return project_results\n",
    "\n",
    "def process_all_projects(root_directory):\n",
    "    all_results = []\n",
    "    for project in os.listdir(root_directory):\n",
    "        project_path = os.path.join(root_directory, project)\n",
    "        if not os.path.isdir(project_path):\n",
    "            continue\n",
    "        print(f\"Processing project: {project}\")\n",
    "        project_results = process_project(project_path)\n",
    "        all_results.extend(project_results)\n",
    "        print(f\"Project {project} completed.\")\n",
    "    return all_results\n",
    "\n",
    "all_results = process_all_projects(root_directory)\n",
    "\n",
    "with open(output_csv, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    fieldnames = [\"project\", \"commit\", \"lines_added\", \"lines_deleted\", \"hunks\"]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(all_results)\n",
    "\n",
    "print(f\"Line statistics with hunks for all projects saved to: {output_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b075642",
   "metadata": {},
   "source": [
    "# finding overlapping changes\n",
    "# one project\n",
    "# only check if any added file is modified\n",
    "# removing empty line change and other cosmetic change like lines containing ony {"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad1d437",
   "metadata": {},
   "source": [
    "# addin one more column how many future versions have overlapping change. then i will add security issue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230ed6f8",
   "metadata": {},
   "source": [
    "# how many lines were added in the fixed version\n",
    "# how many lines were deleted in the fixed version, and\n",
    "# how many hunks were there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa21b9d",
   "metadata": {},
   "source": [
    "# ignore comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba306d4",
   "metadata": {},
   "source": [
    "# how many lines are changed in overlapped version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca66c1e",
   "metadata": {},
   "source": [
    "# how many overlapped changes.\n",
    "# means how many of the added lines are deleted in the ovelapped version, the count is added"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856aa1e9",
   "metadata": {},
   "source": [
    "# add how many hunks are present in the overlapped version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e7d4bb",
   "metadata": {},
   "source": [
    "# overlapped lines are from how many hunks , added"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed5ae75",
   "metadata": {},
   "source": [
    "# remove debug log and add 1 universal log."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d1fa73",
   "metadata": {},
   "source": [
    "# do it for all the projects. with break condition, result stored in each project folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307afa58",
   "metadata": {},
   "source": [
    "# optimize code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d2d45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "root_directory = r\"M:\\\\FULL_DATA_COLLECTED\"\n",
    "\n",
    "universal_log_file = os.path.join(root_directory, \"universal_error_log.txt\")\n",
    "\n",
    "with open(universal_log_file, \"w\", encoding=\"utf-8\") as log_file:\n",
    "    log_file.write(\"Universal Log File for Errors\\n\")\n",
    "\n",
    "log_file = open(universal_log_file, \"a\", encoding=\"utf-8\")\n",
    "\n",
    "def log_error(message):\n",
    "    \"\"\"\n",
    "    Append an error message to the universal log file.\n",
    "    \"\"\"\n",
    "    log_file.write(message + \"\\n\")\n",
    "\n",
    "def parse_lines_and_hunks(diff_file, line_type=\"+\"):\n",
    "    \"\"\"\n",
    "    Parse lines and hunks from a diff file to extract added or removed lines.\n",
    "    Tracks hunk boundaries for overlap analysis.\n",
    "    Skips trivial lines, diff headers, and comments.\n",
    "    \"\"\"\n",
    "    lines = set()\n",
    "    hunks = []\n",
    "    current_hunk = []\n",
    "    try:\n",
    "        with open(diff_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                if line.startswith(\"@@\"):\n",
    "                    if current_hunk:\n",
    "                        hunks.append(current_hunk)\n",
    "                    current_hunk = []\n",
    "                elif line.startswith(line_type) and not line.startswith(line_type * 3):\n",
    "                    original_line = line[1:].strip()\n",
    "\n",
    "                    if original_line in [\"{\", \"}\", \";\", \"(\", \")\"]:\n",
    "                        continue\n",
    "\n",
    "                    if original_line.startswith(\"---\") or original_line.startswith(\"+++\"):\n",
    "                        continue\n",
    "\n",
    "                    if original_line.startswith(\"//\") or original_line.startswith(\"/*\") or original_line.endswith(\"*/\"):\n",
    "                        continue\n",
    "\n",
    "                    if not original_line.strip():\n",
    "                        continue\n",
    "\n",
    "                    lines.add(original_line)\n",
    "\n",
    "                    if current_hunk is not None:\n",
    "                        current_hunk.append(original_line)\n",
    "            if current_hunk:\n",
    "                hunks.append(current_hunk)\n",
    "    except Exception as e:\n",
    "        log_error(f\"Error parsing file {diff_file}: {str(e)}\")\n",
    "    return lines, hunks\n",
    "\n",
    "def count_lines_and_hunks(diff_file):\n",
    "    \"\"\"\n",
    "    Count added, deleted lines, and hunks in a diff file.\n",
    "    \"\"\"\n",
    "    added = deleted = hunks = 0\n",
    "    try:\n",
    "        with open(diff_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                if line.startswith(\"+\") and not line.startswith(\"+++\"):\n",
    "                    added += 1\n",
    "                elif line.startswith(\"-\") and not line.startswith(\"---\"):\n",
    "                    deleted += 1\n",
    "                elif line.startswith(\"@@\"):\n",
    "                    hunks += 1\n",
    "    except Exception as e:\n",
    "        log_error(f\"Error counting lines in file {diff_file}: {str(e)}\")\n",
    "    return added, deleted, hunks\n",
    "\n",
    "def find_overlapping_changes(project_dir):\n",
    "    \"\"\"\n",
    "    Identify overlapping changes between the fixed version and subsequent versions.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    changes_dir = os.path.join(project_dir, \"file_changes_in_versions\")\n",
    "\n",
    "    if not os.path.exists(changes_dir):\n",
    "        log_error(f\"No changes directory found in {project_dir}\")\n",
    "        return\n",
    "\n",
    "    fixed_version_diff = next(\n",
    "        (os.path.join(changes_dir, file) for file in os.listdir(changes_dir) if \"fixed_version\" in file and file.endswith(\".txt\")),\n",
    "        None\n",
    "    )\n",
    "    if not fixed_version_diff:\n",
    "        log_error(f\"No fixed version diff found in {project_dir}\")\n",
    "        return\n",
    "\n",
    "    fixed_version_added, _ = parse_lines_and_hunks(fixed_version_diff, line_type=\"+\")\n",
    "\n",
    "    lines_added, lines_deleted, hunks_in_fixed_version = count_lines_and_hunks(fixed_version_diff)\n",
    "\n",
    "    files = [file for file in os.listdir(changes_dir) if file.endswith(\".txt\") and \"fixed_version\" not in file]\n",
    "    overlapping_changes = {}\n",
    "    total_versions = len(files)\n",
    "\n",
    "    for file in files:\n",
    "        future_version_diff = os.path.join(changes_dir, file)\n",
    "        removed_lines, future_hunks = parse_lines_and_hunks(future_version_diff, line_type=\"-\")\n",
    "\n",
    "        overlap = fixed_version_added & removed_lines\n",
    "        if overlap:\n",
    "            added_in_version, deleted_in_version, hunks_in_version = count_lines_and_hunks(future_version_diff)\n",
    "            deduplicated_overlap = list(overlap)\n",
    "            lines_overlapped = len(deduplicated_overlap)\n",
    "\n",
    "            overlapped_hunks = sum(\n",
    "                1 for hunk in future_hunks if any(line in hunk for line in deduplicated_overlap)\n",
    "            )\n",
    "            overlapping_changes[file] = {\n",
    "                \"Overlapping Lines\": deduplicated_overlap,\n",
    "                \"Lines Added in Version\": added_in_version,\n",
    "                \"Lines Deleted in Version\": deleted_in_version,\n",
    "                \"Lines Overlapped\": lines_overlapped,\n",
    "                \"Hunks in Version\": hunks_in_version,\n",
    "                \"Overlapped Hunks\": overlapped_hunks\n",
    "            }\n",
    "\n",
    "    if overlapping_changes:\n",
    "        results.update({\n",
    "            \"Total Future Versions\": total_versions,\n",
    "            \"Future Versions with Overlaps\": len(overlapping_changes),\n",
    "            \"Lines Added in Fixed Version\": lines_added,\n",
    "            \"Lines Deleted in Fixed Version\": lines_deleted,\n",
    "            \"Hunks in Fixed Version\": hunks_in_fixed_version,\n",
    "            \"Overlapping Changes\": overlapping_changes\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "def process_project(project_path):\n",
    "    \"\"\"\n",
    "    Process all commit directories within the project to identify overlapping changes.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for commit in os.listdir(project_path):\n",
    "        commit_dir = os.path.join(project_path, commit)\n",
    "        if not os.path.isdir(commit_dir):\n",
    "            continue\n",
    "        commit_results = find_overlapping_changes(commit_dir)\n",
    "        if commit_results:\n",
    "            results[commit] = commit_results\n",
    "\n",
    "    return results\n",
    "\n",
    "def process_projects_in_range(start_index, end_index):\n",
    "    \"\"\"\n",
    "    Process projects within the given index range.\n",
    "    \"\"\"\n",
    "    all_projects = sorted(\n",
    "        [project for project in os.listdir(root_directory) if os.path.isdir(os.path.join(root_directory, project))],\n",
    "        key=lambda x: x.lower()\n",
    "    )\n",
    "\n",
    "    selected_projects = all_projects[start_index - 1 : end_index]\n",
    "\n",
    "    for project in selected_projects:\n",
    "        project_path = os.path.join(root_directory, project)\n",
    "        if not os.path.isdir(project_path):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            print(f\"Processing project: {project}\")\n",
    "            project_results = process_project(project_path)\n",
    "            output_json = os.path.join(project_path, \"overlapping_changes_with_counts.json\")\n",
    "            with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(project_results, f, indent=4)\n",
    "        except Exception as e:\n",
    "            log_error(f\"Error processing project {project}: {str(e)}\")\n",
    "\n",
    "start_index = 1\n",
    "end_index = 2\n",
    "process_projects_in_range(start_index, end_index)\n",
    "print(\"Processing complete.\")\n",
    "\n",
    "log_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8e59e5",
   "metadata": {},
   "source": [
    "# adding 25 line proximity thing\n",
    "# adding with current version. overlap + proximity, only proximity no"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e89037",
   "metadata": {},
   "source": [
    "# only proximity, under construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b083ff5",
   "metadata": {},
   "source": [
    "# start over"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d7a61a",
   "metadata": {},
   "source": [
    "# log update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdd885b",
   "metadata": {},
   "source": [
    "# flatten abrt json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a277a127",
   "metadata": {},
   "source": [
    "# proximity wrong. was line based. should be content based"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4384b0",
   "metadata": {},
   "source": [
    "# \\\\ character issue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12da325",
   "metadata": {},
   "source": [
    "# remove 2 unncessary proxmity line fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60afeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "root_directory = r\"M:\\\\FULL_DATA_COLLECTED\"\n",
    "\n",
    "universal_log_file = os.path.join(root_directory, \"universal_error_log.txt\")\n",
    "\n",
    "def log_error(message, log_file):\n",
    "    \"\"\"\n",
    "    Logs an error message to the log file.\n",
    "    \"\"\"\n",
    "    log_file.write(message + \"\\n\")\n",
    "\n",
    "def parse_lines_and_hunks(diff_file, line_type=\"+\"):\n",
    "    \"\"\"\n",
    "    Parse lines and hunks from a diff file to extract added or removed lines.\n",
    "    Tracks hunk boundaries for overlap analysis and proximity checks.\n",
    "    \"\"\"\n",
    "    lines = {}\n",
    "    hunks = []\n",
    "    current_hunk = []\n",
    "    current_line_number = 0\n",
    "\n",
    "    try:\n",
    "        with open(diff_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                if line.startswith(\"@@\"):\n",
    "                    hunk_info = line.split(\" \")\n",
    "                    if len(hunk_info) > 1:\n",
    "                        current_line_number = int(hunk_info[2].split(\",\")[0].lstrip(\"+\"))\n",
    "                    if current_hunk:\n",
    "                        hunks.append(current_hunk)\n",
    "                    current_hunk = []\n",
    "                elif line.startswith(line_type) and not line.startswith(line_type * 3):\n",
    "                    original_line = line[1:].strip()\n",
    "                    if original_line in [\"{\", \"}\", \";\", \"(\", \")\", \"\", \"//\", \"/*\", \"*/\"]:\n",
    "                        continue\n",
    "                    lines[original_line] = current_line_number\n",
    "                    current_line_number += 1\n",
    "                    current_hunk.append(original_line)\n",
    "            if current_hunk:\n",
    "                hunks.append(current_hunk)\n",
    "    except Exception as e:\n",
    "        log_error(f\"Error parsing file {diff_file}: {str(e)}\", log_file)\n",
    "    return lines, hunks\n",
    "\n",
    "def count_lines_and_hunks(diff_file):\n",
    "    added = deleted = hunks = 0\n",
    "    try:\n",
    "        with open(diff_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                if line.startswith(\"+\") and not line.startswith(\"+++\"):\n",
    "                    added += 1\n",
    "                elif line.startswith(\"-\") and not line.startswith(\"---\"):\n",
    "                    deleted += 1\n",
    "                elif line.startswith(\"@@\"):\n",
    "                    hunks += 1\n",
    "    except Exception as e:\n",
    "        log_error(f\"Error counting lines in file {diff_file}: {str(e)}\", log_file)\n",
    "    return added, deleted, hunks\n",
    "\n",
    "def find_overlapping_and_proximity_changes(project_dir, log_file):\n",
    "    results = {}\n",
    "    changes_dir = os.path.join(project_dir, \"file_changes_in_versions\")\n",
    "\n",
    "    if not os.path.exists(changes_dir):\n",
    "        log_error(f\"No changes directory found in {project_dir}\", log_file)\n",
    "        return\n",
    "\n",
    "    fixed_version_diff = next(\n",
    "        (os.path.join(changes_dir, file) for file in os.listdir(changes_dir) if \"fixed_version\" in file and file.endswith(\".txt\")),\n",
    "        None\n",
    "    )\n",
    "    if not fixed_version_diff:\n",
    "        log_error(f\"No fixed version diff found in {project_dir}\", log_file)\n",
    "        return\n",
    "\n",
    "    fixed_version_added, _ = parse_lines_and_hunks(fixed_version_diff, line_type=\"+\")\n",
    "    lines_added, lines_deleted, hunks_in_fixed_version = count_lines_and_hunks(fixed_version_diff)\n",
    "\n",
    "    files = [file for file in os.listdir(changes_dir) if file.endswith(\".txt\") and \"fixed_version\" not in file]\n",
    "    overlapping_changes = {}\n",
    "    proximity_changes = {}\n",
    "    total_versions = len(files)\n",
    "\n",
    "    for file in files:\n",
    "        future_version_diff = os.path.join(changes_dir, file)\n",
    "        removed_lines, future_hunks = parse_lines_and_hunks(future_version_diff, line_type=\"-\")\n",
    "        added_lines, _ = parse_lines_and_hunks(future_version_diff, line_type=\"+\")\n",
    "\n",
    "        overlap = set(fixed_version_added.keys()) & set(removed_lines.keys())\n",
    "        if overlap:\n",
    "            added_in_version, deleted_in_version, hunks_in_version = count_lines_and_hunks(future_version_diff)\n",
    "            deduplicated_overlap = list(overlap)\n",
    "            lines_overlapped = len(deduplicated_overlap)\n",
    "            overlapped_hunks = sum(\n",
    "                1 for hunk in future_hunks if any(line in hunk for line in deduplicated_overlap)\n",
    "            )\n",
    "            overlapping_changes[file] = {\n",
    "                \"Overlapping Lines\": deduplicated_overlap,\n",
    "                \"Lines Added in Version\": added_in_version,\n",
    "                \"Lines Deleted in Version\": deleted_in_version,\n",
    "                \"Lines Overlapped\": lines_overlapped,\n",
    "                \"Hunks in Version\": hunks_in_version,\n",
    "                \"Overlapped Hunks\": overlapped_hunks\n",
    "            }\n",
    "\n",
    "        proximity_found = False\n",
    "        proximity_details = []\n",
    "        for fixed_line in fixed_version_added:\n",
    "            if fixed_line in added_lines:\n",
    "                for added_line in added_lines:\n",
    "                    if added_line != fixed_line:\n",
    "                        proximity_found = True\n",
    "                        proximity_details.append({\n",
    "                            \"Fixed Line\": fixed_line,\n",
    "                            \"Proximate Line\": added_line\n",
    "                        })\n",
    "                        break\n",
    "                if proximity_found:\n",
    "                    break\n",
    "\n",
    "        if proximity_found:\n",
    "            added_in_version, deleted_in_version, hunks_in_version = count_lines_and_hunks(future_version_diff)\n",
    "            proximity_changes[file] = {\n",
    "                \"Proximity Found\": True,\n",
    "                \"Proximity Details\": proximity_details,\n",
    "                \"Lines Added in Version\": added_in_version,\n",
    "                \"Lines Deleted in Version\": deleted_in_version,\n",
    "                \"Hunks in Version\": hunks_in_version\n",
    "            }\n",
    "\n",
    "    results.update({\n",
    "        \"Total Future Versions\": total_versions,\n",
    "        \"Future Versions with Overlaps\": len(overlapping_changes),\n",
    "        \"Future Versions with Proximity\": len(proximity_changes),\n",
    "        \"Lines Added in Fixed Version\": lines_added,\n",
    "        \"Lines Deleted in Fixed Version\": lines_deleted,\n",
    "        \"Hunks in Fixed Version\": hunks_in_fixed_version,\n",
    "        \"Overlapping Changes\": overlapping_changes,\n",
    "        \"Proximity Changes\": proximity_changes\n",
    "    })\n",
    "\n",
    "    return results\n",
    "\n",
    "def process_project(project_path, log_file):\n",
    "    results = {}\n",
    "    for commit in os.listdir(project_path):\n",
    "        commit_dir = os.path.join(project_path, commit)\n",
    "        if not os.path.isdir(commit_dir):\n",
    "            continue\n",
    "        commit_results = find_overlapping_and_proximity_changes(commit_dir, log_file)\n",
    "        if commit_results:\n",
    "            results[commit] = commit_results\n",
    "\n",
    "    return results\n",
    "\n",
    "def process_projects_in_range(start_index, end_index):\n",
    "    \"\"\"\n",
    "    Process projects within the specified range and log completion.\n",
    "    \"\"\"\n",
    "    all_projects = sorted(\n",
    "        [project for project in os.listdir(root_directory) if os.path.isdir(os.path.join(root_directory, project))],\n",
    "        key=lambda x: x.lower()\n",
    "    )\n",
    "    selected_projects = all_projects[start_index - 1 : end_index]\n",
    "\n",
    "    with open(universal_log_file, \"a\", encoding=\"utf-8\") as log_file:\n",
    "        for project in selected_projects:\n",
    "            project_path = os.path.join(root_directory, project)\n",
    "            if not os.path.isdir(project_path):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                print(f\"Processing project: {project}\")\n",
    "                project_results = process_project(project_path, log_file)\n",
    "                output_json = os.path.join(project_path, \"overlapping_and_proximity_changes.json\")\n",
    "                with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(project_results, f, indent=4)\n",
    "                log_file.write(f\"Project '{project}' completed.\\n\")\n",
    "            except Exception as e:\n",
    "                log_error(f\"Error processing project '{project}': {str(e)}\", log_file)\n",
    "\n",
    "start_index = 101\n",
    "end_index = 151\n",
    "process_projects_in_range(start_index, end_index)\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efe8142",
   "metadata": {},
   "source": [
    "# flatten json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6537677b",
   "metadata": {},
   "source": [
    "# update to merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0491256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "\n",
    "root_directory = r\"M:\\\\FULL_DATA_COLLECTED\"\n",
    "\n",
    "output_csv = os.path.join(root_directory, \"flattened_output.csv\")\n",
    "\n",
    "error_log_path = os.path.join(root_directory, \"flattening_errors.log\")\n",
    "\n",
    "def extract_version(file_name):\n",
    "    \"\"\"\n",
    "    Extract version numbers from a file name and return as a tuple of integers.\n",
    "    \"\"\"\n",
    "    match = re.search(r'v(\\d+)_v(\\d+)', file_name)\n",
    "    if match:\n",
    "        return int(match.group(1)), int(match.group(2))\n",
    "    return float('inf'), float('inf')\n",
    "\n",
    "def extract_file_name(found_version):\n",
    "    \"\"\"\n",
    "    Extract the base file name from a 'Found Version' string, e.g.,\n",
    "    'abrt-hook-ccpp.c_changes_v14_v15.txt' -> 'abrt-hook-ccpp.c'.\n",
    "    \"\"\"\n",
    "    match = re.match(r'^(.*)_changes_v\\d+_v\\d+\\.txt$', found_version)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def flatten_project_json(project_name, json_data):\n",
    "    \"\"\"\n",
    "    Flatten the JSON data for a single project into a tabular format.\n",
    "    \"\"\"\n",
    "    flattened_rows = []\n",
    "    for commit, commit_data in json_data.items():\n",
    "        base_columns = {\n",
    "            \"Project\": project_name,\n",
    "            \"Commit\": commit,\n",
    "            \"Total Future Versions\": commit_data.get(\"Total Future Versions\"),\n",
    "            \"Future Versions with Overlaps\": commit_data.get(\"Future Versions with Overlaps\"),\n",
    "            \"Future Versions with Proximity\": commit_data.get(\"Future Versions with Proximity\"),\n",
    "            \"Lines Added in Fixed Version\": commit_data.get(\"Lines Added in Fixed Version\"),\n",
    "            \"Lines Deleted in Fixed Version\": commit_data.get(\"Lines Deleted in Fixed Version\"),\n",
    "            \"Hunks in Fixed Version\": commit_data.get(\"Hunks in Fixed Version\"),\n",
    "        }\n",
    "\n",
    "        all_versions = set()\n",
    "        if \"Overlapping Changes\" in commit_data:\n",
    "            all_versions.update(commit_data[\"Overlapping Changes\"].keys())\n",
    "        if \"Proximity Changes\" in commit_data:\n",
    "            all_versions.update(commit_data[\"Proximity Changes\"].keys())\n",
    "\n",
    "        sorted_versions = sorted(all_versions, key=extract_version)\n",
    "\n",
    "        for version in sorted_versions:\n",
    "            row = base_columns.copy()\n",
    "            row[\"Found Version\"] = version\n",
    "            row[\"File\"] = extract_file_name(version)\n",
    "\n",
    "            overlap_data = commit_data.get(\"Overlapping Changes\", {}).get(version)\n",
    "            if overlap_data:\n",
    "                row[\"Overlapping Found\"] = True\n",
    "                row.update({\n",
    "                    \"Proximity Found\": False,\n",
    "                    \"Lines Added in Version\": overlap_data.get(\"Lines Added in Version\"),\n",
    "                    \"Lines Deleted in Version\": overlap_data.get(\"Lines Deleted in Version\"),\n",
    "                    \"Lines Overlapped\": overlap_data.get(\"Lines Overlapped\"),\n",
    "                    \"Hunks in Version\": overlap_data.get(\"Hunks in Version\"),\n",
    "                    \"Overlapped Hunks\": overlap_data.get(\"Overlapped Hunks\"),\n",
    "                })\n",
    "            else:\n",
    "                row[\"Overlapping Found\"] = False\n",
    "\n",
    "            proximity_data = commit_data.get(\"Proximity Changes\", {}).get(version)\n",
    "            if proximity_data:\n",
    "                row[\"Proximity Found\"] = True\n",
    "                row.update({\n",
    "                    \"Lines Added in Version\": proximity_data.get(\"Lines Added in Version\"),\n",
    "                    \"Lines Deleted in Version\": proximity_data.get(\"Lines Deleted in Version\"),\n",
    "                    \"Hunks in Version\": proximity_data.get(\"Hunks in Version\"),\n",
    "                    \"Lines Overlapped\": row.get(\"Lines Overlapped\", None),\n",
    "                    \"Overlapped Hunks\": row.get(\"Overlapped Hunks\", None),\n",
    "                })\n",
    "\n",
    "            if row[\"Overlapping Found\"] or row[\"Proximity Found\"]:\n",
    "                flattened_rows.append(row)\n",
    "\n",
    "    return flattened_rows\n",
    "\n",
    "def flatten_all_projects(root_directory, output_csv):\n",
    "    \"\"\"\n",
    "    Flatten JSON files for all projects in the root directory into a single CSV file.\n",
    "    \"\"\"\n",
    "    all_rows = []\n",
    "    with open(error_log_path, \"w\") as error_log:\n",
    "        for project_name in os.listdir(root_directory):\n",
    "            project_path = os.path.join(root_directory, project_name)\n",
    "            if not os.path.isdir(project_path):\n",
    "                continue\n",
    "\n",
    "            json_file = os.path.join(project_path, \"overlapping_and_proximity_changes.json\")\n",
    "            if os.path.exists(json_file):\n",
    "                print(f\"Processing JSON for project: {project_name}\")\n",
    "                try:\n",
    "                    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                        json_data = json.load(f)\n",
    "                    flattened_rows = flatten_project_json(project_name, json_data)\n",
    "                    all_rows.extend(flattened_rows)\n",
    "                except Exception as e:\n",
    "                    error_log.write(f\"Error processing {json_file} for project {project_name}: {str(e)}\\n\")\n",
    "            else:\n",
    "                error_log.write(f\"JSON file missing for project {project_name}\\n\")\n",
    "\n",
    "    if all_rows:\n",
    "        df = pd.DataFrame(all_rows)\n",
    "        df.to_csv(output_csv, index=False, encoding=\"utf-8\")\n",
    "        print(f\"Flattened data saved to {output_csv}\")\n",
    "    else:\n",
    "        print(\"No data to flatten.\")\n",
    "\n",
    "start_time = time.time()\n",
    "flatten_all_projects(root_directory, output_csv)\n",
    "end_time = time.time()\n",
    "print(f\"Execution Time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db80d88",
   "metadata": {},
   "source": [
    "# merge security pattern with flattened json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1615256",
   "metadata": {},
   "source": [
    "# with specific range of prjects for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7191f3",
   "metadata": {},
   "source": [
    "# full project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1740d788",
   "metadata": {},
   "source": [
    "# overlapping|proximity and any security"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e4c6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "root_directory = r\"M:\\FULL_DATA_COLLECTED\"\n",
    "flattened_file_path = os.path.join(root_directory, \"flattened_output.csv\")\n",
    "security_file_path = os.path.join(root_directory, \"security_patterns_unified.csv\")\n",
    "merged_output_path = os.path.join(root_directory, \"merged_output.csv\")\n",
    "\n",
    "flattened_df = pd.read_csv(flattened_file_path)\n",
    "security_df = pd.read_csv(security_file_path)\n",
    "\n",
    "merged_df = pd.merge(flattened_df, security_df, how='outer', on=['Project', 'Commit', 'Found Version'])\n",
    "\n",
    "overlap_proximity_columns = ['Overlapping Found', 'Proximity Found']\n",
    "for col in overlap_proximity_columns:\n",
    "    if col not in merged_df.columns:\n",
    "        merged_df[col] = False\n",
    "    merged_df[col] = merged_df[col].fillna(False)\n",
    "\n",
    "if 'Syntactic Confidence' not in merged_df.columns:\n",
    "    merged_df['Syntactic Confidence'] = 0\n",
    "else:\n",
    "    merged_df['Syntactic Confidence'] = merged_df['Syntactic Confidence'].fillna(0)\n",
    "\n",
    "merged_df.to_csv(merged_output_path, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\" Merged output saved to: {merged_output_path}\")\n",
    "print(f\"Total records in merged output: {len(merged_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5f54e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Total distinct (Project, Commit) pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbfa2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "root_directory = r\"M:\\FULL_DATA_COLLECTED\"\n",
    "\n",
    "merged_file_path = os.path.join(root_directory, \"merged_output.csv\")\n",
    "\n",
    "merged_df = pd.read_csv(merged_file_path)\n",
    "\n",
    "unique_commits_count = merged_df.groupby([\"Project\", \"Commit\"]).ngroups\n",
    "\n",
    "print(f\" Total distinct (Project, Commit) pairs: {unique_commits_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef08d0d",
   "metadata": {},
   "source": [
    "# filtering merged output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6583f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "root_directory = r\"M:\\FULL_DATA_COLLECTED\"\n",
    "merged_output_path = os.path.join(root_directory, \"merged_output.csv\")\n",
    "merged_df = pd.read_csv(merged_output_path)\n",
    "\n",
    "merged_df[\"Overlapping Found\"] = merged_df[\"Overlapping Found\"].astype(str).str.lower().eq(\"true\")\n",
    "merged_df[\"Proximity Found\"] = merged_df[\"Proximity Found\"].astype(str).str.lower().eq(\"true\")\n",
    "\n",
    "security_columns = [\"Matches (High)\", \"Matches (Medium)\", \"Bug Numbers\", \"Plain Numbers\", \"Keywords\"]\n",
    "\n",
    "security_filter = merged_df[security_columns].apply(lambda row: any(pd.notna(row) & (row.astype(str).str.strip() != \"\")), axis=1)\n",
    "\n",
    "overlap_proximity_filter = merged_df[\"Overlapping Found\"] | merged_df[\"Proximity Found\"]\n",
    "\n",
    "syntactic_score_2_filter = merged_df[\"Syntactic Confidence\"] == 2\n",
    "syntactic_score_greater_than_0_filter = merged_df[\"Syntactic Confidence\"] > 0\n",
    "\n",
    "records_with_both = merged_df[overlap_proximity_filter & security_filter]\n",
    "records_with_overlap_only = merged_df[overlap_proximity_filter & ~security_filter]\n",
    "records_with_security_only = merged_df[security_filter & ~overlap_proximity_filter]\n",
    "records_with_both_and_syntactic_2 = merged_df[overlap_proximity_filter & security_filter & syntactic_score_2_filter]\n",
    "records_with_both_and_syntactic_gt_0 = merged_df[overlap_proximity_filter & security_filter & syntactic_score_greater_than_0_filter]\n",
    "\n",
    "records_with_both.to_csv(os.path.join(root_directory, \"records_with_overlap_and_security.csv\"), index=False, encoding=\"utf-8\")\n",
    "records_with_overlap_only.to_csv(os.path.join(root_directory, \"records_with_overlap_only.csv\"), index=False, encoding=\"utf-8\")\n",
    "records_with_security_only.to_csv(os.path.join(root_directory, \"records_with_security_only.csv\"), index=False, encoding=\"utf-8\")\n",
    "records_with_both_and_syntactic_2.to_csv(os.path.join(root_directory, \"records_with_overlap_security_syntactic_2.csv\"), index=False, encoding=\"utf-8\")\n",
    "records_with_both_and_syntactic_gt_0.to_csv(os.path.join(root_directory, \"records_with_overlap_security_syntactic_gt_0.csv\"), index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Total records: {len(merged_df)}\")\n",
    "print(f\"Records with (Overlapping OR Proximity) AND Security Keywords: {len(records_with_both)}\")\n",
    "print(f\"Records with only Overlapping/Proximity but no Security Keywords: {len(records_with_overlap_only)}\")\n",
    "print(f\"Records with only Security Keywords but no Overlapping/Proximity: {len(records_with_security_only)}\")\n",
    "print(f\"Records with (Overlapping OR Proximity) AND Security Keywords AND Syntactic Confidence = 2: {len(records_with_both_and_syntactic_2)}\")\n",
    "print(f\"Records with (Overlapping OR Proximity) AND Security Keywords AND Syntactic Confidence > 0: {len(records_with_both_and_syntactic_gt_0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f02a218",
   "metadata": {},
   "source": [
    "# count proximity # count overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38f638a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "file_path = r\"M:\\FULL_DATA_COLLECTED\\flattened_output.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df[\"Overlapping Found\"] = df[\"Overlapping Found\"].astype(str).str.lower().eq(\"true\")\n",
    "df[\"Proximity Found\"] = df[\"Proximity Found\"].astype(str).str.lower().eq(\"true\")\n",
    "\n",
    "overlapping_true_count = df[\"Overlapping Found\"].sum()\n",
    "proximity_true_count = df[\"Proximity Found\"].sum()\n",
    "\n",
    "total_records = len(df)\n",
    "\n",
    "print(\" Summary of Overlapping and Proximity Matches\")\n",
    "print(f\"Total Records Analyzed: {total_records}\")\n",
    "print(f\"Records where 'Overlapping Found' is TRUE: {overlapping_true_count}\")\n",
    "print(f\"Records where 'Proximity Found' is TRUE: {proximity_true_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44746069",
   "metadata": {},
   "source": [
    "# count other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26277d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "file_path = r\"M:\\FULL_DATA_COLLECTED\\security_patterns_unified.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "high_or_medium_present = (\n",
    "    df[\"Matches (High)\"].fillna(\"\").astype(str).str.strip() != \"\"\n",
    ") | (\n",
    "    df[\"Matches (Medium)\"].fillna(\"\").astype(str).str.strip() != \"\"\n",
    ")\n",
    "high_or_medium_count = high_or_medium_present.sum()\n",
    "\n",
    "bug_numbers_count = (df[\"Bug Numbers\"].fillna(\"\").astype(str).str.strip() != \"\").sum()\n",
    "plain_numbers_count = (df[\"Plain Numbers\"].fillna(\"\").astype(str).str.strip() != \"\").sum()\n",
    "keywords_count = (df[\"Keywords\"].fillna(\"\").astype(str).str.strip() != \"\").sum()\n",
    "\n",
    "combined_security_count = (\n",
    "    (df[\"Bug Numbers\"].fillna(\"\").astype(str).str.strip() != \"\") |\n",
    "    (df[\"Plain Numbers\"].fillna(\"\").astype(str).str.strip() != \"\") |\n",
    "    (df[\"Keywords\"].fillna(\"\").astype(str).str.strip() != \"\")\n",
    ").sum()\n",
    "\n",
    "print(\" Summary of Security Indicators\")\n",
    "print(f\"Records with Matches (High) or Matches (Medium): {high_or_medium_count}\")\n",
    "print(f\"Records with Bug Numbers, Plain Numbers, or Keywords: {combined_security_count}\")\n",
    "print(\" Breakdown:\")\n",
    "print(f\"  - Bug Numbers: {bug_numbers_count}\")\n",
    "print(f\"  - Plain Numbers: {plain_numbers_count}\")\n",
    "print(f\"  - Keywords: {keywords_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468fd02c",
   "metadata": {},
   "source": [
    "# statistics from all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86abb026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "root_directory = r\"M:\\FULL_DATA_COLLECTED\"\n",
    "\n",
    "for file in os.listdir(root_directory):\n",
    "    if file.endswith(\".csv\"):\n",
    "        file_path = os.path.join(root_directory, file)\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            total_records = len(df)\n",
    "\n",
    "            if \"Project\" in df.columns and \"Commit\" in df.columns:\n",
    "                unique_commits_count = df.groupby([\"Project\", \"Commit\"]).ngroups\n",
    "            else:\n",
    "                unique_commits_count = \"N/A\"\n",
    "\n",
    "            print(f\"\\n File: {file}\")\n",
    "            print(f\"    Total Records: {total_records}\")\n",
    "            print(f\"    Distinct (Project, Commit) Pairs: {unique_commits_count}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Error processing file {file}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183318f9",
   "metadata": {},
   "source": [
    "# taking 50 sample spanning among different projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f468ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = r\"M:\\FULL_DATA_COLLECTED\\records_with_overlap_and_security.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "if \"Project\" not in df.columns:\n",
    "    print(\"Error: 'Project' column not found in the dataset.\")\n",
    "    exit()\n",
    "\n",
    "df.rename(columns={\"Project\": \"project\"}, inplace=True)\n",
    "\n",
    "df.dropna(subset=[\"project\"], inplace=True)\n",
    "\n",
    "unique_projects = df[\"project\"].unique()\n",
    "samples_per_project = max(1, 50 // len(unique_projects))\n",
    "\n",
    "diverse_samples = df.groupby(\"project\", group_keys=False).apply(\n",
    "    lambda x: x.sample(min(samples_per_project, len(x)), random_state=42)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "diverse_samples = diverse_samples.head(50)\n",
    "\n",
    "output_file = r\"M:\\FULL_DATA_COLLECTED\\diverse_samples.csv\"\n",
    "diverse_samples.to_csv(output_file, index=False)\n",
    "print(f\"50 diverse samples saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6926984f",
   "metadata": {},
   "source": [
    "# generating prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cf1c0c",
   "metadata": {},
   "source": [
    "# remove debug and do for all samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015a316a",
   "metadata": {},
   "source": [
    "# modified in json output format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1597ee84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "BASE_DIR = r\"M:\\FULL_DATA_COLLECTED\"\n",
    "\n",
    "file_path = os.path.join(BASE_DIR, \"diverse_samples.csv\")\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "required_columns = [\"Commit\", \"File\", \"Found Version\", \"project\"]\n",
    "missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "if missing_columns:\n",
    "    print(f\" Error: Missing expected columns: {missing_columns}\")\n",
    "    exit()\n",
    "\n",
    "PROMPT_DIR = os.path.join(BASE_DIR, \"prompts\")\n",
    "os.makedirs(PROMPT_DIR, exist_ok=True)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    project_name = row[\"project\"].strip()\n",
    "    commit_hash = row[\"Commit\"].strip()\n",
    "    file_name = row[\"File\"].strip()\n",
    "    found_version = row[\"Found Version\"].strip()\n",
    "\n",
    "    project_dir = os.path.join(BASE_DIR, project_name)\n",
    "    if not os.path.exists(project_dir):\n",
    "        continue\n",
    "\n",
    "    available_commit_dirs = os.listdir(project_dir)\n",
    "    matching_dirs = [d for d in available_commit_dirs if commit_hash in d]\n",
    "    if not matching_dirs:\n",
    "        continue\n",
    "\n",
    "    commit_dir = os.path.join(project_dir, matching_dirs[0])\n",
    "\n",
    "    commit_messages_dir = os.path.join(commit_dir, \"commit_messages\")\n",
    "    file_changes_dir = os.path.join(commit_dir, \"file_changes_in_versions\")\n",
    "\n",
    "    version_match = re.search(r\"_v\\d+_(v\\d+)\\.txt\", found_version)\n",
    "    if not version_match:\n",
    "        continue\n",
    "    future_commit_version_number = version_match.group(1)\n",
    "\n",
    "    previous_fix_message = \"Previous fix commit message not found\"\n",
    "    expected_previous_fix_message_prefix = f\"{file_name}_v2_\"\n",
    "\n",
    "    if os.path.exists(commit_messages_dir):\n",
    "        commit_message_files = os.listdir(commit_messages_dir)\n",
    "        for msg_file in commit_message_files:\n",
    "            if (\n",
    "                msg_file.startswith(expected_previous_fix_message_prefix)\n",
    "                and (\"_commit_message_fixed_version.txt\" in msg_file or \"_commit_message.txt\" in msg_file)\n",
    "            ):\n",
    "                with open(os.path.join(commit_messages_dir, msg_file), \"r\", encoding=\"utf-8\") as f:\n",
    "                    previous_fix_message = f.read().strip()\n",
    "                break\n",
    "\n",
    "    previous_fix_diff_content = \"No previous fix code diff found\"\n",
    "    previous_fix_diff_file = f\"{file_name}_changes_v1_v2_fixed_version.txt\"\n",
    "\n",
    "    if os.path.exists(file_changes_dir) and previous_fix_diff_file in os.listdir(file_changes_dir):\n",
    "        with open(os.path.join(file_changes_dir, previous_fix_diff_file), \"r\", encoding=\"utf-8\") as f:\n",
    "            previous_fix_diff_content = f.read().strip()\n",
    "\n",
    "    future_commit_message = \"Future commit message not found\"\n",
    "    expected_future_commit_message_prefix = f\"{file_name}_{future_commit_version_number}_\"\n",
    "\n",
    "    if os.path.exists(commit_messages_dir):\n",
    "        for msg_file in commit_message_files:\n",
    "            if msg_file.startswith(expected_future_commit_message_prefix) and msg_file.endswith(\"_commit_message.txt\"):\n",
    "                with open(os.path.join(commit_messages_dir, msg_file), \"r\", encoding=\"utf-8\") as f:\n",
    "                    future_commit_message = f.read().strip()\n",
    "                break\n",
    "\n",
    "    future_diff_content = \"No future commit diff found\"\n",
    "    expected_future_diff_file = found_version\n",
    "\n",
    "    if os.path.exists(file_changes_dir) and expected_future_diff_file in os.listdir(file_changes_dir):\n",
    "        with open(os.path.join(file_changes_dir, expected_future_diff_file), \"r\", encoding=\"utf-8\") as f:\n",
    "            future_diff_content = f.read().strip()\n",
    "\n",
    "    prompt_content = f\"\"\"\n",
    "I am conducting research on how software vulnerabilities evolve over time, specifically focusing on situations where a fix for one vulnerability unintentionally introduces a new vulnerability.\n",
    "\n",
    "You will be provided with details from two commits:\n",
    "\n",
    "1. **Previous Fix Commit**  A commit that fixed a known vulnerability.\n",
    "2. **Future Candidate Commit**  A later commit that modifies the same or nearby code.\n",
    "\n",
    "Your task is to determine whether the **candidate commit** is fixing a new vulnerability that was introduced by the **previous fix**.\n",
    "\n",
    "---\n",
    "\n",
    "**Commit ID:** {commit_hash}\n",
    "**Commit Message:**\n",
    "{previous_fix_message}\n",
    "\n",
    "**Code Changes (Diff Format):**\n",
    "{previous_fix_diff_content}\n",
    "\n",
    "---\n",
    "\n",
    "**Commit Message:**\n",
    "{future_commit_message}\n",
    "\n",
    "**Code Changes (Diff Format):**\n",
    "{future_diff_content}\n",
    "\n",
    "---\n",
    "\n",
    "Please respond **only with a valid JSON object** in the following format:\n",
    "\n",
    "```{{\"answer\": \"Yes\" or \"No\", \"reasoning\": \"Detailed explanation of why the candidate commit is or is not fixing a vulnerability introduced by the previous fix.\"}}```\n",
    "\n",
    "**Do not include any extra text outside the JSON structure.**\n",
    "\"\"\"\n",
    "\n",
    "    prompt_file_path = os.path.join(PROMPT_DIR, f\"{project_name}_{commit_hash}.txt\")\n",
    "    with open(prompt_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(prompt_content.strip())\n",
    "\n",
    "print(\"\\n All prompts generated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7095c28",
   "metadata": {},
   "source": [
    "# modified prompt --- prompt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e88ba8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "BASE_DIR = r\"M:\\FULL_DATA_COLLECTED\"\n",
    "\n",
    "file_path = os.path.join(BASE_DIR, \"diverse_samples.csv\")\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "required_columns = [\"Commit\", \"File\", \"Found Version\", \"project\"]\n",
    "missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "if missing_columns:\n",
    "    print(f\" Error: Missing expected columns: {missing_columns}\")\n",
    "    exit()\n",
    "\n",
    "PROMPT_DIR = os.path.join(BASE_DIR, \"prompts2\")\n",
    "os.makedirs(PROMPT_DIR, exist_ok=True)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    project_name = row[\"project\"].strip()\n",
    "    commit_hash = row[\"Commit\"].strip()\n",
    "    file_name = row[\"File\"].strip()\n",
    "    found_version = row[\"Found Version\"].strip()\n",
    "\n",
    "    project_dir = os.path.join(BASE_DIR, project_name)\n",
    "    if not os.path.exists(project_dir):\n",
    "        continue\n",
    "\n",
    "    available_commit_dirs = os.listdir(project_dir)\n",
    "    matching_dirs = [d for d in available_commit_dirs if commit_hash in d]\n",
    "    if not matching_dirs:\n",
    "        continue\n",
    "\n",
    "    commit_dir = os.path.join(project_dir, matching_dirs[0])\n",
    "\n",
    "    commit_messages_dir = os.path.join(commit_dir, \"commit_messages\")\n",
    "    file_changes_dir = os.path.join(commit_dir, \"file_changes_in_versions\")\n",
    "\n",
    "    version_match = re.search(r\"_v\\d+_(v\\d+)\\.txt\", found_version)\n",
    "    if not version_match:\n",
    "        continue\n",
    "    future_commit_version_number = version_match.group(1)\n",
    "\n",
    "    previous_fix_message = \"Previous fix commit message not found\"\n",
    "    expected_previous_fix_message_prefix = f\"{file_name}_v2_\"\n",
    "\n",
    "    if os.path.exists(commit_messages_dir):\n",
    "        commit_message_files = os.listdir(commit_messages_dir)\n",
    "        for msg_file in commit_message_files:\n",
    "            if (\n",
    "                msg_file.startswith(expected_previous_fix_message_prefix)\n",
    "                and (\"_commit_message_fixed_version.txt\" in msg_file or \"_commit_message.txt\" in msg_file)\n",
    "            ):\n",
    "                with open(os.path.join(commit_messages_dir, msg_file), \"r\", encoding=\"utf-8\") as f:\n",
    "                    previous_fix_message = f.read().strip()\n",
    "                break\n",
    "\n",
    "    previous_fix_diff_content = \"No previous fix code diff found\"\n",
    "    previous_fix_diff_file = f\"{file_name}_changes_v1_v2_fixed_version.txt\"\n",
    "\n",
    "    if os.path.exists(file_changes_dir) and previous_fix_diff_file in os.listdir(file_changes_dir):\n",
    "        with open(os.path.join(file_changes_dir, previous_fix_diff_file), \"r\", encoding=\"utf-8\") as f:\n",
    "            previous_fix_diff_content = f.read().strip()\n",
    "\n",
    "    future_commit_message = \"Future commit message not found\"\n",
    "    expected_future_commit_message_prefix = f\"{file_name}_{future_commit_version_number}_\"\n",
    "\n",
    "    if os.path.exists(commit_messages_dir):\n",
    "        for msg_file in commit_message_files:\n",
    "            if msg_file.startswith(expected_future_commit_message_prefix) and msg_file.endswith(\"_commit_message.txt\"):\n",
    "                with open(os.path.join(commit_messages_dir, msg_file), \"r\", encoding=\"utf-8\") as f:\n",
    "                    future_commit_message = f.read().strip()\n",
    "                break\n",
    "\n",
    "    future_diff_content = \"No future commit diff found\"\n",
    "    expected_future_diff_file = found_version\n",
    "\n",
    "    if os.path.exists(file_changes_dir) and expected_future_diff_file in os.listdir(file_changes_dir):\n",
    "        with open(os.path.join(file_changes_dir, expected_future_diff_file), \"r\", encoding=\"utf-8\") as f:\n",
    "            future_diff_content = f.read().strip()\n",
    "\n",
    "    prompt_content = f\"\"\"\n",
    "I am conducting research on how software security vulnerabilities evolve over time, specifically focusing on situations where a fix for one security vulnerability unintentionally introduces a new security vulnerability.\n",
    "\n",
    "You will be provided with details from two commits:\n",
    "\n",
    "1. **Previous Fix Commit**  A commit that fixed a known vulnerability.\n",
    "2. **Future Candidate Commit**  A later commit that modifies the same or nearby code.\n",
    "\n",
    "Your task is to determine whether the **candidate commit** is fixing a new vulnerability that was introduced by the **previous fix**.\n",
    "\n",
    "---\n",
    "\n",
    "**Commit ID:** {commit_hash}\n",
    "**Commit Message:**\n",
    "{previous_fix_message}\n",
    "\n",
    "**Code Changes (Diff Format):**\n",
    "{previous_fix_diff_content}\n",
    "\n",
    "---\n",
    "\n",
    "**Commit Message:**\n",
    "{future_commit_message}\n",
    "\n",
    "**Code Changes (Diff Format):**\n",
    "{future_diff_content}\n",
    "\n",
    "---\n",
    "\n",
    "Please respond **only with a valid JSON object** in the following format:\n",
    "\n",
    "```{{\"answer\": \"Yes\" or \"No\", \"reasoning\": \"Detailed explanation of why the candidate commit is or is not fixing a vulnerability introduced by the previous fix.\"}}```\n",
    "\n",
    "**Do not include any extra text outside the JSON structure.**\n",
    "**If the previous fix is incomplete, the answer is \"No\" since it did not introduce a new security vulnerability. Similarly, if the previous fix did not properly fix the issue, the answer is still \"No\" unless a new security vulnerability is created.**\n",
    "\"\"\"\n",
    "\n",
    "    prompt_file_path = os.path.join(PROMPT_DIR, f\"{project_name}_{commit_hash}.txt\")\n",
    "    with open(prompt_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(prompt_content.strip())\n",
    "\n",
    "print(\"\\n All prompts generated successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bdce57",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402d107f",
   "metadata": {},
   "source": [
    "# token counts for prompts\n",
    "# i want to discard too large. It is difficult to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e9dd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "\n",
    "BASE_DIR = r\"M:\\FULL_DATA_COLLECTED\"\n",
    "PROMPT_DIR = os.path.join(BASE_DIR, \"prompts2\")\n",
    "\n",
    "if not os.path.exists(PROMPT_DIR):\n",
    "    print(f\" Error: The directory {PROMPT_DIR} does not exist.\")\n",
    "    exit()\n",
    "\n",
    "prompt_files = [f for f in os.listdir(PROMPT_DIR) if f.endswith(\".txt\")]\n",
    "\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "\n",
    "token_counts = []\n",
    "\n",
    "for prompt_file in prompt_files:\n",
    "    prompt_path = os.path.join(PROMPT_DIR, prompt_file)\n",
    "\n",
    "    with open(prompt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        prompt_content = f.read()\n",
    "\n",
    "    token_count = len(tokenizer.encode(prompt_content))\n",
    "\n",
    "    token_counts.append({\"File\": prompt_file, \"Token Count\": token_count})\n",
    "\n",
    "df_tokens = pd.DataFrame(token_counts)\n",
    "\n",
    "output_excel_path = os.path.join(BASE_DIR, \"token_counts.xlsx\")\n",
    "df_tokens.to_excel(output_excel_path, index=False)\n",
    "\n",
    "print(f\"\\n Token count file saved at: {output_excel_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16a2083",
   "metadata": {},
   "source": [
    "# calculate accuracy and other metric for results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b0763b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = r\"M:\\FULL_DATA_COLLECTED\\accuracyCalculation.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "def calculate_metrics(ground_truth, predictions, model_name, total_records):\n",
    "    TP = ((predictions == \"Yes\") & (ground_truth == \"Yes\")).sum()\n",
    "    FP = ((predictions == \"Yes\") & (ground_truth == \"No\")).sum()\n",
    "    FN = ((predictions == \"No\") & (ground_truth == \"Yes\")).sum()\n",
    "    TN = ((predictions == \"No\") & (ground_truth == \"No\")).sum()\n",
    "\n",
    "    accuracy = (TP + TN) / total_records if total_records > 0 else 0\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"Model\": model_name,\n",
    "        \"Total Samples\": total_records,\n",
    "        \"True Positives (TP)\": TP,\n",
    "        \"False Positives (FP)\": FP,\n",
    "        \"False Negatives (FN)\": FN,\n",
    "        \"True Negatives (TN)\": TN,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1_score\n",
    "    }\n",
    "\n",
    "skipped_indices = df[df[\"GPTP1\"].isna()].index.tolist()\n",
    "\n",
    "model_names = [\"GeminiP1\", \"ClaudeP1\", \"GPTP1\",\"Grok1\",\"DeepSeek1\", \"GeminiP2\", \"ClaudeP2\", \"GPTP2\", \"Grok\", \"DeepSeek\", \"Claude0.3\", \"GPT0.3\"]\n",
    "results = []\n",
    "\n",
    "for model in model_names:\n",
    "    if \"GPT\" in model:\n",
    "        filtered_ground_truth = df[\"Ground Truth\"].drop(index=skipped_indices)\n",
    "        filtered_predictions = df[model].drop(index=skipped_indices)\n",
    "        results.append(calculate_metrics(filtered_ground_truth, filtered_predictions, model, total_records=48))\n",
    "    elif \"Grok\" in model:\n",
    "        filtered_ground_truth = df[\"Ground Truth\"].drop(index=skipped_indices)\n",
    "        filtered_predictions = df[model].drop(index=skipped_indices)\n",
    "        results.append(calculate_metrics(filtered_ground_truth, filtered_predictions, model, total_records=49))\n",
    "    elif \"DeepSeek\" in model:\n",
    "        filtered_ground_truth = df[\"Ground Truth\"].drop(index=skipped_indices)\n",
    "        filtered_predictions = df[model].drop(index=skipped_indices)\n",
    "        results.append(calculate_metrics(filtered_ground_truth, filtered_predictions, model, total_records=49))\n",
    "    else:\n",
    "        results.append(calculate_metrics(df[\"Ground Truth\"], df[model], model, total_records=50))\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "output_path = r\"M:\\FULL_DATA_COLLECTED\\accuracy_metrics.xlsx\"\n",
    "results_df.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"\\n Accuracy metrics saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004d548f",
   "metadata": {},
   "source": [
    "# for new samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6317a28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = r\"M:\\FULL_DATA_COLLECTED\\accuracyCalculation2.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "def calculate_metrics(ground_truth, predictions, model_name, total_records):\n",
    "    TP = ((predictions == \"Yes\") & (ground_truth == \"Yes\")).sum()\n",
    "    FP = ((predictions == \"Yes\") & (ground_truth == \"No\")).sum()\n",
    "    FN = ((predictions == \"No\") & (ground_truth == \"Yes\")).sum()\n",
    "    TN = ((predictions == \"No\") & (ground_truth == \"No\")).sum()\n",
    "\n",
    "    accuracy = (TP + TN) / total_records if total_records > 0 else 0\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"Model\": model_name,\n",
    "        \"Total Samples\": total_records,\n",
    "        \"True Positives (TP)\": TP,\n",
    "        \"False Positives (FP)\": FP,\n",
    "        \"False Negatives (FN)\": FN,\n",
    "        \"True Negatives (TN)\": TN,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1_score\n",
    "    }\n",
    "\n",
    "skipped_indices = df[df[\"GPTP1\"].isna()].index.tolist()\n",
    "\n",
    "model_names = [\"GeminiP1\", \"ClaudeP1\", \"GPTP1\", \"Grok\", \"DeepSeek\",\"Claude0.3\", \"GPT0.3\"]\n",
    "results = []\n",
    "\n",
    "for model in model_names:\n",
    "    if \"GPT\" in model:\n",
    "        filtered_ground_truth = df[\"Ground Truth\"].drop(index=skipped_indices)\n",
    "        filtered_predictions = df[model].drop(index=skipped_indices)\n",
    "        results.append(calculate_metrics(filtered_ground_truth, filtered_predictions, model, total_records=45))\n",
    "    else:\n",
    "        results.append(calculate_metrics(df[\"Ground Truth\"], df[model], model, total_records=45))\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "output_path = r\"M:\\FULL_DATA_COLLECTED\\accuracy_metrics2.xlsx\"\n",
    "results_df.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"\\n Accuracy metrics saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aab8f7d",
   "metadata": {},
   "source": [
    "# creating full prompt with range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f4862c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "BASE_DIR = r\"M:\\FULL_DATA_COLLECTED\"\n",
    "\n",
    "file_path = os.path.join(BASE_DIR, \"records_with_overlap_and_security.csv\")\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df.rename(columns={\"Project\": \"project\"}, inplace=True)\n",
    "\n",
    "PROMPT_DIR = os.path.join(BASE_DIR, \"PromptsFull\")\n",
    "os.makedirs(PROMPT_DIR, exist_ok=True)\n",
    "\n",
    "START_INDEX = 501\n",
    "END_INDEX = 9057\n",
    "\n",
    "generated_count = 0\n",
    "\n",
    "for i, (_, row) in enumerate(df.iloc[START_INDEX - 1:END_INDEX].iterrows(), start=START_INDEX):\n",
    "    project_name = str(row[\"project\"]).strip()\n",
    "    commit_hash = str(row[\"Commit\"]).strip()\n",
    "    file_name = str(row[\"File\"]).strip()\n",
    "    found_version = str(row[\"Found Version\"]).strip().replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n",
    "\n",
    "    project_dir = os.path.join(BASE_DIR, project_name)\n",
    "    if not os.path.exists(project_dir):\n",
    "        print(f\" Skipped: Project directory not found  {project_dir}\")\n",
    "        continue\n",
    "\n",
    "    available_commit_dirs = os.listdir(project_dir)\n",
    "    matching_dirs = [d for d in available_commit_dirs if commit_hash in d]\n",
    "    if not matching_dirs:\n",
    "        print(f\" Skipped: No matching commit directory for {commit_hash}\")\n",
    "        continue\n",
    "\n",
    "    commit_dir = os.path.join(project_dir, matching_dirs[0])\n",
    "\n",
    "    commit_messages_dir = os.path.join(commit_dir, \"commit_messages\")\n",
    "    file_changes_dir = os.path.join(commit_dir, \"file_changes_in_versions\")\n",
    "\n",
    "    version_match = re.search(r\"_v\\d+_(v\\d+)\\.txt\", found_version)\n",
    "    if not version_match:\n",
    "        print(f\" Skipped: Could not extract future commit version from {found_version}\")\n",
    "        continue\n",
    "    future_commit_version_number = version_match.group(1)\n",
    "\n",
    "    previous_fix_message = \"Previous fix commit message not found\"\n",
    "    expected_previous_fix_message_prefix = f\"{file_name}_v2_\"\n",
    "\n",
    "    if os.path.exists(commit_messages_dir):\n",
    "        commit_message_files = os.listdir(commit_messages_dir)\n",
    "        for msg_file in commit_message_files:\n",
    "            if (\n",
    "                msg_file.startswith(expected_previous_fix_message_prefix)\n",
    "                and (\"_commit_message_fixed_version.txt\" in msg_file or \"_commit_message.txt\" in msg_file)\n",
    "            ):\n",
    "                with open(os.path.join(commit_messages_dir, msg_file), \"r\", encoding=\"utf-8\") as f:\n",
    "                    previous_fix_message = f.read().strip()\n",
    "                break\n",
    "\n",
    "    previous_fix_diff_content = \"No previous fix code diff found\"\n",
    "    previous_fix_diff_file = f\"{file_name}_changes_v1_v2_fixed_version.txt\"\n",
    "\n",
    "    if os.path.exists(file_changes_dir) and previous_fix_diff_file in os.listdir(file_changes_dir):\n",
    "        with open(os.path.join(file_changes_dir, previous_fix_diff_file), \"r\", encoding=\"utf-8\") as f:\n",
    "            previous_fix_diff_content = f.read().strip()\n",
    "\n",
    "    future_commit_message = \"Future commit message not found\"\n",
    "    expected_future_commit_message_prefix = f\"{file_name}_{future_commit_version_number}_\"\n",
    "\n",
    "    if os.path.exists(commit_messages_dir):\n",
    "        for msg_file in commit_message_files:\n",
    "            if msg_file.startswith(expected_future_commit_message_prefix) and msg_file.endswith(\"_commit_message.txt\"):\n",
    "                with open(os.path.join(commit_messages_dir, msg_file), \"r\", encoding=\"utf-8\") as f:\n",
    "                    future_commit_message = f.read().strip()\n",
    "                break\n",
    "\n",
    "    future_diff_content = \"No future commit diff found\"\n",
    "    expected_future_diff_file = found_version\n",
    "\n",
    "    if os.path.exists(file_changes_dir) and expected_future_diff_file in os.listdir(file_changes_dir):\n",
    "        with open(os.path.join(file_changes_dir, expected_future_diff_file), \"r\", encoding=\"utf-8\") as f:\n",
    "            future_diff_content = f.read().strip()\n",
    "\n",
    "    prompt_content = f\"\"\"\n",
    "I am conducting research on how software security vulnerabilities evolve over time, specifically focusing on situations where a fix for one security vulnerability unintentionally introduces a new security vulnerability.\n",
    "\n",
    "You will be provided with details from two commits:\n",
    "\n",
    "1. **Previous Fix Commit**  A commit that fixed a known vulnerability.\n",
    "2. **Future Candidate Commit**  A later commit that modifies the same or nearby code.\n",
    "\n",
    "Your task is to determine whether the **candidate commit** is fixing a new vulnerability that was introduced by the **previous fix**.\n",
    "---\n",
    "\n",
    "**Commit ID:** {commit_hash}\n",
    "**Commit Message:**\n",
    "{previous_fix_message}\n",
    "\n",
    "**Code Changes (Diff Format):**\n",
    "{previous_fix_diff_content}\n",
    "\n",
    "---\n",
    "\n",
    "**Commit Message:**\n",
    "{future_commit_message}\n",
    "\n",
    "**Code Changes (Diff Format):**\n",
    "{future_diff_content}\n",
    "\n",
    "---\n",
    "\n",
    "Please respond **only with a valid JSON object** in the following format:\n",
    "\n",
    "```{{\"answer\": \"Yes\" or \"No\", \"reasoning\": \"Detailed explanation of why the candidate commit is or is not fixing a vulnerability introduced by the previous fix.\", \"confidence\": \"How confidence LLM is answering the question (In a scale of 1 to 10)?\"}}```\n",
    "\n",
    "**Do not include any extra text outside the JSON structure.**\n",
    "**If the previous fix is incomplete, the answer is \"No\" since it did not introduce a new security vulnerability. Similarly, if the previous fix did not properly fix the issue, the answer is still \"No\" unless a new security vulnerability is created.**\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    prompt_file_path = os.path.join(PROMPT_DIR, f\"{i}_{project_name}_{commit_hash}_{found_version}.txt\")\n",
    "\n",
    "    with open(prompt_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(prompt_content.strip())\n",
    "\n",
    "    generated_count += 1\n",
    "\n",
    "print(f\"\\n Prompts successfully generated for records {START_INDEX} to {END_INDEX}.\")\n",
    "print(f\" Total Prompts Generated: {generated_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98646895",
   "metadata": {},
   "source": [
    "# modified prompt with confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e281ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "BASE_DIR = r\"M:\\FULL_DATA_COLLECTED\"\n",
    "\n",
    "file_path = os.path.join(BASE_DIR, \"diverse_samples.csv\")\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "required_columns = [\"Commit\", \"File\", \"Found Version\", \"project\"]\n",
    "missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "if missing_columns:\n",
    "    print(f\" Error: Missing expected columns: {missing_columns}\")\n",
    "    exit()\n",
    "\n",
    "PROMPT_DIR = os.path.join(BASE_DIR, \"prompts3\")\n",
    "os.makedirs(PROMPT_DIR, exist_ok=True)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    project_name = row[\"project\"].strip()\n",
    "    commit_hash = row[\"Commit\"].strip()\n",
    "    file_name = row[\"File\"].strip()\n",
    "    found_version = row[\"Found Version\"].strip()\n",
    "\n",
    "    project_dir = os.path.join(BASE_DIR, project_name)\n",
    "    if not os.path.exists(project_dir):\n",
    "        continue\n",
    "\n",
    "    available_commit_dirs = os.listdir(project_dir)\n",
    "    matching_dirs = [d for d in available_commit_dirs if commit_hash in d]\n",
    "    if not matching_dirs:\n",
    "        continue\n",
    "\n",
    "    commit_dir = os.path.join(project_dir, matching_dirs[0])\n",
    "\n",
    "    commit_messages_dir = os.path.join(commit_dir, \"commit_messages\")\n",
    "    file_changes_dir = os.path.join(commit_dir, \"file_changes_in_versions\")\n",
    "\n",
    "    version_match = re.search(r\"_v\\d+_(v\\d+)\\.txt\", found_version)\n",
    "    if not version_match:\n",
    "        continue\n",
    "    future_commit_version_number = version_match.group(1)\n",
    "\n",
    "    previous_fix_message = \"Previous fix commit message not found\"\n",
    "    expected_previous_fix_message_prefix = f\"{file_name}_v2_\"\n",
    "\n",
    "    if os.path.exists(commit_messages_dir):\n",
    "        commit_message_files = os.listdir(commit_messages_dir)\n",
    "        for msg_file in commit_message_files:\n",
    "            if (\n",
    "                msg_file.startswith(expected_previous_fix_message_prefix)\n",
    "                and (\"_commit_message_fixed_version.txt\" in msg_file or \"_commit_message.txt\" in msg_file)\n",
    "            ):\n",
    "                with open(os.path.join(commit_messages_dir, msg_file), \"r\", encoding=\"utf-8\") as f:\n",
    "                    previous_fix_message = f.read().strip()\n",
    "                break\n",
    "\n",
    "    previous_fix_diff_content = \"No previous fix code diff found\"\n",
    "    previous_fix_diff_file = f\"{file_name}_changes_v1_v2_fixed_version.txt\"\n",
    "\n",
    "    if os.path.exists(file_changes_dir) and previous_fix_diff_file in os.listdir(file_changes_dir):\n",
    "        with open(os.path.join(file_changes_dir, previous_fix_diff_file), \"r\", encoding=\"utf-8\") as f:\n",
    "            previous_fix_diff_content = f.read().strip()\n",
    "\n",
    "    future_commit_message = \"Future commit message not found\"\n",
    "    expected_future_commit_message_prefix = f\"{file_name}_{future_commit_version_number}_\"\n",
    "\n",
    "    if os.path.exists(commit_messages_dir):\n",
    "        for msg_file in commit_message_files:\n",
    "            if msg_file.startswith(expected_future_commit_message_prefix) and msg_file.endswith(\"_commit_message.txt\"):\n",
    "                with open(os.path.join(commit_messages_dir, msg_file), \"r\", encoding=\"utf-8\") as f:\n",
    "                    future_commit_message = f.read().strip()\n",
    "                break\n",
    "\n",
    "    future_diff_content = \"No future commit diff found\"\n",
    "    expected_future_diff_file = found_version\n",
    "\n",
    "    if os.path.exists(file_changes_dir) and expected_future_diff_file in os.listdir(file_changes_dir):\n",
    "        with open(os.path.join(file_changes_dir, expected_future_diff_file), \"r\", encoding=\"utf-8\") as f:\n",
    "            future_diff_content = f.read().strip()\n",
    "\n",
    "    prompt_content = f\"\"\"\n",
    "I am conducting research on how software security vulnerabilities evolve over time, specifically focusing on situations where a fix for one security vulnerability unintentionally introduces a new security vulnerability.\n",
    "\n",
    "You will be provided with details from two commits:\n",
    "\n",
    "1. **Previous Fix Commit**  A commit that fixed a known vulnerability.\n",
    "2. **Future Candidate Commit**  A later commit that modifies the same or nearby code.\n",
    "\n",
    "Your task is to determine whether the **candidate commit** is fixing a new vulnerability that was introduced by the **previous fix**.\n",
    "---\n",
    "\n",
    "**Commit ID:** {commit_hash}\n",
    "**Commit Message:**\n",
    "{previous_fix_message}\n",
    "\n",
    "**Code Changes (Diff Format):**\n",
    "{previous_fix_diff_content}\n",
    "\n",
    "---\n",
    "\n",
    "**Commit Message:**\n",
    "{future_commit_message}\n",
    "\n",
    "**Code Changes (Diff Format):**\n",
    "{future_diff_content}\n",
    "\n",
    "---\n",
    "\n",
    "Please respond **only with a valid JSON object** in the following format:\n",
    "\n",
    "```{{\"answer\": \"Yes\" or \"No\", \"reasoning\": \"Detailed explanation of why the candidate commit is or is not fixing a vulnerability introduced by the previous fix.\", \"confidence\": \"How confidence LLM is answering the question (In a scale of 1 to 10)?\"}}```\n",
    "\n",
    "**Do not include any extra text outside the JSON structure.**\n",
    "**If the previous fix is incomplete, the answer is \"No\" since it did not introduce a new security vulnerability. Similarly, if the previous fix did not properly fix the issue, the answer is still \"No\" unless a new security vulnerability is created.**\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    prompt_file_path = os.path.join(PROMPT_DIR, f\"{project_name}_{commit_hash}.txt\")\n",
    "    with open(prompt_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(prompt_content.strip())\n",
    "\n",
    "print(\"\\n All prompts generated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f46744",
   "metadata": {},
   "source": [
    "# 50 new samples\n",
    "# wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52336fbe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da715d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "full_data_path = r\"M:\\FULL_DATA_COLLECTED\\records_with_overlap_and_security.csv\"\n",
    "previous_samples_path = r\"M:\\FULL_DATA_COLLECTED\\diverse_samples.csv\"\n",
    "new_output_path = r\"M:\\FULL_DATA_COLLECTED\\new_diverse_samples.csv\"\n",
    "\n",
    "df = pd.read_csv(full_data_path)\n",
    "\n",
    "if \"Project\" in df.columns:\n",
    "    df.rename(columns={\"Project\": \"project\"}, inplace=True)\n",
    "\n",
    "df.insert(0, \"original_index\", df.index + 1)\n",
    "\n",
    "previous_samples = pd.read_csv(previous_samples_path)\n",
    "\n",
    "if \"Project\" in previous_samples.columns:\n",
    "    previous_samples.rename(columns={\"Project\": \"project\"}, inplace=True)\n",
    "\n",
    "df_filtered = df[~df.isin(previous_samples)].dropna(how=\"all\")\n",
    "\n",
    "unique_projects = df_filtered[\"project\"].unique()\n",
    "samples_per_project = max(1, 50 // len(unique_projects))\n",
    "\n",
    "new_diverse_samples = df_filtered.groupby(\"project\", group_keys=False).apply(\n",
    "    lambda x: x.sample(min(samples_per_project, len(x)), random_state=42)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "new_diverse_samples = new_diverse_samples.head(50)\n",
    "\n",
    "new_diverse_samples.to_csv(new_output_path, index=False)\n",
    "print(f\"50 new diverse samples saved to {new_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fe19ec",
   "metadata": {},
   "source": [
    "# 50 new samples with new projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6d331c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "full_data_path = r\"M:\\FULL_DATA_COLLECTED\\records_with_overlap_and_security.csv\"\n",
    "previous_samples_path = r\"M:\\FULL_DATA_COLLECTED\\diverse_samples.csv\"\n",
    "new_output_path = r\"M:\\FULL_DATA_COLLECTED\\new_diverse_samples.csv\"\n",
    "\n",
    "df = pd.read_csv(full_data_path)\n",
    "\n",
    "if \"Project\" in df.columns:\n",
    "    df.rename(columns={\"Project\": \"project\"}, inplace=True)\n",
    "\n",
    "df.insert(0, \"original_index\", df.index + 1)\n",
    "\n",
    "previous_samples = pd.read_csv(previous_samples_path)\n",
    "\n",
    "if \"Project\" in previous_samples.columns:\n",
    "    previous_samples.rename(columns={\"Project\": \"project\"}, inplace=True)\n",
    "\n",
    "previous_projects = set(previous_samples[\"project\"].unique())\n",
    "df_filtered = df[~df[\"project\"].isin(previous_projects)]\n",
    "\n",
    "unique_projects = df_filtered[\"project\"].unique()\n",
    "new_selected_projects = pd.Series(unique_projects).sample(n=min(50, len(unique_projects)), random_state=42).tolist()\n",
    "\n",
    "new_diverse_samples = df_filtered[df_filtered[\"project\"].isin(new_selected_projects)].groupby(\"project\", group_keys=False).apply(\n",
    "    lambda x: x.sample(1, random_state=42)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "new_diverse_samples.to_csv(new_output_path, index=False)\n",
    "\n",
    "print(f\" 50 new diverse projects saved to {new_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571d5419",
   "metadata": {},
   "source": [
    "# 50 prompts filtering from full prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606758f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "BASE_DIR = r\"M:\\FULL_DATA_COLLECTED\"\n",
    "SOURCE_PROMPT_DIR = os.path.join(BASE_DIR, \"PromptsFull\")\n",
    "DEST_PROMPT_DIR = os.path.join(BASE_DIR, \"newDiverseSample\")\n",
    "os.makedirs(DEST_PROMPT_DIR, exist_ok=True)\n",
    "\n",
    "new_samples_path = os.path.join(BASE_DIR, \"new_diverse_samples.csv\")\n",
    "new_samples = pd.read_csv(new_samples_path)\n",
    "\n",
    "if \"Project\" in new_samples.columns:\n",
    "    new_samples.rename(columns={\"Project\": \"project\"}, inplace=True)\n",
    "\n",
    "copied_count = 0\n",
    "missing_files = []\n",
    "\n",
    "for _, row in new_samples.iterrows():\n",
    "    original_index = row[\"original_index\"]\n",
    "    project_name = str(row[\"project\"]).strip()\n",
    "    commit_hash = str(row[\"Commit\"]).strip()\n",
    "    found_version = str(row[\"Found Version\"]).strip().replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n",
    "\n",
    "    prompt_filename = f\"{original_index}_{project_name}_{commit_hash}_{found_version}.txt\"\n",
    "\n",
    "    source_prompt_path = os.path.join(SOURCE_PROMPT_DIR, prompt_filename)\n",
    "    dest_prompt_path = os.path.join(DEST_PROMPT_DIR, prompt_filename)\n",
    "\n",
    "    if os.path.exists(source_prompt_path):\n",
    "        shutil.copy2(source_prompt_path, dest_prompt_path)\n",
    "        copied_count += 1\n",
    "    else:\n",
    "        missing_files.append(prompt_filename)\n",
    "\n",
    "print(f\"\\n Prompts successfully copied to 'newDiverseSample'.\")\n",
    "print(f\" Total Prompts Copied: {copied_count}\")\n",
    "if missing_files:\n",
    "    print(f\" Missing Prompts ({len(missing_files)}):\")\n",
    "    for missing in missing_files:\n",
    "        print(f\"   - {missing}\")\n",
    "else:\n",
    "    print(\" No missing prompts.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5b6488",
   "metadata": {},
   "source": [
    "# merge GPT result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6dff0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "RESULTS_DIR = r\"M:\\FULL_DATA_COLLECTED\\PromptsFull\\gpt-4o\"\n",
    "\n",
    "if not os.path.exists(RESULTS_DIR):\n",
    "    print(f\" Directory not found: {RESULTS_DIR}\")\n",
    "    exit()\n",
    "\n",
    "xlsx_files = [f for f in os.listdir(RESULTS_DIR) if f.startswith(\"gpt-4o_results_\") and f.endswith(\".xlsx\")]\n",
    "\n",
    "if not xlsx_files:\n",
    "    print(\" No LLM result files found in the directory.\")\n",
    "    exit()\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for file in xlsx_files:\n",
    "    file_path = os.path.join(RESULTS_DIR, file)\n",
    "    df = pd.read_excel(file_path)\n",
    "\n",
    "    expected_columns = [\"Prompt File\", \"Answer\", \"Reasoning\", \"Confidence\"]\n",
    "    if not all(col in df.columns for col in expected_columns):\n",
    "        print(f\" Warning: {file} has missing or unexpected columns. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    df_list.append(df)\n",
    "\n",
    "if df_list:\n",
    "    merged_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    merged_df[\"Sort Index\"] = merged_df[\"Prompt File\"].apply(\n",
    "        lambda x: int(re.search(r\"^\\d+\", str(x)).group()) if re.search(r\"^\\d+\", str(x)) else float('inf')\n",
    "    )\n",
    "\n",
    "    merged_df = merged_df.sort_values(by=\"Sort Index\").drop(columns=[\"Sort Index\"])\n",
    "\n",
    "    merged_results_path = os.path.join(RESULTS_DIR, \"gpt-4o_all_results.xlsx\")\n",
    "\n",
    "    merged_df.to_excel(merged_results_path, index=False)\n",
    "\n",
    "    print(f\"\\n Successfully merged {len(df_list)} files into {merged_results_path}.\")\n",
    "else:\n",
    "    print(\" No valid result files found for merging.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d26b34",
   "metadata": {},
   "source": [
    "# merge Gemini output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea22385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "RESULTS_DIR = r\"M:\\FULL_DATA_COLLECTED\\PromptsFull\\gemini-2.0-flash\"\n",
    "\n",
    "if not os.path.exists(RESULTS_DIR):\n",
    "    print(f\" Directory not found: {RESULTS_DIR}\")\n",
    "    exit()\n",
    "\n",
    "xlsx_files = [f for f in os.listdir(RESULTS_DIR) if f.startswith(\"gemini-2.0-flash_results_\") and f.endswith(\".xlsx\")]\n",
    "\n",
    "if not xlsx_files:\n",
    "    print(\" No Gemini LLM result files found in the directory.\")\n",
    "    exit()\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for file in xlsx_files:\n",
    "    file_path = os.path.join(RESULTS_DIR, file)\n",
    "    df = pd.read_excel(file_path)\n",
    "\n",
    "    expected_columns = [\"Prompt File\", \"Answer\", \"Reasoning\", \"Confidence\"]\n",
    "    if not all(col in df.columns for col in expected_columns):\n",
    "        print(f\" Warning: {file} has missing or unexpected columns. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    df_list.append(df)\n",
    "\n",
    "if df_list:\n",
    "    merged_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    merged_df[\"Sort Index\"] = merged_df[\"Prompt File\"].apply(\n",
    "        lambda x: int(re.search(r\"^\\d+\", str(x)).group()) if re.search(r\"^\\d+\", str(x)) else float('inf')\n",
    "    )\n",
    "\n",
    "    merged_df = merged_df.sort_values(by=\"Sort Index\").drop(columns=[\"Sort Index\"])\n",
    "\n",
    "    merged_results_path = os.path.join(RESULTS_DIR, \"gemini-2.0-flash_all_results.xlsx\")\n",
    "\n",
    "    merged_df.to_excel(merged_results_path, index=False)\n",
    "\n",
    "    print(f\"\\n Successfully merged {len(df_list)} files into {merged_results_path}.\")\n",
    "else:\n",
    "    print(\" No valid result files found for merging.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20939d62",
   "metadata": {},
   "source": [
    "# merging output from Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3a66c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "RESULTS_DIR = r\"M:\\FULL_DATA_COLLECTED\\PromptsFull\\claude-3-5-sonnet-20241022\"\n",
    "\n",
    "if not os.path.exists(RESULTS_DIR):\n",
    "    print(f\" Directory not found: {RESULTS_DIR}\")\n",
    "    exit()\n",
    "\n",
    "xlsx_files = [f for f in os.listdir(RESULTS_DIR) if f.startswith(\"claude-3-5-sonnet-20241022_results_\") and f.endswith(\".xlsx\")]\n",
    "\n",
    "if not xlsx_files:\n",
    "    print(\" No Claude LLM result files found in the directory.\")\n",
    "    exit()\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for file in xlsx_files:\n",
    "    file_path = os.path.join(RESULTS_DIR, file)\n",
    "    df = pd.read_excel(file_path)\n",
    "\n",
    "    expected_columns = [\"Prompt File\", \"Answer\", \"Reasoning\", \"Confidence\"]\n",
    "    if not all(col in df.columns for col in expected_columns):\n",
    "        print(f\" Warning: {file} has missing or unexpected columns. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    df_list.append(df)\n",
    "\n",
    "if df_list:\n",
    "    merged_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    merged_df[\"Sort Index\"] = merged_df[\"Prompt File\"].apply(\n",
    "        lambda x: int(re.search(r\"^\\d+\", str(x)).group()) if re.search(r\"^\\d+\", str(x)) else float('inf')\n",
    "    )\n",
    "\n",
    "    merged_df = merged_df.sort_values(by=\"Sort Index\").drop(columns=[\"Sort Index\"])\n",
    "\n",
    "    merged_results_path = os.path.join(RESULTS_DIR, \"claude-3-5-sonnet-20241022_all_results.xlsx\")\n",
    "\n",
    "    merged_df.to_excel(merged_results_path, index=False)\n",
    "\n",
    "    print(f\"\\n Successfully merged {len(df_list)} files into {merged_results_path}.\")\n",
    "else:\n",
    "    print(\" No valid result files found for merging.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fc8385",
   "metadata": {},
   "source": [
    "# merged LLM output for new samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39a5e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "BASE_DIR = r\"M:\\FULL_DATA_COLLECTED\\PromptsFull\"\n",
    "SAMPLED_PROMPTS_FILE = r\"M:\\FULL_DATA_COLLECTED\\new_diverse_samples.csv\"\n",
    "\n",
    "LLM_FILES = {\n",
    "    \"GPT-4o\": os.path.join(BASE_DIR, \"gpt-4o\", \"gpt-4o_all_results.xlsx\"),\n",
    "    \"Gemini\": os.path.join(BASE_DIR, \"gemini-2.0-flash\", \"gemini-2.0-flash_all_results.xlsx\"),\n",
    "    \"Claude\": os.path.join(BASE_DIR, \"claude-3-5-sonnet-20241022\", \"claude-3-5-sonnet-20241022_all_results.xlsx\"),\n",
    "}\n",
    "\n",
    "sampled_prompts_df = pd.read_csv(SAMPLED_PROMPTS_FILE)\n",
    "\n",
    "sampled_prompt_indices = sampled_prompts_df[\"original_index\"].astype(str) + \"_\"\n",
    "\n",
    "final_df = pd.DataFrame(columns=[\n",
    "    \"Prompt File\",\n",
    "    \"GPT-4o Answer\", \"GPT-4o Reasoning\", \"GPT-4o Confidence\",\n",
    "    \"Gemini Answer\", \"Gemini Reasoning\", \"Gemini Confidence\",\n",
    "    \"Claude Answer\", \"Claude Reasoning\", \"Claude Confidence\"\n",
    "])\n",
    "\n",
    "for model, file_path in LLM_FILES.items():\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\" Missing LLM response file: {file_path}\")\n",
    "        continue\n",
    "\n",
    "    llm_df = pd.read_excel(file_path)\n",
    "\n",
    "    llm_df[\"Index_Key\"] = llm_df[\"Prompt File\"].apply(lambda x: re.match(r\"^\\d+_\", str(x)).group() if re.match(r\"^\\d+_\", str(x)) else \"\")\n",
    "\n",
    "    llm_df[\"Index_Key\"] = llm_df[\"Index_Key\"].str.strip()\n",
    "\n",
    "    filtered_df = llm_df[llm_df[\"Index_Key\"].isin(sampled_prompt_indices)]\n",
    "\n",
    "    filtered_df = filtered_df.rename(columns={\n",
    "        \"Answer\": f\"{model} Answer\",\n",
    "        \"Reasoning\": f\"{model} Reasoning\",\n",
    "        \"Confidence\": f\"{model} Confidence\"\n",
    "    })\n",
    "\n",
    "    if final_df.empty:\n",
    "        final_df = filtered_df[[\"Prompt File\"]].copy()\n",
    "\n",
    "    final_df = final_df.merge(\n",
    "        filtered_df[[\"Prompt File\", f\"{model} Answer\", f\"{model} Reasoning\", f\"{model} Confidence\"]],\n",
    "        on=\"Prompt File\", how=\"left\"\n",
    "    )\n",
    "\n",
    "output_path = os.path.join(BASE_DIR, \"sampled_prompts_responses.xlsx\")\n",
    "final_df.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"\\n Sampled prompt responses saved in: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62e5be8",
   "metadata": {},
   "source": [
    "# taking all positive samples Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4963930b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "base_dir = \"M:/FULL_DATA_COLLECTED/PromptsFull/claude-3-5-sonnet-20241022\"\n",
    "input_file = os.path.join(base_dir, \"claude-3-5-sonnet-20241022_all_results.xlsx\")\n",
    "output_yes_file = \"M:/FULL_DATA_COLLECTED/claude_yes_samples.xlsx\"\n",
    "output_commit_file = \"M:/FULL_DATA_COLLECTED/claude_distinct_commits.xlsx\"\n",
    "\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "yes_column = \"Answer\"\n",
    "filename_column = \"Prompt File\"\n",
    "\n",
    "df_yes = df[df[yes_column].str.strip().str.lower() == \"yes\"]\n",
    "\n",
    "def extract_project_commit(filename):\n",
    "    commit_match = re.search(r\"([a-fA-F0-9]{40})\", filename)\n",
    "    commit_id = commit_match.group(1) if commit_match else None\n",
    "\n",
    "    project_match = re.search(r\"([^_/]+)_([a-fA-F0-9]{40})\", filename)\n",
    "    project_name = project_match.group(1) if project_match else None\n",
    "\n",
    "    return project_name, commit_id\n",
    "\n",
    "df_yes[[\"Project_Name\", \"Commit_ID\"]] = df_yes[filename_column].apply(\n",
    "    lambda x: pd.Series(extract_project_commit(x))\n",
    ")\n",
    "\n",
    "df_unique_commits = df_yes[[\"Project_Name\", \"Commit_ID\"]].dropna().drop_duplicates()\n",
    "\n",
    "df_yes.to_excel(output_yes_file, index=False)\n",
    "\n",
    "df_unique_commits.to_excel(output_commit_file, index=False)\n",
    "\n",
    "print(\" Processing complete!\")\n",
    "print(f\"Total 'Yes' samples: {len(df_yes)}\")\n",
    "print(f\"Total distinct project-commit pairs: {len(df_unique_commits)}\")\n",
    "print(f\"Files saved:\\n  - {output_yes_file}\\n  - {output_commit_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e079d480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "base_dir = \"M:/FULL_DATA_COLLECTED/PromptsFull/claude-3-5-sonnet-20241022\"\n",
    "input_file = os.path.join(base_dir, \"claude-3-5-sonnet-20241022_all_results.xlsx\")\n",
    "output_yes_file = \"M:/FULL_DATA_COLLECTED/claude_yes_samples.xlsx\"\n",
    "output_commit_file = \"M:/FULL_DATA_COLLECTED/claude_distinct_commits.xlsx\"\n",
    "\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "yes_column = \"Answer\"\n",
    "filename_column = \"Prompt File\"\n",
    "\n",
    "df_yes = df[df[yes_column].str.strip().str.lower() == \"yes\"]\n",
    "\n",
    "def extract_project_commit(filename):\n",
    "    if not isinstance(filename, str):\n",
    "        return None, None\n",
    "\n",
    "    commit_match = re.search(r\"([a-fA-F0-9]{40})\", filename)\n",
    "    commit_id = commit_match.group(1) if commit_match else None\n",
    "\n",
    "    project_name = None\n",
    "    if commit_id:\n",
    "        clean_filename = filename.split(\"_\", 1)[-1]\n",
    "\n",
    "        project_name = clean_filename.rsplit(f\"_{commit_id}\", 1)[0]\n",
    "\n",
    "        parts = project_name.split(\"_\")\n",
    "        mid = len(parts) // 2\n",
    "        if parts[:mid] == parts[mid:]:\n",
    "            project_name = \"_\".join(parts[:mid])\n",
    "\n",
    "    return project_name, commit_id\n",
    "\n",
    "\n",
    "\n",
    "df_yes[[\"Project_Name\", \"Commit_ID\"]] = df_yes[filename_column].apply(\n",
    "    lambda x: pd.Series(extract_project_commit(x))\n",
    ")\n",
    "\n",
    "df_unique_commits = df_yes[[\"Project_Name\", \"Commit_ID\"]].dropna().drop_duplicates()\n",
    "\n",
    "df_yes.to_excel(output_yes_file, index=False)\n",
    "\n",
    "df_unique_commits.to_excel(output_commit_file, index=False)\n",
    "\n",
    "print(\" Processing complete!\")\n",
    "print(f\"Total 'Yes' samples: {len(df_yes)}\")\n",
    "print(f\"Total distinct project-commit pairs: {len(df_unique_commits)}\")\n",
    "print(f\"Files saved:\\n  - {output_yes_file}\\n  - {output_commit_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc33bf93",
   "metadata": {},
   "source": [
    "# \"yes\" commit statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a61d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "distinct_commits_file = \"M:/FULL_DATA_COLLECTED/claude_distinct_commits.xlsx\"\n",
    "df_distinct = pd.read_excel(distinct_commits_file)\n",
    "\n",
    "records_file = \"M:/FULL_DATA_COLLECTED/records_with_overlap_and_security.csv\"\n",
    "df_records = pd.read_csv(records_file)\n",
    "\n",
    "df_distinct.columns = df_distinct.columns.str.strip()\n",
    "df_records.columns = df_records.columns.str.strip()\n",
    "\n",
    "df_distinct[\"Commit\"] = df_distinct[\"Project_Name\"] + \"_\" + df_distinct[\"Commit_ID\"]\n",
    "\n",
    "df_matched = df_records.merge(df_distinct, on=\"Commit\", how=\"inner\")\n",
    "\n",
    "df_matched = df_matched.drop_duplicates(subset=[\"Commit\"])\n",
    "\n",
    "df_matched = df_matched[[\"Project\", \"Commit\", \"Lines Added in Fixed Version\",\t\"Lines Deleted in Fixed Version\",\t\"Hunks in Fixed Version\"\n",
    "]]\n",
    "\n",
    "output_file = \"M:/FULL_DATA_COLLECTED/matched_commits_statistics.csv\"\n",
    "df_matched.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"File saved successfully: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baeae99",
   "metadata": {},
   "source": [
    "# commit id was followed by project name. corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4464dd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "distinct_commits_file = \"M:/FULL_DATA_COLLECTED/claude_distinct_commits.xlsx\"\n",
    "df_distinct = pd.read_excel(distinct_commits_file)\n",
    "\n",
    "records_file = \"M:/FULL_DATA_COLLECTED/records_with_overlap_and_security.csv\"\n",
    "df_records = pd.read_csv(records_file)\n",
    "\n",
    "df_distinct.columns = df_distinct.columns.str.strip()\n",
    "df_records.columns = df_records.columns.str.strip()\n",
    "\n",
    "df_distinct[\"Commit\"] = df_distinct[\"Commit_ID\"].astype(str)\n",
    "\n",
    "df_records[\"Commit\"] = df_records[\"Commit\"].str.split(\"_\").str[-1]\n",
    "\n",
    "df_matched = df_records.merge(df_distinct, on=\"Commit\", how=\"inner\")\n",
    "\n",
    "df_matched = df_matched.drop_duplicates(subset=[\"Commit\"])\n",
    "\n",
    "df_matched = df_matched[[\"Project\", \"Commit\", \"Lines Added in Fixed Version\",\n",
    "                         \"Lines Deleted in Fixed Version\", \"Hunks in Fixed Version\"]]\n",
    "\n",
    "output_file = \"M:/FULL_DATA_COLLECTED/matched_commits_statistics.csv\"\n",
    "df_matched.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"File saved successfully: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4028c4af",
   "metadata": {},
   "source": [
    "# distinct commits in final matched commit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28840add",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"M:/FULL_DATA_COLLECTED/matched_commits_statistics.csv\")\n",
    "countProject = df[\"Project\"].nunique()\n",
    "print(countProject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ca4ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddf5dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644df533",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install fpdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ae02d4",
   "metadata": {},
   "source": [
    "# visual representation from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a89267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from fpdf import FPDF\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "output_dir = \"M:/FULL_DATA_COLLECTED/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "file_path = os.path.join(output_dir, \"matched_commits_statistics.csv\")\n",
    "df_stats = pd.read_csv(file_path)\n",
    "\n",
    "df_stats[\"Lines Changed\"] = df_stats[\"Lines Added in Fixed Version\"] + df_stats[\"Lines Deleted in Fixed Version\"]\n",
    "\n",
    "img1_path = os.path.join(output_dir, \"lines_changed_violin.png\")\n",
    "img2_path = os.path.join(output_dir, \"hunks_violin.png\")\n",
    "merged_img_path = os.path.join(output_dir, \"violin_plots_merged.png\")\n",
    "pdf_path = os.path.join(output_dir, \"violin_plots.pdf\")\n",
    "\n",
    "lines_changed_xlsx = os.path.join(output_dir, \"lines_changed_distribution.xlsx\")\n",
    "hunks_xlsx = os.path.join(output_dir, \"hunks_distribution.xlsx\")\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(7, 6))\n",
    "sns.violinplot(y=df_stats[\"Lines Changed\"], ax=ax1, color=\"purple\", bw=0.5)\n",
    "ax1.set_ylabel(\"Lines Changed (Added + Deleted)\")\n",
    "ax1.set_ylim(0, df_stats[\"Lines Changed\"].max() + 10)\n",
    "plt.savefig(img1_path, dpi=300, bbox_inches=\"tight\", transparent=True)\n",
    "plt.close()\n",
    "\n",
    "fig, ax2 = plt.subplots(figsize=(7, 6))\n",
    "sns.violinplot(y=df_stats[\"Hunks in Fixed Version\"], ax=ax2, color=\"green\", bw=0.5)\n",
    "ax2.set_ylabel(\"Hunks in Version\")\n",
    "ax2.set_ylim(0, df_stats[\"Hunks in Fixed Version\"].max() + 5)\n",
    "plt.savefig(img2_path, dpi=300, bbox_inches=\"tight\", transparent=True)\n",
    "plt.close()\n",
    "\n",
    "image1 = Image.open(img1_path)\n",
    "image2 = Image.open(img2_path)\n",
    "\n",
    "merged_width = image1.width + image2.width\n",
    "merged_height = max(image1.height, image2.height)\n",
    "\n",
    "merged_image = Image.new(\"RGB\", (merged_width, merged_height), (255, 255, 255))\n",
    "merged_image.paste(image1, (0, 0))\n",
    "merged_image.paste(image2, (image1.width, 0))\n",
    "merged_image.save(merged_img_path)\n",
    "\n",
    "pdf = FPDF(unit=\"pt\", format=[merged_width, merged_height])\n",
    "pdf.add_page()\n",
    "pdf.image(merged_img_path, x=0, y=0, w=merged_width, h=merged_height)\n",
    "pdf.output(pdf_path, \"F\")\n",
    "\n",
    "lines_bins = np.linspace(0, df_stats[\"Lines Changed\"].max(), num=15)\n",
    "df_stats[\"Lines Changed Bin\"] = pd.cut(df_stats[\"Lines Changed\"], bins=lines_bins, right=True)\n",
    "lines_distribution = df_stats[\"Lines Changed Bin\"].value_counts().sort_index().reset_index()\n",
    "lines_distribution.columns = [\"Range\", \"Frequency\"]\n",
    "lines_distribution.to_excel(lines_changed_xlsx, index=False)\n",
    "\n",
    "hunks_bins = np.linspace(0, df_stats[\"Hunks in Fixed Version\"].max(), num=15)\n",
    "df_stats[\"Hunks Bin\"] = pd.cut(df_stats[\"Hunks in Fixed Version\"], bins=hunks_bins, right=True)\n",
    "hunks_distribution = df_stats[\"Hunks Bin\"].value_counts().sort_index().reset_index()\n",
    "hunks_distribution.columns = [\"Range\", \"Frequency\"]\n",
    "hunks_distribution.to_excel(hunks_xlsx, index=False)\n",
    "\n",
    "print(f\" Violin plots saved with no margin in: {pdf_path}\")\n",
    "print(f\" 'Lines Changed' distribution saved in: {lines_changed_xlsx}\")\n",
    "print(f\" 'Hunks in Fixed Version' distribution saved in: {hunks_xlsx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d969366",
   "metadata": {},
   "source": [
    "# further breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4903f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "output_dir = \"M:/FULL_DATA_COLLECTED/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "file_path = os.path.join(output_dir, \"matched_commits_statistics.csv\")\n",
    "df_stats = pd.read_csv(file_path)\n",
    "\n",
    "df_stats[\"Lines Changed\"] = df_stats[\"Lines Added in Fixed Version\"] + df_stats[\"Lines Deleted in Fixed Version\"]\n",
    "\n",
    "lines_bins = [0, 10, 20, 50, 100, 200, 500, 1000, 2000, 5000, df_stats[\"Lines Changed\"].max()]\n",
    "df_stats[\"Lines Changed Bin\"] = pd.cut(df_stats[\"Lines Changed\"], bins=lines_bins, right=True)\n",
    "lines_distribution = df_stats[\"Lines Changed Bin\"].value_counts().sort_index().reset_index()\n",
    "lines_distribution.columns = [\"Range\", \"Frequency\"]\n",
    "\n",
    "hunks_bins = list(range(1, 11)) + [df_stats[\"Hunks in Fixed Version\"].max()]\n",
    "df_stats[\"Hunks Bin\"] = pd.cut(df_stats[\"Hunks in Fixed Version\"], bins=hunks_bins, right=True, include_lowest=True)\n",
    "hunks_distribution = df_stats[\"Hunks Bin\"].value_counts().sort_index().reset_index()\n",
    "hunks_distribution.columns = [\"Range\", \"Frequency\"]\n",
    "\n",
    "lines_changed_xlsx = os.path.join(output_dir, \"lines_changed_detailed.xlsx\")\n",
    "hunks_xlsx = os.path.join(output_dir, \"hunks_detailed.xlsx\")\n",
    "\n",
    "lines_distribution.to_excel(lines_changed_xlsx, index=False)\n",
    "hunks_distribution.to_excel(hunks_xlsx, index=False)\n",
    "\n",
    "print(f\" 'Lines Changed' detailed distribution saved in: {lines_changed_xlsx}\")\n",
    "print(f\" 'Hunks in Fixed Version' detailed distribution saved in: {hunks_xlsx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a0127d",
   "metadata": {},
   "source": [
    "# merge CVE and CWE id to the final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a53f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "commits_file = r\"M:\\FULL_DATA_COLLECTED\\matched_commits_statistics.csv\"\n",
    "cve_cwe_file = r\"L:\\co-ev\\GitHubDataCollection\\commitUrlFinalBigVul.csv\"\n",
    "output_file = r\"M:\\FULL_DATA_COLLECTED\\merged_dataset_with_CVE_CWE.csv\"\n",
    "\n",
    "df_commits = pd.read_csv(commits_file)\n",
    "df_cve_cwe = pd.read_csv(cve_cwe_file)\n",
    "\n",
    "df_commits[\"Commit\"] = df_commits[\"Commit\"].str.strip().str.lower()\n",
    "\n",
    "df_cve_cwe[\"commit_id\"] = df_cve_cwe[\"commit_id\"].astype(str).str.strip().str.lower()\n",
    "df_cve_cwe[\"commit_id\"] = df_cve_cwe[\"commit_id\"].apply(lambda x: re.split(r\"[\\?\\\n",
    "\n",
    "df_merged = df_commits.merge(df_cve_cwe, left_on=\"Commit\", right_on=\"commit_id\", how=\"left\")\n",
    "\n",
    "df_merged.drop(columns=[\"commit_id\"], inplace=True)\n",
    "\n",
    "df_merged.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Fixed merged dataset saved as: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f5da39",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_file = r\"M:\\FULL_DATA_COLLECTED\\merged_dataset_with_CVE_CWE.csv\"\n",
    "output_file = r\"M:\\FULL_DATA_COLLECTED\\cwe_id_counts.csv\"\n",
    "\n",
    "df_merged = pd.read_csv(merged_file)\n",
    "\n",
    "cwe_counts = df_merged[\"CWE ID\"].value_counts().reset_index()\n",
    "\n",
    "cwe_counts.columns = [\"CWE ID\", \"Count\"]\n",
    "\n",
    "cwe_counts.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"CWE ID counts saved successfully to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbba7cff",
   "metadata": {},
   "source": [
    "# CVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d99aa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "merged_file = r\"M:\\FULL_DATA_COLLECTED\\merged_dataset_with_CVE_CWE.csv\"\n",
    "output_cwe_file = r\"M:\\FULL_DATA_COLLECTED\\cwe_id_counts.csv\"\n",
    "output_cve_file = r\"M:\\FULL_DATA_COLLECTED\\cve_id_counts.csv\"\n",
    "\n",
    "df_merged = pd.read_csv(merged_file)\n",
    "\n",
    "cwe_counts = df_merged[\"CWE ID\"].value_counts().reset_index()\n",
    "cwe_counts.columns = [\"CWE ID\", \"Count\"]\n",
    "cwe_counts.to_csv(output_cwe_file, index=False)\n",
    "\n",
    "cve_counts = df_merged[\"CVE ID\"].value_counts().reset_index()\n",
    "cve_counts.columns = [\"CVE ID\", \"Count\"]\n",
    "cve_counts.to_csv(output_cve_file, index=False)\n",
    "\n",
    "print(f\"CWE ID counts saved successfully to: {output_cwe_file}\")\n",
    "print(f\"CVE ID counts saved successfully to: {output_cve_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e5ea0d",
   "metadata": {},
   "source": [
    "# create a directory and store the final dataset there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d21da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "root_dir = r\"M:\\FULL_DATA_COLLECTED\"\n",
    "csv_path = os.path.join(root_dir, \"merged_dataset_with_CVE_CWE.csv\")\n",
    "final_dataset_root = os.path.join(root_dir, \"FINAL_DATASET\")\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "if not {'Project', 'Commit'}.issubset(df.columns):\n",
    "    raise ValueError(\"CSV must contain 'project' and 'commit' columns.\")\n",
    "\n",
    "os.makedirs(final_dataset_root, exist_ok=True)\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    project = row['Project']\n",
    "    commit = row['Commit']\n",
    "\n",
    "    source_folder = os.path.join(root_dir, project, f\"{project}_{commit}\")\n",
    "    dest_project_folder = os.path.join(final_dataset_root, project)\n",
    "    dest_folder = os.path.join(dest_project_folder, f\"{project}_{commit}\")\n",
    "\n",
    "    if os.path.exists(source_folder):\n",
    "        os.makedirs(dest_project_folder, exist_ok=True)\n",
    "        if not os.path.exists(dest_folder):\n",
    "            shutil.copytree(source_folder, dest_folder)\n",
    "            print(f\"[{idx}] Copied: {source_folder}  {dest_folder}\")\n",
    "        else:\n",
    "            print(f\"[{idx}] Skipped (already exists): {dest_folder}\")\n",
    "    else:\n",
    "        print(f\"[{idx}] Source not found: {source_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222a77b4",
   "metadata": {},
   "source": [
    "# All diff files cleaned storing to generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c80ec44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "root_dir = r\"M:\\FULL_DATA_COLLECTED\\FINAL_DATASET\"\n",
    "csv_path = os.path.join(root_dir, \"merged_dataset_with_CVE_CWE.csv\")\n",
    "output_dir = os.path.join(root_dir, \"all_diffs_cleaned\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "saved_count = 0\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    project = row['Project']\n",
    "    commit = row['Commit']\n",
    "    commit_folder = f\"{project}_{commit}\"\n",
    "    diff_dir = os.path.join(root_dir, project, commit_folder, \"file_changes_in_versions\")\n",
    "\n",
    "    output_filename = f\"{idx + 1}_{project}_{commit}.txt\"\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "\n",
    "    if not os.path.exists(diff_dir):\n",
    "        print(f\"[{idx + 1}] Skipped (no folder): {diff_dir}\")\n",
    "        continue\n",
    "\n",
    "    matched_files = [\n",
    "        f for f in os.listdir(diff_dir)\n",
    "        if f.endswith(\"_changes_v1_v2_fixed_version.txt\")\n",
    "    ]\n",
    "\n",
    "    if not matched_files:\n",
    "        print(f\"[{idx + 1}] Skipped (no fixed diffs): {diff_dir}\")\n",
    "        continue\n",
    "\n",
    "    all_lines = []\n",
    "\n",
    "    for filename in matched_files:\n",
    "        file_path = os.path.join(diff_dir, filename)\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                lines = f.readlines()\n",
    "                cleaned = [\n",
    "                    line for line in lines\n",
    "                    if not (line.startswith('---') or line.startswith('+++') or line.startswith('@@'))\n",
    "                ]\n",
    "                non_blank_cleaned = [line for line in cleaned if line.strip()]\n",
    "                all_lines.extend(non_blank_cleaned)\n",
    "        except Exception as e:\n",
    "            print(f\"[{idx + 1}] Error reading {file_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if all_lines:\n",
    "        try:\n",
    "            with open(output_path, 'w', encoding='utf-8') as out_f:\n",
    "                out_f.writelines(all_lines)\n",
    "            saved_count += 1\n",
    "            print(f\"[{idx + 1}] Saved: {output_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[{idx + 1}] Write error: {output_filename}: {e}\")\n",
    "\n",
    "print(f\"\\n Finished. Total diffs saved: {saved_count}/{len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffec4fc",
   "metadata": {},
   "source": [
    "# diff file and commit message merged. hunk info preserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef42a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "root_dir = r\"M:\\FULL_DATA_COLLECTED\\FINAL_DATASET\"\n",
    "csv_path = os.path.join(root_dir, \"merged_dataset_with_CVE_CWE.csv\")\n",
    "\n",
    "parent_dir = os.path.join(root_dir, \"all_diffs_cleaned\")\n",
    "output_dir = os.path.join(parent_dir, \"allDiffMerged\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "saved_count = 0\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    project = row['Project']\n",
    "    commit = row['Commit']\n",
    "    commit_folder = f\"{project}_{commit}\"\n",
    "\n",
    "    diff_dir = os.path.join(root_dir, project, commit_folder, \"file_changes_in_versions\")\n",
    "    commit_msg_dir = os.path.join(root_dir, project, commit_folder, \"commit_messages\")\n",
    "\n",
    "    output_filename = f\"{idx + 1}_{project}_{commit}.txt\"\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "\n",
    "    if not os.path.exists(diff_dir):\n",
    "        print(f\"[{idx + 1}] Skipped (no folder): {diff_dir}\")\n",
    "        continue\n",
    "\n",
    "    matched_files = [\n",
    "        f for f in os.listdir(diff_dir)\n",
    "        if f.endswith(\"_changes_v1_v2_fixed_version.txt\")\n",
    "    ]\n",
    "\n",
    "    if not matched_files:\n",
    "        print(f\"[{idx + 1}] Skipped (no fixed diffs): {diff_dir}\")\n",
    "        continue\n",
    "\n",
    "    all_lines = []\n",
    "    for filename in matched_files:\n",
    "        file_path = os.path.join(diff_dir, filename)\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                lines = f.readlines()\n",
    "                all_lines.extend(lines)\n",
    "        except Exception as e:\n",
    "            print(f\"[{idx + 1}] Error reading {file_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    commit_msg = \"\"\n",
    "    if os.path.exists(commit_msg_dir):\n",
    "        msg_files = [f for f in os.listdir(commit_msg_dir) if f.endswith(\"_commit_message_fixed_version.txt\")]\n",
    "        if msg_files:\n",
    "            msg_file_path = os.path.join(commit_msg_dir, msg_files[0])\n",
    "            try:\n",
    "                with open(msg_file_path, 'r', encoding='utf-8') as f:\n",
    "                    commit_msg = f.read().strip()\n",
    "            except Exception as e:\n",
    "                print(f\"[{idx + 1}] Error reading commit message: {msg_file_path}: {e}\")\n",
    "\n",
    "    if all_lines:\n",
    "        try:\n",
    "            with open(output_path, 'w', encoding='utf-8') as out_f:\n",
    "                out_f.writelines(all_lines)\n",
    "                out_f.write(\"\\n\\nCOMMIT MESSAGE:\\n\")\n",
    "                out_f.write(commit_msg)\n",
    "            saved_count += 1\n",
    "            print(f\"[{idx + 1}] Saved: {output_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[{idx + 1}] Write error: {output_filename}: {e}\")\n",
    "\n",
    "print(f\"\\n Finished. Total diffs saved: {saved_count}/{len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159863c9",
   "metadata": {},
   "source": [
    "# prompt generation to get description of code changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bb7d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "root_dir = r\"M:\\FULL_DATA_COLLECTED\\FINAL_DATASET\"\n",
    "csv_path = os.path.join(root_dir, \"merged_dataset_with_CVE_CWE.csv\")\n",
    "\n",
    "prompts_dir = os.path.join(root_dir, \"prompts\")\n",
    "os.makedirs(prompts_dir, exist_ok=True)\n",
    "\n",
    "instruction_prompt = (\n",
    "    \"You will be given a code diff and the corresponding commit message. \"\n",
    "    \"Your task is to generate a brief summary that captures the intent and effect of the code change. \"\n",
    "    \"Focus on what functionality is being modified, added, or removed, and highlight any potential issues being addressed or introduced. \"\n",
    "    \"Use the information from both the diff and the commit message to produce a meaningful, semantic description of the change.\\n\\n\"\n",
    ")\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "saved_count = 0\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    project = row['Project']\n",
    "    commit = row['Commit']\n",
    "    commit_folder = f\"{project}_{commit}\"\n",
    "\n",
    "    diff_dir = os.path.join(root_dir, project, commit_folder, \"file_changes_in_versions\")\n",
    "    commit_msg_dir = os.path.join(root_dir, project, commit_folder, \"commit_messages\")\n",
    "\n",
    "    output_filename = f\"{idx + 1}_{project}_{commit}.txt\"\n",
    "    output_path = os.path.join(prompts_dir, output_filename)\n",
    "\n",
    "    if not os.path.exists(diff_dir):\n",
    "        print(f\"[{idx + 1}] Skipped (no folder): {diff_dir}\")\n",
    "        continue\n",
    "\n",
    "    matched_files = [\n",
    "        f for f in os.listdir(diff_dir)\n",
    "        if f.endswith(\"_changes_v1_v2_fixed_version.txt\")\n",
    "    ]\n",
    "\n",
    "    if not matched_files:\n",
    "        print(f\"[{idx + 1}] Skipped (no fixed diffs): {diff_dir}\")\n",
    "        continue\n",
    "\n",
    "    all_lines = []\n",
    "    for filename in matched_files:\n",
    "        file_path = os.path.join(diff_dir, filename)\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                lines = f.readlines()\n",
    "                all_lines.extend(lines)\n",
    "        except Exception as e:\n",
    "            print(f\"[{idx + 1}] Error reading {file_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    commit_msg = \"\"\n",
    "    if os.path.exists(commit_msg_dir):\n",
    "        msg_files = [f for f in os.listdir(commit_msg_dir) if f.endswith(\"_commit_message_fixed_version.txt\")]\n",
    "        if msg_files:\n",
    "            msg_file_path = os.path.join(commit_msg_dir, msg_files[0])\n",
    "            try:\n",
    "                with open(msg_file_path, 'r', encoding='utf-8') as f:\n",
    "                    commit_msg = f.read().strip()\n",
    "            except Exception as e:\n",
    "                print(f\"[{idx + 1}] Error reading commit message: {msg_file_path}: {e}\")\n",
    "\n",
    "    if all_lines:\n",
    "        try:\n",
    "            with open(output_path, 'w', encoding='utf-8') as out_f:\n",
    "                out_f.write(instruction_prompt)\n",
    "                out_f.writelines(all_lines)\n",
    "                out_f.write(\"\\n\\nCOMMIT MESSAGE:\\n\")\n",
    "                out_f.write(commit_msg)\n",
    "            saved_count += 1\n",
    "            print(f\"[{idx + 1}] Prompt saved: {output_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[{idx + 1}] Write error: {output_filename}: {e}\")\n",
    "\n",
    "print(f\"\\n Finished. Total prompt files saved: {saved_count}/{len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cae4a8",
   "metadata": {},
   "source": [
    "# prompts modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b847e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "root_dir = r\"M:\\FULL_DATA_COLLECTED\\FINAL_DATASET\"\n",
    "csv_path = os.path.join(root_dir, \"merged_dataset_with_CVE_CWE.csv\")\n",
    "\n",
    "prompts_dir = os.path.join(root_dir, \"prompts2\")\n",
    "os.makedirs(prompts_dir, exist_ok=True)\n",
    "\n",
    "instruction_prompt = (\n",
    "    \"You will be given a code diff and the corresponding commit message. \"\n",
    "    \"Your task is to generate a brief summary that describes what was changed in the code. \"\n",
    "    \"Focus on the semantic meaning of the changewhat operations, conditions, or behavior were modified, added, or removed. \"\n",
    "    \"Do not explain why the change was made or evaluate its impact. Just describe the essential code transformations in clear, neutral terms.\\n\\n\"\n",
    ")\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "saved_count = 0\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    project = row['Project']\n",
    "    commit = row['Commit']\n",
    "    commit_folder = f\"{project}_{commit}\"\n",
    "\n",
    "    diff_dir = os.path.join(root_dir, project, commit_folder, \"file_changes_in_versions\")\n",
    "    commit_msg_dir = os.path.join(root_dir, project, commit_folder, \"commit_messages\")\n",
    "\n",
    "    output_filename = f\"{idx + 1}_{project}_{commit}.txt\"\n",
    "    output_path = os.path.join(prompts_dir, output_filename)\n",
    "\n",
    "    if not os.path.exists(diff_dir):\n",
    "        print(f\"[{idx + 1}] Skipped (no folder): {diff_dir}\")\n",
    "        continue\n",
    "\n",
    "    matched_files = [\n",
    "        f for f in os.listdir(diff_dir)\n",
    "        if f.endswith(\"_changes_v1_v2_fixed_version.txt\")\n",
    "    ]\n",
    "\n",
    "    if not matched_files:\n",
    "        print(f\"[{idx + 1}] Skipped (no fixed diffs): {diff_dir}\")\n",
    "        continue\n",
    "\n",
    "    all_lines = []\n",
    "    for filename in matched_files:\n",
    "        file_path = os.path.join(diff_dir, filename)\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                lines = f.readlines()\n",
    "                all_lines.extend(lines)\n",
    "        except Exception as e:\n",
    "            print(f\"[{idx + 1}] Error reading {file_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    commit_msg = \"\"\n",
    "    if os.path.exists(commit_msg_dir):\n",
    "        msg_files = [f for f in os.listdir(commit_msg_dir) if f.endswith(\"_commit_message_fixed_version.txt\")]\n",
    "        if msg_files:\n",
    "            msg_file_path = os.path.join(commit_msg_dir, msg_files[0])\n",
    "            try:\n",
    "                with open(msg_file_path, 'r', encoding='utf-8') as f:\n",
    "                    commit_msg = f.read().strip()\n",
    "            except Exception as e:\n",
    "                print(f\"[{idx + 1}] Error reading commit message: {msg_file_path}: {e}\")\n",
    "\n",
    "    if all_lines:\n",
    "        try:\n",
    "            with open(output_path, 'w', encoding='utf-8') as out_f:\n",
    "                out_f.write(instruction_prompt)\n",
    "                out_f.writelines(all_lines)\n",
    "                out_f.write(\"\\n\\nCOMMIT MESSAGE:\\n\")\n",
    "                out_f.write(commit_msg)\n",
    "            saved_count += 1\n",
    "            print(f\"[{idx + 1}] Prompt saved: {output_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[{idx + 1}] Write error: {output_filename}: {e}\")\n",
    "\n",
    "print(f\"\\n Finished. Total prompt files saved: {saved_count}/{len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98529c93",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
