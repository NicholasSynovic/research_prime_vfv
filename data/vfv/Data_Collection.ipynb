{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709f857e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3462fe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e13af24",
   "metadata": {},
   "source": [
    "# read dfVulCWE (BigVul Commits processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a609d004",
   "metadata": {},
   "source": [
    "# all path need to be modified accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f25f879",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfVulCWE = pd.read_csv(\"dfVulCWE.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cb3c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cwe_per_commit = dfVulCWE.groupby('commit_id')['CVE ID'].nunique()\n",
    "\n",
    "commits_with_multiple_cwes = cwe_per_commit[cwe_per_commit > 1]\n",
    "\n",
    "print(\"Commits with multiple distinct CWE IDs:\")\n",
    "print(commits_with_multiple_cwes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b329b1",
   "metadata": {},
   "source": [
    "# new code to get comit urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e7f4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_filtered = dfVulCWE[['project', 'codeLink', 'commit_id', 'CWE ID', 'CVE ID']].drop_duplicates()\n",
    "\n",
    "df_filtered = df_filtered[df_filtered['codeLink'].str.startswith('https://github.com/', na=False)]\n",
    "\n",
    "df_filtered = df_filtered.sort_values(by='project')\n",
    "\n",
    "df_filtered.to_csv(\"commitUrlFinalBigVul.csv\", index=False)\n",
    "\n",
    "dfResult = df_filtered[['codeLink']].drop_duplicates()\n",
    "\n",
    "commit_urls = dfResult['codeLink'].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e558432b",
   "metadata": {},
   "source": [
    "# get file extensions to check which one to discard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f69215",
   "metadata": {},
   "source": [
    "# Github Token need to be added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a37978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import shutil\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "from base64 import b64decode\n",
    "from datetime import datetime\n",
    "import difflib\n",
    "import time\n",
    "import logging\n",
    "import csv\n",
    "\n",
    "GITHUB_TOKENS = [\n",
    "\n",
    "]\n",
    "\n",
    "LOG_FILE_PATH = r\"M:\\FULL_DATA_COLLECTED\\commit_errors_log.txt\"\n",
    "logging.basicConfig(\n",
    "    filename=LOG_FILE_PATH,\n",
    "    level=logging.ERROR,\n",
    "    format='[%(asctime)s] %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "current_token_index = 0\n",
    "REQUEST_TIMEOUT = 10\n",
    "MAX_RETRIES = 3\n",
    "RETRY_DELAY = 5\n",
    "FETCH_DELAY = 0.5\n",
    "FILE_PROCESS_DELAY = 0.5\n",
    "COMMIT_PROCESS_DELAY = 2\n",
    "\n",
    "TIME_LOG_PATH = r\"M:\\FULL_DATA_COLLECTED\\execution_time_log.csv\"\n",
    "\n",
    "logged_errors = set()\n",
    "\n",
    "def get_current_token():\n",
    "    global current_token_index\n",
    "    return GITHUB_TOKENS[current_token_index]\n",
    "\n",
    "def switch_to_next_token():\n",
    "    global current_token_index\n",
    "    current_token_index = (current_token_index + 1) % len(GITHUB_TOKENS)\n",
    "\n",
    "def check_rate_limit():\n",
    "    current_token = get_current_token()\n",
    "    headers = {'Authorization': f'token {current_token}'}\n",
    "    rate_limit_url = \"https://api.github.com/rate_limit\"\n",
    "    response = requests.get(rate_limit_url, headers=headers, timeout=REQUEST_TIMEOUT)\n",
    "    response.raise_for_status()\n",
    "    rate_limit_data = response.json()\n",
    "\n",
    "    remaining = rate_limit_data['rate']['remaining']\n",
    "\n",
    "    if remaining < 1000:\n",
    "        switch_to_next_token()\n",
    "\n",
    "def log_error(commit_url, error_message, index=None):\n",
    "    error_key = (commit_url, error_message)\n",
    "    if error_key not in logged_errors:\n",
    "        log_message = f\"Commit URL: {commit_url}\\n  - Error: {error_message}\"\n",
    "        if index is not None:\n",
    "            log_message = f\"[Index: {index}] {log_message}\"\n",
    "        logging.error(log_message)\n",
    "        logged_errors.add(error_key)\n",
    "\n",
    "def get_with_retries(url, headers, params=None, index=None):\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, params=params, timeout=REQUEST_TIMEOUT)\n",
    "            response.raise_for_status()\n",
    "            time.sleep(FETCH_DELAY)\n",
    "            return response\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if response.status_code == 404:\n",
    "                error_message = f\"404 Not Found - The requested resource could not be found.\"\n",
    "                log_error(url, error_message, index)\n",
    "                return None\n",
    "            else:\n",
    "                error_message = f\"HTTPError - {str(e)}\"\n",
    "                log_error(url, error_message, index)\n",
    "                time.sleep(RETRY_DELAY)\n",
    "        except (requests.exceptions.ConnectTimeout, requests.exceptions.ReadTimeout) as e:\n",
    "            error_message = f\"ConnectionTimeout - {str(e)}\"\n",
    "            log_error(url, error_message, index)\n",
    "            time.sleep(RETRY_DELAY)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            error_message = f\"Request failed: {str(e)}\"\n",
    "            log_error(url, error_message, index)\n",
    "            break\n",
    "    raise Exception(f\"Failed to get a response from {url} after {MAX_RETRIES} attempts.\")\n",
    "\n",
    "def fetch_all_commits_for_file(repo_owner, repo_name, file_path, index):\n",
    "    commits_url = f'https://api.github.com/repos/{repo_owner}/{repo_name}/commits'\n",
    "    params = {'path': file_path, 'per_page': 100}\n",
    "    all_commits = []\n",
    "    page = 1\n",
    "\n",
    "    while True:\n",
    "        headers = {'Authorization': f'token {get_current_token()}'}\n",
    "        params['page'] = page\n",
    "        response = get_with_retries(commits_url, headers, params=params, index=index)\n",
    "        if response is None:\n",
    "            log_error(commits_url, \"404 error when fetching commits\", index)\n",
    "            break\n",
    "        commits = response.json()\n",
    "        if not commits:\n",
    "            break\n",
    "        all_commits.extend(commits)\n",
    "        page += 1\n",
    "\n",
    "    return all_commits\n",
    "\n",
    "def remove_empty_directory(directory):\n",
    "    if os.path.exists(directory) and not os.listdir(directory):\n",
    "        shutil.rmtree(directory)\n",
    "        print(f\"Removed empty directory: {directory}\")\n",
    "\n",
    "NON_CODE_EXTENSIONS = ['', '.txt', '.md', '.jpg', '.png', '.jpeg', '.pdf', '.xml', '.conf', '.man', '.texi']\n",
    "\n",
    "def is_non_code(file_name):\n",
    "    file_extension = os.path.splitext(file_name)[1]\n",
    "    return file_extension in NON_CODE_EXTENSIONS\n",
    "\n",
    "def log_execution_time(range_str, execution_time):\n",
    "    file_exists = os.path.isfile(TIME_LOG_PATH)\n",
    "\n",
    "    with open(TIME_LOG_PATH, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        if not file_exists:\n",
    "            writer.writerow([\"Range\", \"Total Execution Time (seconds)\"])\n",
    "        writer.writerow([range_str, execution_time])\n",
    "\n",
    "def process_commit_link(commit_url, index):\n",
    "    try:\n",
    "        parsed_url = urlparse(commit_url)\n",
    "        path = parsed_url.path\n",
    "\n",
    "        match = re.match(r'/([^/]+)/([^/]+)/commit/([a-f0-9]+)', path)\n",
    "        if match:\n",
    "            REPO_OWNER = match.group(1)\n",
    "            REPO_NAME = match.group(2)\n",
    "            COMMIT_HASH = match.group(3)\n",
    "        else:\n",
    "            error_message = f\"Invalid commit URL: {commit_url}\"\n",
    "            log_error(commit_url, error_message, index)\n",
    "            return\n",
    "\n",
    "        FULL_DATA_DIR = r\"M:\\FULL_DATA_COLLECTED\"\n",
    "        PROJECT_DIR = os.path.join(FULL_DATA_DIR, REPO_NAME)\n",
    "        COMMIT_DIR = os.path.join(PROJECT_DIR, f\"{REPO_NAME}_{COMMIT_HASH}\")\n",
    "        OUTPUT_DIR = os.path.join(COMMIT_DIR, 'all_versions')\n",
    "        COMMIT_MESSAGES_DIR = os.path.join(COMMIT_DIR, 'commit_messages')\n",
    "        CHANGES_DIR = os.path.join(COMMIT_DIR, 'file_changes_in_versions')\n",
    "\n",
    "        check_rate_limit()\n",
    "\n",
    "        commit_api_url = f'https://api.github.com/repos/{REPO_OWNER}/{REPO_NAME}/commits/{COMMIT_HASH}'\n",
    "\n",
    "        headers = {'Authorization': f'token {get_current_token()}'}\n",
    "        response = get_with_retries(commit_api_url, headers, index=index)\n",
    "        if response is None:\n",
    "            return\n",
    "        commit_data = response.json()\n",
    "\n",
    "        files = commit_data['files']\n",
    "\n",
    "        if files:\n",
    "            for file_info in files:\n",
    "                file_path = file_info['filename']\n",
    "\n",
    "                if is_non_code(file_path):\n",
    "                    continue\n",
    "\n",
    "                commits = fetch_all_commits_for_file(REPO_OWNER, REPO_NAME, file_path, index)\n",
    "\n",
    "                commits.reverse()\n",
    "                fixed_version_index = None\n",
    "                for idx, commit in enumerate(commits):\n",
    "                    if commit['sha'] == COMMIT_HASH:\n",
    "                        fixed_version_index = idx\n",
    "                        break\n",
    "\n",
    "                if fixed_version_index is None:\n",
    "                    error_message = f\"Fixed version {COMMIT_HASH} not found in commit history for {file_path}.\"\n",
    "                    log_error(commit_url, error_message, index)\n",
    "                    continue\n",
    "\n",
    "                if fixed_version_index > 0:\n",
    "                    commits_to_process = commits[fixed_version_index - 1:]\n",
    "                else:\n",
    "                    commits_to_process = commits[fixed_version_index:]\n",
    "\n",
    "                os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "                os.makedirs(COMMIT_MESSAGES_DIR, exist_ok=True)\n",
    "                os.makedirs(CHANGES_DIR, exist_ok=True)\n",
    "\n",
    "                previous_content = None\n",
    "                version_number = 1\n",
    "\n",
    "                for commit in commits_to_process:\n",
    "                    sha = commit['sha']\n",
    "                    commit_date = commit['commit']['committer']['date']\n",
    "                    formatted_date = datetime.strptime(commit_date, '%Y-%m-%dT%H:%M:%SZ').strftime('%Y%m%d_%H%M%S')\n",
    "                    commit_message = commit['commit']['message'].strip().replace('\\n', ' ')\n",
    "                    file_url = f'https://api.github.com/repos/{REPO_OWNER}/{REPO_NAME}/contents/{file_path}?ref={sha}'\n",
    "\n",
    "                    is_fixed_version = (sha == COMMIT_HASH)\n",
    "                    version_suffix = \"_fixed_version\" if is_fixed_version else \"\"\n",
    "\n",
    "                    try:\n",
    "                        headers = {'Authorization': f'token {get_current_token()}'}\n",
    "                        file_response = get_with_retries(file_url, headers, index=index)\n",
    "                        if file_response is None:\n",
    "                            continue\n",
    "                        file_content_base64 = file_response.json()['content']\n",
    "                        file_content = b64decode(file_content_base64).decode('utf-8')\n",
    "\n",
    "                        file_name = os.path.basename(file_path)\n",
    "                        file_version_path = os.path.join(OUTPUT_DIR, f\"{file_name}_v{version_number}_{formatted_date}_{sha}{version_suffix}.txt\")\n",
    "                        with open(file_version_path, 'w', encoding='utf-8') as f:\n",
    "                            f.write(file_content)\n",
    "\n",
    "                        commit_message_path = os.path.join(COMMIT_MESSAGES_DIR, f\"{file_name}_v{version_number}_{formatted_date}_{sha}_commit_message{version_suffix}.txt\")\n",
    "                        with open(commit_message_path, 'w', encoding='utf-8') as f:\n",
    "                            f.write(commit_message)\n",
    "\n",
    "                        if previous_content is not None:\n",
    "                            diff = difflib.unified_diff(previous_content.splitlines(), file_content.splitlines(),\n",
    "                                                        fromfile=f'v{version_number-1}', tofile=f'v{version_number}')\n",
    "                            changes_file_path = os.path.join(CHANGES_DIR, f\"{file_name}_changes_v{version_number-1}_v{version_number}{version_suffix}.txt\")\n",
    "                            with open(changes_file_path, 'w', encoding='utf-8') as f:\n",
    "                                f.write('\\n'.join(diff))\n",
    "\n",
    "                        previous_content = file_content\n",
    "                        version_number += 1\n",
    "\n",
    "                        time.sleep(FILE_PROCESS_DELAY)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        error_message = f\"Unexpected error while processing the file: '{file_path}'. Error: {str(e)}\"\n",
    "                        log_error(commit_url, error_message, index)\n",
    "\n",
    "        remove_empty_directory(COMMIT_DIR)\n",
    "\n",
    "        check_rate_limit()\n",
    "\n",
    "    except Exception as e:\n",
    "        error_message = f\"Exception - {str(e)}\"\n",
    "        log_error(commit_url, error_message, index)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "start_index = 401\n",
    "end_index = 450\n",
    "\n",
    "for index in range(start_index, end_index + 1):\n",
    "    try:\n",
    "        process_commit_link(commit_urls[index], index)\n",
    "        print(f\"{index}\")\n",
    "    except Exception as e:\n",
    "        log_error(commit_urls[index], f\"Error processing commit at index {index}. Error: {str(e)}\", index)\n",
    "    time.sleep(COMMIT_PROCESS_DELAY)\n",
    "\n",
    "end_time = time.time()\n",
    "total_execution_time = end_time - start_time\n",
    "\n",
    "log_execution_time(f\"{start_index}-{end_index}\", total_execution_time)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64b5a53",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
